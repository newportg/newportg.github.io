[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gary Newport MBCS",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "docs/Content/Journal/posts/Test Note/index.html",
    "href": "docs/Content/Journal/posts/Test Note/index.html",
    "title": "Test Note",
    "section": "",
    "text": "Hello World! This is a test note."
  },
  {
    "objectID": "Content/Projects/index.html",
    "href": "Content/Projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "February 1, 2025\n        \n        \n            Frontify Marketing Platform\n\n            \n            \n                \n                \n                    Frontify\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "Content/Journal/posts/MediatR/index.html#hows-this-work",
    "href": "Content/Journal/posts/MediatR/index.html#hows-this-work",
    "title": "MediatR",
    "section": "How‚Äôs this work?",
    "text": "How‚Äôs this work?\n\nINotification (Mediator)\n\nDefines an interface for communicating with Colleague objects\n\nNotifier1, Notifier2 are\n\nEach Colleague class knows its Mediator object\nEach colleague communicates with its mediator whenever it would have otherwise communicated with another colleague\nConcrete implementations of the task to be performed,\nThey need to implement the Handler method defined by the INotificationHandler interface.\n\nNotificationMessage\n\nImplements cooperative behavior by coordinating Colleague objects knows and maintains its colleagues\nInstantiates the Notifier classes"
  },
  {
    "objectID": "Content/Journal/posts/MediatR/index.html#install",
    "href": "Content/Journal/posts/MediatR/index.html#install",
    "title": "MediatR",
    "section": "Install",
    "text": "Install\nInstall-Package MediatR"
  },
  {
    "objectID": "Content/Journal/posts/MediatR/index.html#reference",
    "href": "Content/Journal/posts/MediatR/index.html#reference",
    "title": "MediatR",
    "section": "Reference",
    "text": "Reference\n\nMediatr Framework\nDot Factory Pattern\nMedium Article\nMediator Tutorial 1\nMediator Tutorial 2\nMediator Tutorial 3"
  },
  {
    "objectID": "Content/Journal/posts/EIP-ContentBasedRouting/index.html",
    "href": "Content/Journal/posts/EIP-ContentBasedRouting/index.html",
    "title": "EIP Content Based Routing",
    "section": "",
    "text": "LA Pattern"
  },
  {
    "objectID": "Content/Journal/posts/AzureApplicationHosting/index.html",
    "href": "Content/Journal/posts/AzureApplicationHosting/index.html",
    "title": "Azure Application Hosting",
    "section": "",
    "text": "Each component is a separately deploy-able artefact, but we need a coherent single URL to link them all. The normal method would be to deploy out each individual component to Azure and each would get its own ‚Äòaurewebsites.com‚Äô URL. This approach would lead to confusion, as it would mean you would need to keep lists of URL‚Äôs By using the proxy feature of Azure Functions we can define routes to each of the installed artefacts while preserving a single URL for the application. So:-\n\n\n\nRoute\nResult\n\n\n\n\nzoomalong.co.uk\nWebsite\n\n\nzoomalong.co.uk/api\nAzure functions\n\n\nzoomalong.co.uk/static\nAzure Storage Account\n\n\n\nUsed to store any files or images etc. Because both the API and Web Application exist on the same URL then we won‚Äôt run into any CORS issues. Remember to bind the DNS Cname to the Azure function proxy and not the website.\n\n\n\naah1\n\n\nHow ? In your Azure Functions Project Create a files called proxies.json and insert the following code\nproxies.json\n{\n  \"$schema\": \"http://json.schemastore.org/proxies\",\n    \"proxies\": {\n      \"api\": {\n        \"matchCondition\": {\n          \"route\": \"/api/{*url}\"\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"app\": {\n        \"matchCondition\": {\n          \"route\": \"{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"appResources\": {\n        \"matchCondition\": {\n          \"route\": \"/static/{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      }\n    }\n  }\nChange in the build. * Build should include Azure App Service Deploy V3 or greater * Update this section with the file to be changed. *  * Add your substitutions to the Variables section * As Path to variable to be replace element.element.element * Value to be replaced. * as below\n\n\n\n\n\n\n\nProxy\nURL\n\n\n\n\nproxies.api.backendUri\nhttps://azure website url\n\n\nproxies.app.backendUri\nhttps://azure function url\n\n\nproxies.appResources.backendUri\nhttps:// azure storage account blob strorage\n\n\n\n\nRepeat in the release."
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html",
    "href": "Content/Journal/posts/APIFirst/index.html",
    "title": "API First",
    "section": "",
    "text": "API First Process"
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#description",
    "href": "Content/Journal/posts/APIFirst/index.html#description",
    "title": "API First",
    "section": "Description",
    "text": "Description\nAPI First is an approach to software development that prioritizes the design and implementation of APIs before building the underlying application or service. This approach emphasizes the importance of APIs as the primary means of communication between different components of a system, as well as between different systems.\nBy designing APIs first, developers can ensure that the APIs are well-defined, consistent, and easy to use. This can lead to better collaboration between teams, faster development cycles, and more efficient integration with third-party services.\nThe approach detailed here takes the responsibility for creating the API definition away from the developer and places it in the hands of a specialist tool. This ensures that the API definition is created in a consistent manner and adheres to best practices. The API definition is then used to generate the client and server code, which can be integrated into the application or service. This approach can help to reduce errors and inconsistencies in the code, as well as improve the overall quality of the API. The API First approach can be particularly beneficial in complex systems where multiple teams are working on different components. By defining the APIs first, teams can work independently while still ensuring that their components will integrate seamlessly with the rest of the system. The API First approach can also help to improve the overall user experience of the application or service. By designing APIs that are easy to use and understand, developers can create applications that are more intuitive and user-friendly. Overall, the API First approach is a powerful way to improve the development process and create high-quality applications and services that are built around well-designed APIs."
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#tools",
    "href": "Content/Journal/posts/APIFirst/index.html#tools",
    "title": "API First",
    "section": "Tools",
    "text": "Tools\nThe tools used in this process are:\n\nTypeSpec - A tool for designing and generating APIs\nAutoRest - A tool for generating client and server code from API definitions\nQuarto - A tool for creating and publishing documents\nLUA - A lightweight scripting language used for automation\nPowerShell - A task automation and configuration management framework\nC# - A programming language used for building applications and services\nAzure Functions - A serverless compute service used for building and deploying applications and services"
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#process",
    "href": "Content/Journal/posts/APIFirst/index.html#process",
    "title": "API First",
    "section": "Process",
    "text": "Process\nThe process for creating an API using the API First approach is as follows:\n\nDesign the API - Use TypeSpec to design the API, including the endpoints, request and response formats, and any authentication or authorization requirements.\nGenerate the API definition - Use TypeSpec to generate the API definition in a standard format, such as OpenAPI or Swagger.\nGenerate the client and server code - Use AutoRest to generate the client and server code from the API definition.\nIntegrate the code - Integrate the generated code into the application or service.\nTest the API - Test the API to ensure that it works as expected and meets the requirements.\n\nDocument the API - Use Quarto to create documentation for the API, including usage examples and any relevant information for developers.\nAutomate the process - Use LUA and PowerShell to automate the process of generating the API definition, client and server code, and documentation."
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#benefits",
    "href": "Content/Journal/posts/APIFirst/index.html#benefits",
    "title": "API First",
    "section": "Benefits",
    "text": "Benefits\nThe benefits of using the API First approach include:\n\n\n\n\n\n\n\nCategory\nQualities\n\n\n\n\nüßë‚Äçüíª Developer Experience & Productivity\n- Improved developer productivity and satisfaction  - Easier onboarding of new developers and teams  - Reduced errors and inconsistencies in the code  - Easier maintenance and updates to the API  - Improved versioning and backward compatibility of the API  - Better integration with DevOps and CI/CD processes\n\n\nüöÄ Development Efficiency & Technical Excellence\n- Faster development cycles  - More efficient integration with third-party services  - Consistent and well-defined APIs  - Better quality APIs  - Better support for multiple platforms and devices  - More effective monitoring and analytics of API usage  - More effective management of API lifecycle and governance\n\n\nüåê Business Alignment & Strategic Impact\n- Better alignment with business goals and requirements  - Increased flexibility and scalability of the application or service  - Enhanced ability to adapt to changing market and customer needs  - Greater overall success and impact of the application or service in the market  - Increased revenue and business opportunities through the API  - Better alignment with industry trends and best practices in API development\n\n\nü§ù Collaboration & Communication\n- Improved collaboration between teams  - Improved communication and transparency with stakeholders and customers\n\n\nüé® Innovation & Design\n- Greater innovation and creativity in API design  - Improved user experience\n\n\nüîí Trust, Security & Adoption\n- Enhanced security and compliance with industry standards  - Greater adoption and usage of the API by developers and users  - Enhanced reputation and credibility of the organization providing the API"
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#summary",
    "href": "Content/Journal/posts/APIFirst/index.html#summary",
    "title": "API First",
    "section": "Summary",
    "text": "Summary\nThe API First approach is a powerful way to improve the development process and create high-quality applications and services that are built around well-designed APIs. By prioritizing the design and implementation of APIs, developers can ensure that their applications are more efficient, flexible, and user-friendly. The tools and processes outlined in this document can help to streamline the API First approach and make it easier for developers to create and maintain high-quality APIs."
  },
  {
    "objectID": "Content/Journal/posts/2025-05-01-TestingAzureFunctions.html",
    "href": "Content/Journal/posts/2025-05-01-TestingAzureFunctions.html",
    "title": "Integration Testing Azure Functions",
    "section": "",
    "text": "Integration testing of azure functions is essential when demostrating that a service does what its supposed to do, and also responds in the way you expect. This post shows how you can construct a unit test, in this case Specflow, so that it excutes the function as a background process, so that you can test it using standard http calls.\nThis process uses the Azure Functions Core Tools cli interface https://go.microsoft.com/fwlink/?linkid=2174087 which should be installed. This should also be installed as part of the build process.\n  - task: PowerShell@2\n    inputs:\n      targetType: 'inline'\n      script: 'choco install azure-functions-core-tools -y'\nThis snippet demostrates how to instatiate the Azure function at the start of the unit test and kill it off after the test has completed. N.B the Directory Info object you pass is the directory of your azure function source code.\nnamespace ImageResizerTests\n{\n    [Binding]\n    public class ResizerSteps\n    {\n        private string request;\n        private IFlurlResponse response;\n        private static TemporaryAzureFunctionsApplication azfunc;\n\n        [BeforeTestRun(Order = 1)]\n        public static void Before()\n        {\n            var dirInfo = new DirectoryInfo(\"..\\\\..\\\\..\\\\..\\\\..\\\\src\\\\Application\\\\KnightFrank.Hub.Watermark\");\n            Console.WriteLine($\"Directory : {dirInfo.ToString()}\");\n\n            azfunc = TemporaryAzureFunctionsApplication.StartNewAsync(dirInfo).Result;\n        }\n\n        [AfterTestRun]\n        public static void After()\n        {\n            azfunc.DisposeAsync();\n        }\n\n        [When(@\"the URI is evaluated\")]\n        public void WhenTheURIIsEvaluated()\n        {\n            try\n            {\n                response = \"http://localhost:7071/api/Resize\"\n                    .WithHeader(\"Accept\", \"*/*\")\n                    .WithHeader(\"Content_Type\", \"application/json\")\n                    .AllowAnyHttpStatus()\n                    .PostJsonAsync(new { Size = \"SmallThumbnail\", InputUri = request }).Result;\n            }\n            catch(Exception ex)\n            {\n                Console.WriteLine(ex.Message);\n            }\n        }\n    }\n}\nCopy the following into its own file, as is.\nnamespace ImageResizerTests\n{\n    using Polly;\n    using Polly.Retry;\n    using System;\n    using System.Diagnostics;\n    using System.IO;\n    using System.Net.Http;\n    using System.Threading.Tasks;\n\n    public class TemporaryAzureFunctionsApplication : IAsyncDisposable\n    {\n        private readonly Process _application;\n        private static readonly HttpClient HttpClient = new HttpClient();\n\n        private TemporaryAzureFunctionsApplication(Process application)\n        {\n            _application = application;\n        }\n\n        public static async Task&lt;TemporaryAzureFunctionsApplication&gt; StartNewAsync(DirectoryInfo projectDirectory)\n        {\n            int port = 7071;\n            Process app = StartApplication(port, projectDirectory);\n            await WaitUntilTriggerIsAvailableAsync($\"http://localhost:{port}/\");\n\n            return new TemporaryAzureFunctionsApplication(app);\n        }\n\n        private static Process StartApplication(int port, DirectoryInfo projectDirectory)\n        {\n            var appInfo = new ProcessStartInfo(\"func\", $\"start --port {port} --prefix bin/Debug/net8.0\")\n            {\n                UseShellExecute = false,\n                CreateNoWindow = false,\n                WorkingDirectory = projectDirectory.FullName\n            };\n\n            var app = new Process { StartInfo = appInfo };\n            app.Start();\n            return app;\n        }\n\n        private static async Task WaitUntilTriggerIsAvailableAsync(string endpoint)\n        {\n            AsyncRetryPolicy retryPolicy =\n                    Policy.Handle&lt;Exception&gt;()\n                          .WaitAndRetryForeverAsync(index =&gt; TimeSpan.FromMilliseconds(500));\n\n            PolicyResult&lt;HttpResponseMessage&gt; result =\n                await Policy.TimeoutAsync(TimeSpan.FromSeconds(30))\n                            .WrapAsync(retryPolicy)\n                            .ExecuteAndCaptureAsync(() =&gt; HttpClient.GetAsync(endpoint));\n\n            if (result.Outcome == OutcomeType.Failure)\n            {\n                throw new InvalidOperationException(\n                    \"The Azure Functions project doesn't seem to be running, \"\n                    + \"please check any build or runtime errors that could occur during startup\");\n            }\n        }\n\n        public ValueTask DisposeAsync()\n        {\n            if (!_application.HasExited)\n            {\n                _application.Kill(entireProcessTree: true);\n            }\n\n            _application.Dispose();\n            return ValueTask.CompletedTask;\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/2024-06-14-DataAnnotationVsFluentValidation.html",
    "href": "Content/Journal/posts/2024-06-14-DataAnnotationVsFluentValidation.html",
    "title": "Data Annotation Vs FluentValidation",
    "section": "",
    "text": "Data Annotation\n\nFor\n\nIs SOLID\n\nValidations are placed in the model, next to the defintions\n\nPart of standard library\n\nAgainst\n\nValidations are not Rules, and cannot span members\nOnly represents simple validations in the openapi schema\n\n\n\n\nFluent Validation\n\nFor\n\nRules can span multiple methods\n\nAgainst\n\nNot SOLID\n\nValidations and Rules are specified in a separate file\n\nOnly represents simple validations in the openapi schema\nAdditonal setup\n\n\n\n\nConclusion\nI‚Äôve changed my mind, which is not uncommon. Where your class is interfacing with a different system such as being used in a API interface, then you need to convey to the user the maximum amount of information, about what is or isnt valid, so the validation details have to be encoded in the model. Where Data Annotations is lacking, E.g. OneOf, AnyOf, then you should build a custom validator to fill the gap.\nFluent Validation should be used internally, as it not only can apply general validations, it can also encode groups of validations into rules and apply them under different conditions.\n\n\nRef\nThis is a interesting read on validators, and also the implementation of custom validators. https://weblogs.asp.net/ricardoperes/net-8-data-annotations-validation"
  },
  {
    "objectID": "Content/Journal/posts/2023-09-05-CastToReflectedType.html",
    "href": "Content/Journal/posts/2023-09-05-CastToReflectedType.html",
    "title": "Cast To a Reflected Type",
    "section": "",
    "text": "How many times do you find yourselves not knowing the type of a object, and wanting to cast it.\nIn my senario, I was working in a table storage library, and I was trying to merge json objects. The issue being, in the library you have no visibility of the data structure, so you have to use reflection.\nentity is the object to be merged. mergedJson is the merged json object.\nThe issue here is the Update method takes a object of TableEntity. If you simply map to a table entity then your object will be missing the data. So in this case you get the type of the passed ‚Äòentity‚Äô in this case AddressDto after you have merged the json you need to deserailise it back into a object. The issue is the deserialise object is of type object and not TableEntity. The ObjectExtensions code was obtain, and performs the cast correctly and creates a eek object of type AddressDto, which is good to update.\n    Type objectType = entity.GetType();\n    var merged2 = JsonConvert.DeserializeObject(mergedJson, objectType);\n    var eek = ObjectExtensions.CastToReflected(merged2, objectType);\n    res = Update(eek);\n\n\n    public static class ObjectExtensions\n    {\n        public static T CastTo&lt;T&gt;(this object o) =&gt; (T)o;\n\n        public static dynamic CastToReflected(this object o, Type type)\n        {\n            var methodInfo = typeof(ObjectExtensions).GetMethod(nameof(CastTo), BindingFlags.Static | BindingFlags.Public);\n            var genericArguments = new[] { type };\n            var genericMethodInfo = methodInfo?.MakeGenericMethod(genericArguments);\n            return genericMethodInfo?.Invoke(null, new[] { o });\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/2023-05-01-Package Development.html",
    "href": "Content/Journal/posts/2023-05-01-Package Development.html",
    "title": "How to build Cloud components without any Azure Experience",
    "section": "",
    "text": "How to build Cloud components without any Azure Experience\nIt is a bit of a misnomer; you need people with cloud experience.\nSorry I am not going to say anything new here, but it just might be new here.\nIn typical application code is the same no matter what platform you develop on, you have an interface to the world, domain/business rules, and a data interface. Only the interface to the world needs to be managed by someone with more specialised knowledge. You could divide up the code and create two projects, one for the API, specialised to the environment it will be deployed, and one for the domain/business logic. This would enable you to simply wrap the domain/business rules in its own NuGet package and use normal developers to create them and allow them to use tradition tools to build and evaluate them.\n\nThe previous solution would be fine, but it still leaves scope for organically grown code, which encapsulates complex business rules. The build test burden could be reduced further by making the projects even smaller. Reducing their complexity and increasing their automatic testability. In this case the Domain logic could be reduced to encoding process flow only, and business rules evaluation could be managed by a more suitable tool. Also if the data store interface is not specific to this application but a general interface used by many applications, then it also could make use of package management.\n\nThe questions, I keep hearing when there is a proposed change in development strategy, such as ‚Ä¢ How do we keep up with package versioning ‚Ä¢ How do we make sure all applications are using the same package versions\nThe simple answer is developers should use good development practices. 1. At the start of any new development, they should a. Run the unit tests b. Get the latest packages c.¬†Run the unit tests d.¬†Fix any issues caused by the updated packages e. Run the unit tests 2. Create new code 3. Create new tests a. Run the unit tests 4. Create a check in note 5. Check in The only project where package version matters is the API project, as that what is presented to the user."
  },
  {
    "objectID": "Content/Journal/posts/2023-01-20-AzureFunctionSQLChangeTracking.html",
    "href": "Content/Journal/posts/2023-01-20-AzureFunctionSQLChangeTracking.html",
    "title": "SQL Change Tracking",
    "section": "",
    "text": "As of novemebr 2022, Azure Functions have aquired a new trigger, SQL. With some minor changes to a sql database configuration, a Azure Function can capture sql trigger events and report on them. A useful implementation of this would be to feed data into stream analytics and have a constantly updated dashboard.\n\nALTER DATABASE [Hub.Dev]\nSET CHANGE_TRACKING = ON\n(CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON);\n\nALTER TABLE [dbo].[WebEnquiry]\nENABLE CHANGE_TRACKING;\n\n\n    public class WebEnquiry\n    {\n        public string ReferenceId { get; set; }\n        public string FirstName { get; set; }\n        public string LastName { get; set; }\n        public string Type { get; set; }\n        public string Source { get; set; }\n        public string AdvertPostCode { get; set; }\n        public string PropertyBedrooms { get; set; }\n        public string OfficeId { get; set; }\n        public string PropertyReferenceNumber { get; set; }\n        public Guid AssetId { get; set; }\n    }\n\n\n\n\n    public static class ToDoTrigger\n    {\n        [FunctionName(\"WebEnquiry\")]\n        public static void Run(\n            [SqlTrigger(\"[Hub.Dev].[dbo].[WebEnquiry]\", ConnectionStringSetting = \"SqlConnectionString\")]\n            IReadOnlyList&lt;SqlChange&lt;WebEnquiry&gt;&gt; enquires,\n            ILogger logger)\n        {\n            foreach (SqlChange&lt;WebEnquiry&gt; enquiry in enquires)\n            {\n                WebEnquiry item = enquiry.Item;\n                logger.LogInformation($\"ReferenceId: {item.ReferenceId}, Source: {item.Source}, AdvertPostCode: {item.AdvertPostCode}, PropertyReferenceNumber: {item.PropertyReferenceNumber}\");\n            }\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/2023-01-20-AzureFunctionSQLChangeTracking.html#sql-table-class",
    "href": "Content/Journal/posts/2023-01-20-AzureFunctionSQLChangeTracking.html#sql-table-class",
    "title": "SQL Change Tracking",
    "section": "",
    "text": "public class WebEnquiry\n    {\n        public string ReferenceId { get; set; }\n        public string FirstName { get; set; }\n        public string LastName { get; set; }\n        public string Type { get; set; }\n        public string Source { get; set; }\n        public string AdvertPostCode { get; set; }\n        public string PropertyBedrooms { get; set; }\n        public string OfficeId { get; set; }\n        public string PropertyReferenceNumber { get; set; }\n        public Guid AssetId { get; set; }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/2023-01-20-AzureFunctionSQLChangeTracking.html#azure-function-sql-trigger",
    "href": "Content/Journal/posts/2023-01-20-AzureFunctionSQLChangeTracking.html#azure-function-sql-trigger",
    "title": "SQL Change Tracking",
    "section": "",
    "text": "public static class ToDoTrigger\n    {\n        [FunctionName(\"WebEnquiry\")]\n        public static void Run(\n            [SqlTrigger(\"[Hub.Dev].[dbo].[WebEnquiry]\", ConnectionStringSetting = \"SqlConnectionString\")]\n            IReadOnlyList&lt;SqlChange&lt;WebEnquiry&gt;&gt; enquires,\n            ILogger logger)\n        {\n            foreach (SqlChange&lt;WebEnquiry&gt; enquiry in enquires)\n            {\n                WebEnquiry item = enquiry.Item;\n                logger.LogInformation($\"ReferenceId: {item.ReferenceId}, Source: {item.Source}, AdvertPostCode: {item.AdvertPostCode}, PropertyReferenceNumber: {item.PropertyReferenceNumber}\");\n            }\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/2022-11-10-ServiceApplicationDevelopment.html",
    "href": "Content/Journal/posts/2022-11-10-ServiceApplicationDevelopment.html",
    "title": "Service Application Development",
    "section": "",
    "text": "Description\nFor to long there has been little thought about the consequences of changing code that is not strictly associated with the task assigned, this has been made possible by the solution containing all the code for the entire application. This had been the practice years ago but has been widely dismissed due the revolution of agile design and coding. Moving forward with a service-oriented application design, the code should be split along functional lines, where each function becomes a self-contained package which can be reused. Packaging up functionality has been common practice for several years as it promotes * Specialisation, forcing the components to do ‚Äòone thing and one thing only‚Äô. * Simpler testing. * Versioning * Ease of upgrade\n\n\n\nConsequences\nThis break up of the monolith is not without its drawbacks, at least in the begining. The usual complaints are :- * Code is being duplicated. * Yes, it will be, until all the relervant functionality has been encapsulated. * Paying again, for no additional functionality. * Yes, we will be, For historical reasons of a lack of control on our part. * Or a philosophy of keep the hopper full and pushing ever forward with business functionality, and forgetting any technical requirements."
  },
  {
    "objectID": "Content/Journal/posts/2022-07-20-AzurePipelineGitCommit.html",
    "href": "Content/Journal/posts/2022-07-20-AzurePipelineGitCommit.html",
    "title": "Azure Pipeline Git Commit",
    "section": "",
    "text": "If you are trying to use git within a devops pipeline then you will need to enable the pipeline the bility to commit.\nYou need to grant the permissions to Project Collection Build Service (account name):\n\n\n\nPuml"
  },
  {
    "objectID": "Content/Journal/posts/2020-06-19-AzureFunctionsLogging.html",
    "href": "Content/Journal/posts/2020-06-19-AzureFunctionsLogging.html",
    "title": "Azure Functions Logging",
    "section": "",
    "text": "Azure Functions include a built in ILogger so you can fulfill all your logging needs, There is one issue, if you try to use the Ilogger in any of you subsequent classes the ILogger seems to be null.\n\nExample\nnamespace MyFunctionApp\n{\n    public class MyFunctionClass\n    {\n        private readonly ILogger logger;\n\n        // DOESN'T WORK!\n        public MyFunctionClass(ILogger logger)\n        {\n              this.logger = logger;\n        }\n\n        [FunctionName(\"MyFunctionName\")]\n        public async Task&lt;IActionResult&gt; MyFunctionMethod(\n            [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)]\n            HttpRequest req,\n            ILogger log) // WORKS!\n       {\n           ...\n       }\n    }\n}\n\n\nSolution\nThe solution is simple but a little annoying :( Instead of using the ILogger use ILogger&lt;&gt; instead putting the class name as the type.\npublic class MyFunctionClass\n    {\n        private readonly ILogger logger;\n\n        // WORKS!\n        public MyFunctionClass(ILogger&lt;MyFunctionClass&gt; logger)\n        {\n              this.logger = logger;\n        }\n        ...\n    }\nThere is an additional gotcha, which has been raised to Microsoft as a bug, and that is you need to specify your NameSpaces that uses the ILogger in the hosts.json file\n{\n    \"version\": \"2.0\",\n    \"logging\": {\n        \"logLevel\": {\n            \"MyFunctionApp\": \"Trace\"\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/2019-12-01-Item.html",
    "href": "Content/Journal/posts/2019-12-01-Item.html",
    "title": "Online Spreadsheet Database",
    "section": "",
    "text": "Description\nA while a go I was working on a spreadsheet style applicaiton for the web, and we needed a method to store the 2 dimentional data in a 1 deminentional database. This was the resultant database structure, which is a linked list, which allowed both data and formula‚Äôs to included. There was either a template structure so you could load predefined sheets. I‚Äôll get round to recreating the web page‚Ä¶. eventually.\n\n\n\nGreenDog\n\n\n\nTables\n\nR2Data\nR2Item\nR3Data\nR6TemplateItem\n\n\nItemId, ParentId, GdDdId, Format, DivisionId, Depth, ProcessId, Attribute\nCREATE TABLE R6TemplateItem (\n       R6TemplateItemId INT NOT NULL IDENTITY PRIMARY KEY\n     , R6ParentId INT\n     , R6GdDdId VARCHAR(50)\n     , R6Depth INT\n     , R6Lineage VARCHAR(100)\n     , R6Format VARCHAR(50)\n     , R6Row INT\n     , R6Col INT\n     , R6Value VARCHAR(250)\n     , R6SeqNo INT\n     , R6ItemLinkId INT\n     , CONSTRAINT FK_R6TemplateItem_1 FOREIGN KEY (R6ParentId)\n                  REFERENCES R6TemplateItem (R6TemplateItemId)\n);\n\nCREATE TABLE R5Division (\n       R5DivisionId INT NOT NULL IDENTITY PRIMARY KEY\n     , R5ProcessId INT\n     , R5CountryId INT\n     , R5ItemId INT\n     , R5Name VARCHAR(50)\n     , R5RecipientId INT\n     , R5Seq INT\n     , R5Creator BIT\n     , R5Valid BIT\n     , R5Deleted BIT\n     , CONSTRAINT FK_R5Division_1 FOREIGN KEY (R5ProcessId)\n                  REFERENCES Q4Process (Q4ProcessId)\n     , CONSTRAINT FK_R5Division_3 FOREIGN KEY (R5CountryId)\n                  REFERENCES P3Country (P3CountryId)\n     , CONSTRAINT FK_R5Division_2 FOREIGN KEY (R5ItemId)\n                  REFERENCES R2Item (R2ItemId) ON DELETE CASCADE\n);\n\nCREATE TABLE R3Data (\n       R3DataId INT NOT NULL IDENTITY PRIMARY KEY\n     , R3ItemId INT NOT NULL\n     , R3DivisionId INT\n     , R3Value VARCHAR(250)\n     , R3ItemLinkId INT\n     , R3TaxPackageRef VARCHAR(50)\n     , R3ProcessId INT\n     , R3Attribute INT\n     , CONSTRAINT FK_R4Data_4 FOREIGN KEY (R3ItemId)\n                  REFERENCES R2Item (R2ItemId) ON DELETE CASCADE\n     , CONSTRAINT FK_R3Data_3 FOREIGN KEY (R3DivisionId)\n                  REFERENCES R5Division (R5DivisionId)\n);\n\nCREATE TABLE R1IpackTemplate (\n       R1IpackTemplateId INT NOT NULL IDENTITY PRIMARY KEY\n     , R1Name VARCHAR(20)\n     , R1TemplateItemId INT\n     , R1Deleted BIT\n     , CONSTRAINT FK_R1IpackTemplate_2 FOREIGN KEY (R1TemplateItemId)\n                  REFERENCES R6TemplateItem (R6TemplateItemId) ON DELETE CASCADE\n);\n\n\nGet Item Tree\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_getItemTree'\n       AND    type = 'P')\n    DROP PROCEDURE gd_getItemTree\nGO\n\nCREATE PROCEDURE gd_getItemTree\n    @ItemId int = NULL,\n    @DivisionId int = NULL\nAS\n\n    IF @ItemId IS NOT NULL and @DivisionId = -1\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            a.R2GdDdId AS GdDdId,\n            a.R2Format AS Format,\n            b.R3Value AS Value,\n            b.R3DivisionId AS DivisionId,\n            a.R2Depth AS Depth,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b\n        WHERE a.R2Lineage LIKE '%'+cast( @ItemId AS varchar(10)) +'%'\n            and a.R2GdDdId IN ( SELECT R6GdDdId FROM R6TemplateItem WHERE R6GdDdId = a.R2GdDdId and R6ParentId = 0)\n            and b.R3ItemId = a.R2ItemId\n            and b.R3DivisionId IS NULL\n        ORDER BY a.R2SeqNo\n    END\n    ELSE IF @ItemId IS NOT NULL and @DivisionId IS NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            a.R2GdDdId AS GdDdId,\n            a.R2Format AS Format,\n            b.R3Value AS Value,\n            b.R3DivisionId AS DivisionId,\n            a.R2Depth AS Depth,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b\n        WHERE a.R2Lineage LIKE '%'+cast( @ItemId AS varchar(10)) +'%'\n            and a.R2GdDdId IN ( SELECT R6GdDdId FROM R6TemplateItem WHERE R6GdDdId = a.R2GdDdId and R6ParentId = 0)\n            and b.R3ItemId = a.R2ItemId\n            and (b.R3DivisionId IS NULL\n            or b.R3DivisionId = (\n                SELECT R5DivisionId\n                FROM R5Division\n                WHERE b.R3DivisionId IS NOT NULL\n                and R5DivisionId = b.R3DivisionId\n                and R5Deleted IS NULL))\n        ORDER BY a.R2SeqNo\n    END\n    ELSE IF @ItemId IS NOT NULL and @DivisionId IS NOT NULL\n    BEGIN\n        -- Make sure its not marked as deleted.\n        IF( SELECT R5DivisionId FROM R5Division WHERE R5DivisionId = @DivisionId and R5Deleted IS NULL) &gt; 0\n        BEGIN\n            SELECT a.R2ItemId AS ItemId,\n                a.R2ParentId AS ParentId,\n                a.R2GdDdId AS GdDdId,\n                a.R2Format AS Format,\n                b.R3Value AS Value,\n                b.R3DivisionId AS DivisionId,\n                a.R2Depth AS Depth,\n                b.R3ProcessId AS ProcessId,\n                b.R3Attribute AS Attribute\n            FROM R2Item a, R3Data b\n            WHERE a.R2Lineage LIKE '%'+cast( @ItemId AS varchar(10)) +'%'\n                and a.R2GdDdId IN ( SELECT R6GdDdId FROM R6TemplateItem WHERE R6GdDdId = a.R2GdDdId and R6ParentId = 0)\n                and b.R3ItemId = a.R2ItemId\n                and (b.R3DivisionId = @DivisionId or b.R3DivisionId IS NULL)\n            ORDER BY a.R2SeqNo\n        END\n    END\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n-- EXECUTE gd_getItemTree 1803, 167\nGO\n\n\nGet Item\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_getItem'\n       AND    type = 'P')\n    DROP PROCEDURE gd_getItem\nGO\n\nCREATE PROCEDURE gd_getItem\n    @ItemId int = NULL,\n    @DivisionId int = NULL\nAS\n\n    DECLARE @TemplateParentId int;\n    SET @TemplateParentId = (SELECT a.R6TemplateItemId FROM R6TemplateItem a\n                WHERE a.R6GdDdId = (SELECT a.R2GdDdId AS GdDdId FROM R2Item a WHERE a.R2ItemId = @ItemId))\n\n    IF @ItemId IS NOT NULL and @DivisionId IS NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            b.R3DataId AS DataId,\n            a.R2GdDdId AS GdDdId,\n            c.R6Format AS Format,\n            c.R6Row AS Row,\n            c.R6Col AS Col,\n            a.R2SeqNo AS SeqNo,\n            c.R6SeqNo AS TemplateSeqNo,\n            b.R3DivisionId AS DivisionId,\n            b.R3Value AS Value,\n            b.R3ItemLinkId AS ItemLinkId,\n            b.R3TaxPackageRef AS TaxPackageRef,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b, R6TemplateItem c\n        WHERE ( a.R2ItemId = @ItemId or a.R2ParentId = @ItemId\n            or a.R2ParentId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId))\n            or a.R2ItemId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId)))\n            and b.R3ItemId = a.R2ItemId\n            and a.R2GdDdId = c.R6GdDdId\n            and (c.R6ParentId = @TemplateParentId\n                or c.R6TemplateItemId = @TemplateParentId)\n            order by c.R6Row, c.R6Col\n    END\n    IF @ItemId IS NOT NULL and @DivisionId IS NOT NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            b.R3DataId AS DataId,\n            a.R2GdDdId AS GdDdId,\n            c.R6Format AS Format,\n            c.R6Row AS Row,\n            c.R6Col AS Col,\n            a.R2SeqNo AS SeqNo,\n            c.R6SeqNo AS TemplateSeqNo,\n            b.R3DivisionId AS DivisionId,\n            b.R3Value AS Value,\n            b.R3ItemLinkId AS ItemLinkId,\n            b.R3TaxPackageRef AS TaxPackageRef,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b, R6TemplateItem c\n        WHERE ( a.R2ItemId = @ItemId or a.R2ParentId = @ItemId\n            or a.R2ParentId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId))\n            or a.R2ItemId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId)))\n            and b.R3ItemId = a.R2ItemId\n            and a.R2GdDdId = c.R6GdDdId\n            and (c.R6ParentId = @TemplateParentId\n                or c.R6TemplateItemId = @TemplateParentId)\n            and ( b.R3DivisionId = @DivisionId or b.R3DivisionId IS NULL)\n            order by c.R6Row, c.R6Col\n    END\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--EXECUTE gd_getItem 74, 165\n--GO\nselect * from R2Item\n--select * from R6TemplateItem\n\n\nSet Item\n-- =============================================\n-- Create procedure gd_setItem\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_setItem'\n       AND    type = 'P')\n    DROP PROCEDURE gd_setItem\nGO\n\nCREATE PROCEDURE gd_setItem\n    @ItemId int = NULL,\n    @ParentId int = NULL,\n    @GdDdId varchar(30) = NULL,\n    @Format varchar(50) = NULL,\n    @Row int = NULL,\n    @Col int = NULL,\n    @SeqNo int = NULL,\n    @DivisionId int = NULL,\n    @Value varchar(250) = NULL,\n    @ItemLinkId int = NULL,\n    @TaxPackageRef varchar(50) = NULL,\n    @ProcessId int = NULL,\n    @Attribute int = NULL\n\nAS\nSET ROWCOUNT 0\nDECLARE @ErrorMsgID int\nDECLARE @DataListId int\nDECLARE @div int\nDECLARE @DataId int\n\n-- A Item conists of two parts \n-- 1, its navigation, specified by the R2Item table\n-- 2, its data, specified by the R3Data table\n-- There can only ever be one navigation record \n-- There can be many Data records.\n-- A Item can be uniquly identified by its @ParentId, @GdDdId, @SeqNo and @DivisionId\n\nBEGIN TRAN T1\n\n    IF @ItemId IS NULL \n    BEGIN\n        -- Does a Item exist for this record\n        SET @ItemId = ( SELECT R2ItemId \n                        FROM R2Item \n                        WHERE R2ParentId = @ParentId and R2GdDdId = @GdDdId and R2SeqNo = @SeqNo)\n    END\n                \n    IF @ItemId IS NULL or @ItemId = 0 or @ItemId = -1\n    BEGIN\n        -- Create a new item record\n        -- Create a data record\n\n        DECLARE @depth int;\n        DECLARE @lineage varchar(100);\n\n        SELECT @depth=R2Depth, @lineage=R2Lineage FROM R2Item where R2ItemId = @ParentId\n        SET @depth = @depth + 1;\n        SET @lineage = @lineage + LTRIM(Str(@ParentId, 6,0)) + '.'\n\n        INSERT R2Item ( R2ParentId, R2GdDdId, R2Format, R2Row, R2Col, R2SeqNo, R2Depth, R2Lineage)\n        VALUES( @ParentId, @GdDdId, @Format, @Row, @Col, @SeqNo, @depth, @lineage)\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_setItem: Unable to Insert a new Navigation Item',10,1)\n            RETURN -1\n        END\n\n        SET @ItemId = @@IDENTITY;\n\n        INSERT R3Data( R3ItemId, R3DivisionId, R3Value, R3ItemLinkId, R3TaxPackageRef, R3ProcessId, R3Attribute)\n        VALUES( @ItemId, @DivisionId, @Value, @ItemLinkId, @TaxPackageRef, @ProcessId, @Attribute)\n        \n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_setItem: Unable to Insert a new Data Item',10,1)\n            RETURN -1\n        END\n\n\n        -- Update Depth and lineage\n        IF (SELECT COUNT( R2ItemId) FROM R2Item) = 1\n        BEGIN\n            UPDATE R2Item\n            SET R2Depth = 0 , R2Lineage = '.'\n            WHERE R2ItemId = 1\n        END\n    END\n    ELSE IF @ItemId IS NOT NULL\n    BEGIN\n        -- Update the item record\n\n        UPDATE R2Item\n        SET R2ParentId = @ParentId\n            , R2GdDdId = @GdDdId\n            , R2Format = @Format\n            , R2Row = @Row\n            , R2Col = @Col\n            , R2SeqNo = @SeqNo\n        WHERE R2ItemId = @ItemId\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_setItem: Unable to Update a Item',10,1)\n            RETURN -1\n        END\n \n        -- Does the data record exist\n\n        IF @DivisionId IS NULL\n        BEGIN\n            SET @DataId = ( SELECT R3DataId \n                            FROM R3Data\n                            WHERE R3ItemId = @ItemId and R3DivisionId is null)\n        END\n        ELSE\n        BEGIN\n            SET @DataId = ( SELECT R3DataId \n                            FROM R3Data\n                            WHERE R3ItemId = @ItemId and R3DivisionId = @DivisionId)\n        END\n\n        IF @DataId IS NULL\n        BEGIN\n            -- Create a new Data Record\n\n            INSERT R3Data( R3ItemId, R3DivisionId, R3Value, R3ItemLinkId, R3ProcessId, R3Attribute)\n            VALUES( @ItemId, @DivisionId, @Value, @ItemLinkId, @ProcessId, @Attribute)\n\n            SET @ErrorMsgID =@@ERROR\n            IF @ErrorMsgID &lt;&gt;0\n            BEGIN\n                ROLLBACK TRAN T1\n                RAISERROR ('gd_setItem: Unable to Insert a new Data Item',10,1)\n                RETURN -1\n            END\n        END\n        ELSE\n        BEGIN\n            -- Update the Data Record\n\n            UPDATE R3Data\n            SET R3DivisionId = @DivisionId\n                , R3Value = @Value\n                , R3ItemLinkId = @ItemLinkId\n                , R3TaxPackageRef = @TaxPackageRef\n                , R3ProcessId = @ProcessId\n                , R3Attribute = @Attribute\n            WHERE R3DataId = @DataId\n\n            SET @ErrorMsgID =@@ERROR\n            IF @ErrorMsgID &lt;&gt;0\n            BEGIN\n                ROLLBACK TRAN T1\n                RAISERROR ('gd_setItem: Unable to Update a Data Item',10,1)\n                RETURN -1\n            END\n        END\n    END\n    ELSE\n    BEGIN\n        ROLLBACK TRAN T1\n        RAISERROR ('gd_setItem: Should not of got here',10,1)\n        RETURN -1\n    END\n\n    COMMIT TRAN T1\n    RETURN @ItemId\nGO  \n\n\nDelete Item\n-- =============================================\n-- Create procedure gd_delItem\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_delItem' \n       AND    type = 'P')\n    DROP PROCEDURE gd_delItem\nGO\n\n\nCREATE PROCEDURE gd_delItem \n    @ItemId int = NULL\nAS\n    DECLARE @ErrorMsgID int\n\n    -- Delete a item and all the rows that refer to it.\n\n    BEGIN TRAN T1\n        \n        -- Delete children of this Item\n        DELETE \n        FROM R2Item \n        WHERE R2Lineage like '%.'+cast( @ItemId AS varchar(10)) +'.%'\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_delItem: Unable to Delete Item',10,1)\n            RETURN -1\n        END\n\n        -- Clear any links to this Item\n        UPDATE R3Data\n        SET R3ItemLinkId = NULL\n        WHERE R3ItemLinkId = @ItemId\n\n\n        -- Delete this Item\n        DELETE \n        FROM R2Item\n        WHERE R2ItemId = @ItemId\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_delItem: Unable to Delete Item',10,1)\n            RETURN -1\n        END\n\n    COMMIT TRAN T1\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--gd_delItem 13\n\n\nDelete Item Child\n-- =============================================\n-- Create procedure gd_delItemChild\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_delItemChild' \n       AND    type = 'P')\n    DROP PROCEDURE gd_delItemChild\nGO\n\n\nCREATE PROCEDURE gd_delItemChild \n    @ItemId int = NULL\nAS\n    DECLARE @ErrorMsgID int\n\n    -- Delete a item and all the rows that refer to it.\n\n    BEGIN TRAN T1\n\n        DELETE\n        FROM R2Item \n        WHERE R2ParentId = @ItemId\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_delItem: Unable to Delete Items Children',10,1)\n            RETURN -1\n        END\n\n    COMMIT TRAN T1\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--gd_delItem 10067\n\n\nGet TemplateItem\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_getTemplateItem' \n       AND    type = 'P')\n    DROP PROCEDURE gd_getTemplateItem\nGO\n\nCREATE PROCEDURE gd_getTemplateItem \n    @ItemId int = NULL\nAS\n\n    IF @ItemId IS NOT NULL\n    BEGIN\n        SELECT a.R6TemplateItemId AS ItemId, a.R6ParentId AS ParentId, a.R6GdDdId AS GdDdId,\n            a.R6Format AS Format, a.R6Row AS Row, a.R6Col AS Col, a.R6SeqNo AS SeqNo,\n            a.R6Value AS Value, a.R6ItemLinkId AS ItemLinkId\n        FROM R6TemplateItem a\n        WHERE (a.R6TemplateItemId = ( SELECT R6TemplateItemId FROM R6TemplateItem WHERE R6ParentId = 0 \n                and R6GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId))\n            or a.R6ParentId = ( SELECT R6TemplateItemId FROM R6TemplateItem WHERE R6ParentId = 0 \n                and R6GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId)))\n        ORDER BY a.R6ParentId, a.R6Row, a.R6Col\n    END\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\nEXECUTE gd_getTemplateItem 10\nGO\n\n\nGet Data Item\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_getDataItem'\n       AND    type = 'P')\n    DROP PROCEDURE gd_getDataItem\nGO\n\nCREATE PROCEDURE gd_getDataItem\n    @ItemId int = NULL,\n    @DivisionId int = NULL\nAS\n\n\n    IF @ItemId IS NOT NULL and @DivisionId IS NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            a.R2GdDdId AS GdDdId,\n            a.R2Format AS Format,\n            a.R2Row AS Row,\n            a.R2Col AS Col,\n            a.R2SeqNo AS SeqNo,\n        a.R2Depth AS Depth,\n            b.R3DivisionId AS DivisionId,\n            b.R3Value AS Value,\n            b.R3ItemLinkId AS ItemLinkId,\n            b.R3TaxPackageRef AS TaxPackageRef,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute,\n            b.R3DataId AS DataId\n        FROM R2Item a, R3Data b\n        WHERE (a.R2ItemId = @ItemId or a.R2ParentId = @ItemId)\n        and b.R3ItemId = a.R2ItemId\n    and (b.R3DivisionId IS NULL\n    or b.R3DivisionId = (\n            SELECT R5DivisionId\n            FROM R5Division\n            WHERE b.R3DivisionId IS NOT NULL\n            and R5DivisionId = b.R3DivisionId\n            and R5Deleted IS NULL))\n        ORDER BY a.R2ParentId,a.R2GdDdId,a.R2SeqNo\n    END\n    ELSE IF @ItemId IS NOT NULL and @DivisionId IS NOT NULL\n    BEGIN\n    -- Make sure its not marked as deleted.\n    IF( SELECT R5DivisionId FROM R5Division WHERE R5DivisionId = @DivisionId and R5Deleted IS NULL) &gt; 0\n    BEGIN\n            SELECT a.R2ItemId AS ItemId,\n                a.R2ParentId AS ParentId,\n                b.R3DataId AS DataId,\n                a.R2GdDdId AS GdDdId,\n                a.R2Format AS Format,\n                a.R2Row AS Row,\n                a.R2Col AS Col,\n                a.R2SeqNo AS SeqNo,\n                a.R2Depth AS Depth,\n                b.R3DivisionId AS DivisionId,\n                b.R3Value AS Value,\n                b.R3ItemLinkId AS ItemLinkId,\n                b.R3TaxPackageRef AS TaxPackageRef,\n                b.R3ProcessId AS ProcessId,\n                b.R3Attribute AS Attribute,\n                b.R3DataId AS DataId\n               FROM R2Item a, R3Data b\n            WHERE (a.R2ItemId = @ItemId or a.R2ParentId = @ItemId)\n            and b.R3ItemId = a.R2ItemId\n        and (b.R3DivisionId = @DivisionId or b.R3DivisionId IS NULL)\n            ORDER BY a.R2ParentId,a.R2GdDdId,a.R2SeqNo\n    END\n    END\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\nEXECUTE gd_getDataItem 1803, 167\nGO\n\n\nSet Template Item\n-- =============================================\n-- Create procedure gd_setTemplateItem\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_setTemplateItem' \n       AND    type = 'P')\n    DROP PROCEDURE gd_setTemplateItem\nGO\n\nCREATE PROCEDURE gd_setTemplateItem \n    @ItemId int = NULL,\n    @ParentId int = NULL,\n    @GdDdId varchar(30) = NULL,\n    @Format varchar(50) = NULL,\n    @Row int = NULL, \n    @Col int = NULL,\n    @SeqNo int = NULL,\n    @DivisionId int = NULL,\n    @Value varchar(50) = NULL,\n    @ItemLinkId int = NULL\nAS\n    SET ROWCOUNT 0\n    DECLARE @ErrorMsgID int\n    DECLARE @DataListId int\n    DECLARE @div int\n\n    BEGIN TRAN Template\n\n    IF @ItemId IS NULL\n    BEGIN\n        INSERT R6TemplateItem ( R6ParentId, R6GdDdId, R6Format, R6Row, R6Col, R6SeqNo, R6Value, R6ItemLinkId ) \n        VALUES( @ParentId, @GdDdId, @Format, @Row, @Col, @SeqNo, @Value, @ItemLinkId)\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN Template\n            RAISERROR ('gd_setTemplateItem: Unable to Insert a new Item',10,1)\n            RETURN -1\n        END\n\n        SET @ItemId = @@IDENTITY;\n\n        -- Update Depth and lineage\n        IF (SELECT COUNT( R6TemplateItemId) FROM R6TemplateItem) = 1\n        BEGIN\n            UPDATE R6TemplateItem\n            SET R6Depth = 0 , R6Lineage = '.'\n            WHERE R6TemplateItemId = 1\n        END\n\n        WHILE EXISTS (SELECT * FROM R6Item WHERE R6Depth IS NULL)\n        UPDATE T SET T.R6Depth = P.R6Depth +1, \n                T.R6Lineage = P.R2Lineage + LTRIM(Str(T.R6ParentId, 6,0)) + '.'\n        FROM R2Item As T\n            INNER JOIN R6Item AS P on (T.R6ParentId=P.R6ItemId)\n        WHERE P.R2Depth &gt;=0\n            AND P.R6Lineage IS NOT NULL\n            AND T.R6Depth IS NULL\n\n    END\n    ELSE IF @ItemId IS NOT NULL\n    BEGIN\n        UPDATE R6TemplateItem\n        SET R6ParentId = @ParentId, R6GdDdId = @GdDdId, \n            R6Format = @Format, R6Row = @Row, R6Col = @Col, R6SeqNo = @SeqNo, R6Value = @Value, R6ItemLinkId = @ItemLinkId\n        WHERE R6TemplateItemId = @ItemId\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN Template\n            RAISERROR ('gd_setTemplateItem: Unable to Update a Item',10,1)\n            RETURN -1\n        END\n    \n\n    END\n    ELSE\n    BEGIN\n        ROLLBACK TRAN Template\n        RAISERROR ('gd_setTemplateItem: Should not of got here',10,1)\n        RETURN -1\n    END\n\n    COMMIT TRAN Template\n\n    -- SET the OUTPUT Parameter\n--  SET @RTN = @ItemId;\n\n--  EXECUTE gd_getTemplateItem @ItemId;\n    RETURN @ItemId\n\nGO\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--gd_setTemplateItem @ParentId = 1, @GdDdId = 'fred', @Format = 'xxx', @Row = 3, @Col = 2, @Value = '54'\n\n--gd_getItem 10001\n--gd_getTemplateItem 25990\n--select * from gd_vItem\n\n-- set a new template item\n-- gd_setTemplateItem @ParentId = NULL, @GdDdId = '', @Format = '', @Row= NULL, @Col = NULL, @Value = NULL\n\n-- update a template item\n-- gd_setTemplateItem @ItemId = NULL, @ParentId = NULL, @GdDdId = '', @Format = '', @Row= NULL, @Col = NULL, @Value = NULL\n\n\n\n\nView Data Item\n-- =============================================\n-- Create view gd_vDataItem\n-- =============================================\nIF EXISTS (SELECT TABLE_NAME\n       FROM   INFORMATION_SCHEMA.VIEWS\n       WHERE  TABLE_NAME = N'gd_vDataItem')\n    DROP VIEW gd_vDataItem\nGO\n\nCREATE VIEW gd_vDataItem\nAS\n    SELECT a.R2ItemId AS ItemId,\n        a.R2ParentId AS ParentId,\n        a.R2GdDdId AS GdDdId,\n        a.R2Depth AS Depth,\n        c.R6Format AS Format,\n        c.R6Row AS Row,\n        c.R6Col AS Col,\n        a.R2SeqNo AS SeqNo,\n        c.R6SeqNo AS TemplateSeqNo,\n        b.R3DivisionId AS DivisionId,\n        b.R3Value AS Value,\n        b.R3ItemLinkId AS ItemLinkId,\n        b.R3TaxPackageRef AS TaxPackageRef,\n        b.R3ProcessId AS ProcessId,\n        b.R3Attribute AS Attribute\n    FROM R2Item a, R3Data b, R6TemplateItem c\n    WHERE a.R2ItemId = b.R3ItemId\n        and a.R2GdDdId = c.R6GdDdId\n\nGO\nselect * from gd_vDataItem\n\n\nView Template Item\n-- =============================================\n-- Create view gd_vTemplateItem\n-- =============================================\nIF EXISTS (SELECT TABLE_NAME \n       FROM   INFORMATION_SCHEMA.VIEWS \n       WHERE  TABLE_NAME = N'gd_vTemplateItem')\n    DROP VIEW gd_vTemplateItem\nGO\n\nCREATE VIEW gd_vTemplateItem\nAS \n    SELECT R6TemplateItemId AS ItemId\n        , R6ParentId AS ParentId\n        , R6GdDdId AS GdDdId\n        , R6Depth AS Depth\n        , R6Lineage AS Lineage\n        , R6Format AS Format\n        , R6Row AS Row\n        , R6Col AS Col\n        , R6Value AS Value\n        , R6SeqNo AS SeqNo\n        , R6ItemLinkId AS ItemLinkId\n    FROM R6TemplateItem ti\n\n\nGO\nselect * from gd_vTemplateItem \n\n\nView Template Tree\n-- =============================================\n-- Create view gd_vTemplateTree\n-- =============================================\nIF EXISTS (SELECT TABLE_NAME \n       FROM   INFORMATION_SCHEMA.VIEWS \n       WHERE  TABLE_NAME = N'gd_vTemplateTree')\n    DROP VIEW gd_vTemplateTree\nGO\n\nCREATE VIEW gd_vTemplateTree\nAS \n    SELECT R6TemplateItemId AS ItemId\n        , R6ParentId AS ParentId\n        , R6GdDdId AS GdDdId\n        , R6Depth AS Depth\n        , R6Lineage AS Lineage\n        , R6Format AS Format\n        , R6Row AS Row\n        , R6Col AS Col\n        , R6Value AS Value\n        , R6SeqNo AS SeqNo\n        , R6ItemLinkId AS ItemLinkId\n    FROM R6TemplateItem ti\n    WHERE R6ParentId = 0\n\nGO\nselect * from gd_vTemplateTree"
  },
  {
    "objectID": "Content/Journal/posts/2019-08-08-Loqate.html",
    "href": "Content/Journal/posts/2019-08-08-Loqate.html",
    "title": "EIP Loqate",
    "section": "",
    "text": "Loqate\nLoqate is a service which supplies addresses and geocoding for a given search string. The Loqate service employs a iterative id base system where by, if you supply the id to subsequent calls then you will get narrower results. When you receive a result of type ‚ÄòAddress‚Äô then you can call Loqate‚Äôs Retrieve api call to get that items address details.\n\n\n\nEIP Pattern\n\n\n\n\n\nLA Pattern"
  },
  {
    "objectID": "Content/Journal/posts/2019-07-25-Documentation.html",
    "href": "Content/Journal/posts/2019-07-25-Documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation is task that is best shared among all the relevant people, each supplying enough information that satisfies there understanding of the issue.\n\nBusiness User\n\nWhat they need\nWhat would they recognize as success\nWhat would indicate a failure\n\nBusiness Analyst\n\nGather\n\nGather requirements from the business user\nGather Success and Failure criteria from the business user\n\nRefactor\n\nBreak down the requirements into distinct items\nChallenge contradictions and Remove duplications\n\nAgainst previous requirements\n\n\nReview\nInterpret the Business Users requirements\nMake sure it doesn‚Äôt conflict with previous requirements\nGenerate Feature/Stories\nWith Acceptance Criteria !!\n\nDeveloper\n\nCreate a BDD Feature File\nGenerate scenarios to satisfy the story\n\nTest\nCode\nRefactor\n\n\nThe same principals of Test,Code,Refactor should be applied to the Requirement gathering process"
  },
  {
    "objectID": "Content/Journal/posts/2019-07-16-FluentAsserts.html",
    "href": "Content/Journal/posts/2019-07-16-FluentAsserts.html",
    "title": "Fluent Assertions",
    "section": "",
    "text": "Fluent Assertions is a helper library for testing that tries to add better context to your assertions.\nE.g. * Where you would have used * Assert.AreEqual(HttpStatusCode.OK, response.StatusCode); * or should it have been * Assert.AreEqual(response.StatusCode, HttpStatusCode.OK);\n\nYou can now do this :-\n\nresponse.StatusCode.Should().Be(HttpStatusCode.OK);\n\nOther than making the calling more explicit, it also gives a a clearer error message on failure.\nMessage: Expected response.StatusCode to be NotFound, but found OK.\n\n## Reference: * https://fluentassertions.com/"
  },
  {
    "objectID": "Content/Journal/posts/2019-05-22-Mediator Pattern.html",
    "href": "Content/Journal/posts/2019-05-22-Mediator Pattern.html",
    "title": "GOF Mediator Pattern",
    "section": "",
    "text": "GOF Mediator Class"
  },
  {
    "objectID": "Content/Journal/posts/2019-05-22-Mediator Pattern.html#good-for",
    "href": "Content/Journal/posts/2019-05-22-Mediator Pattern.html#good-for",
    "title": "GOF Mediator Pattern",
    "section": "Good For",
    "text": "Good For\n\nLoose coupling\nDelegates the interaction between the objects to a separate mediation object."
  },
  {
    "objectID": "Content/Journal/posts/2019-05-22-Mediator Pattern.html#references",
    "href": "Content/Journal/posts/2019-05-22-Mediator Pattern.html#references",
    "title": "GOF Mediator Pattern",
    "section": "References",
    "text": "References\n\nhttps://www.dofactory.com/net/mediator-design-pattern\nhttps://en.wikipedia.org/wiki/Mediator_pattern"
  },
  {
    "objectID": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html",
    "href": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html",
    "title": "Storage Account Encryption",
    "section": "",
    "text": "By default storage accounts are encrypted, and Microsoft holds the keys. The encryption that is used is AES 256 bit, as it is one of the strongest ciphers currently available.\nThe one big issue with this is that Microsoft owns the encryption key and potentially has unrestricted access to the users data. They have included the facility for the user to specify an encryption key, so you can meet your individual company security or regulatory compliance needs. To use your own key, you will need. * A Key Vault * Needs to be in the same region as the storage * Does not need to be in the same subscription * Storage needs permissions to access your Key Vault. * Need to grant wrapKey, unwrapKey privileges"
  },
  {
    "objectID": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html#data-encryption",
    "href": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html#data-encryption",
    "title": "Storage Account Encryption",
    "section": "Data Encryption",
    "text": "Data Encryption\nA key point to understand, the data itself is not encrypted with the key. Microsoft employs a two stage encryption process which involves a DEK (Data Encryption Key) and a KEK (Key Encryption Key). The DEK is generated when the storage account is created, and is used to encrypt the data. The DEK is it self is encrypted with a key that Microsoft holds (KEK), Its this Microsoft key that can be replaced with the users own key."
  },
  {
    "objectID": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html#key-rotation",
    "href": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html#key-rotation",
    "title": "Storage Account Encryption",
    "section": "Key Rotation",
    "text": "Key Rotation\nThis is currently under development by Microsoft. Key rotation is a process where the KEK (see Data Encryption above) is rotated every 90 days, this involves decrypting the existing key and re-encrypting it with a newly generated key."
  },
  {
    "objectID": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html#references",
    "href": "Content/Journal/posts/2019-03-04-StorageAccountEncryption.html#references",
    "title": "Storage Account Encryption",
    "section": "References",
    "text": "References\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-service-encryption"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-12-PfxBase64.html",
    "href": "Content/Journal/posts/2019-02-12-PfxBase64.html",
    "title": "Certificate pfx to Base64",
    "section": "",
    "text": "To convert certificate that is in .pfx to base64 format in PowerShell, you can use .NET namespace available in PowerShell to convert. I had a scenario where I was required to use base64 encoding to upload certificate to Azure to secure communication to backend instance. Since Microsoft Azure provides rich API to work with. I was able to make a patch request and push certificate to Azure. In this tutorial, I will show you how to convert certificate from .pfx to base64. Open PowerShell as an administrator. Now that we have PowerShell console opened. Let‚Äôs first load the content into a variable.\nParam([string] $file) $file = '.\\certificate.pfx' $pfxFileBytes = get-content $file -Encoding Byte [System.Convert]::ToBase64String($pfxFileBytes) | Out-File 'PfxFileBytes-Base64.txt'\nOnce you have your Base64 string you can insert it in the ARM template as shown below."
  },
  {
    "objectID": "Content/Journal/posts/2019-02-12-PfxBase64.html#azuredeploy.parameters.json",
    "href": "Content/Journal/posts/2019-02-12-PfxBase64.html#azuredeploy.parameters.json",
    "title": "Certificate pfx to Base64",
    "section": "Azuredeploy.parameters.json",
    "text": "Azuredeploy.parameters.json\n    \"cert\": {\"value\": \"MIIT...\"},\n    \"certPass\": { \"value\": \"password\" }"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-12-PfxBase64.html#azuredeploy.json",
    "href": "Content/Journal/posts/2019-02-12-PfxBase64.html#azuredeploy.json",
    "title": "Certificate pfx to Base64",
    "section": "Azuredeploy.json",
    "text": "Azuredeploy.json\n  \"parameters\": {\n      \"cert\": {\n      \"type\": \"securestring\"\n    },\n    \"certPass\": {\n      \"type\": \"securestring\"\n    }\n  }\n  \"variables\": {\n    \"var_cert_name\": \"[concat( tolower(parameters('para_application_name')), uniqueString(resourceGroup().id))]\",\n  }\n  \"resources\": [\n      {\n      \"apiVersion\": \"2015-08-01\",\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_cert_name')]\",\n      \"properties\": {\n        \"pfxBlob\": \"[parameters('cert')]\",\n        \"password\": \"[parameters('certPass')]\"\n      },\n      \"scale\": null,\n      \"tags\": {\n        \"displayName\": \"Certificate\"\n      },\n      \"type\": \"Microsoft.Web/certificates\"\n    }\n  ]"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-11-KeyVault.html",
    "href": "Content/Journal/posts/2019-02-11-KeyVault.html",
    "title": "KeyVault",
    "section": "",
    "text": "I‚Äôm always looking for ways to simply automate deployments, so I don‚Äôt have to hard code, or repeat myself. This blog is looking at Key Vault deployments, and its use."
  },
  {
    "objectID": "Content/Journal/posts/2019-02-11-KeyVault.html#parameters",
    "href": "Content/Journal/posts/2019-02-11-KeyVault.html#parameters",
    "title": "KeyVault",
    "section": "Parameters",
    "text": "Parameters\nFor Key Vaults I don‚Äôt like to specify the normal key/value pairs, as it would mean that I have to specify the name of the secret within the deployment script. The obvious draw back here is, as the code evolves and the secret changes then you would need to update both the parameters plus the deployment script. So first off in the parameters file put the secrets into an array.\n    \"para_kvSecretsObject\": {\n      \"value\": {\n        \"secrets\": [\n          {\n            \"secretName\": \"applicationuser\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          },\n          {\n            \"secretName\": \"AnotherSecret\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          }\n        ]\n      }\nStraight away you can see that the actual secret is injected from the CI tool, this is so you can inject a different secret depending on the environment. Putting the secrets into an array in this way means that we can cycle through the array in the deployment script without actually referring to the secret itself, only using the array index."
  },
  {
    "objectID": "Content/Journal/posts/2019-02-11-KeyVault.html#load-secrets",
    "href": "Content/Journal/posts/2019-02-11-KeyVault.html#load-secrets",
    "title": "KeyVault",
    "section": "Load Secrets",
    "text": "Load Secrets\nLoading of secrets is normally achieved via the actual KeyVault deployment specifying each secret in turn, but as our secrets are specified as part of an array, we will need to apply them in a slightly different way. At the resource level we create a new feature which has a type of Microsoft.KeyVault/vaults/secrets. In the following code block the lines to look out for are the Copy/Name and Value properties. * The Copy property looks at the array and determines the number of items we need to process. * The Name property set the secret name to the ‚ÄòsecrectName‚Äô value in the array and it is indexed by the copyIndex() parameter * The Value is specified in the same way as the Name just using the secretValue array name.\nOne important line which is often overlooked is the DependsOn property. Each resource in the deployment file can be applied by Azure in parallel so by specifying the DependsOn property stops the deployment until the dependant resource exists.\n    {\n      \"apiVersion\": \"2015-06-01\",\n      \"copy\": {\n        \"name\": \"secretsCopy\",\n        \"count\": \"[length(parameters('para_kvSecretsObject').secrets)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_kv_name'), '/', parameters('para_kvSecretsObject').secrets[copyIndex()].secretName)]\",\n      \"properties\": {\n        \"value\": \"[parameters('para_kvSecretsObject').secrets[copyIndex()].secretValue]\"\n      },\n      \"tags\": {\n        \"displayName\": \"Key Vault Secrets\"\n      },\n      \"type\": \"Microsoft.KeyVault/vaults/secrets\"\n    }"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-11-KeyVault.html#accessing-the-key-vault",
    "href": "Content/Journal/posts/2019-02-11-KeyVault.html#accessing-the-key-vault",
    "title": "KeyVault",
    "section": "Accessing the Key Vault",
    "text": "Accessing the Key Vault\nA Key Vault cannot exist on its own, when it is deployed it needs the service principal, at the moment the service principal can only be obtained from an AD associated application, such as a web app/api app or function.\n\n\n\nPlantUml flowchart"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-08-PlantUml.html",
    "href": "Content/Journal/posts/2019-02-08-PlantUml.html",
    "title": "PlantUml",
    "section": "",
    "text": "PlantUML is not directly supported by GitHub, but its still possible. Basically we pass your puml file to PlantUML to generate, and they return a PNG which gets included in the page\n\nGenerate you .puml file in the usual way using your favorite editor and confirm the diagram is as you want it.\nSave the file to the assets directory\nThen paste the following into you Markup.\n\nPlantUML is a text descriptive language which gets converted into UML and other types of graphs So this :-\n\nwould produce this :-\n\n\n\nPlantUml flowchart\n\n\nA more complex example :-\n![PlantUml flowchart](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/\nnewportg/newportg.github.io/assets/Plantuml/test.puml)\n\n\n\nPlantUml flowchart"
  },
  {
    "objectID": "Content/Journal/posts/2018-10-15-ARMTemplateParameter-VariableSetup.html",
    "href": "Content/Journal/posts/2018-10-15-ARMTemplateParameter-VariableSetup.html",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "",
    "text": "For something so simple, arm templates can become complex things, so I prefer to try to set some ground rules before I go to deep. N.B this works for me, and may not suit everyone üòâ"
  },
  {
    "objectID": "Content/Journal/posts/2018-10-15-ARMTemplateParameter-VariableSetup.html#parameters",
    "href": "Content/Journal/posts/2018-10-15-ARMTemplateParameter-VariableSetup.html#parameters",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "Parameters",
    "text": "Parameters\nI prefer to inject any unique values via a VSTS/VSO or if your prefer Azure DevOps deployment process.\nIn the first part of the file I spell out the acronyms which form part of the naming convention for the resources, you could use nested templates for this, but I feel they add unnecessary complications, as the nested template must be available via a URL. The second part involves parameters that are specific to this application, such as the tenant id, application name etc.\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"para_acronym_region\": { \"value\": \"we\" },\n        \"para_acronym_resgrp\": { \"value\": \"resgrp\" },\n        \"para_acronym_appsvc\": { \"value\": \"appsvc\" },\n        \"para_acronym_svcpln\": { \"value\": \"svcpln\" },\n        \"para_acronym_stract\": { \"value\": \"str\" },\n        \"para_acronym_kv\": { \"value\": \"kv\" },\n        \"para_acronym_azfunc\": { \"value\": \"fn\" },\n        \"para_acronym_appin\": { \"value\": \"appins\" },\n        \"para_acronym_webapp\": { \"value\": \"webapp\" },\n        \"para_ad_tenantid\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_application_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_vanity_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_target_env\": { \"value\": \"dev\" },\n        \"para_kvSecretsObject\": {\n            \"value\": {\n                \"secrets\": [\n                        {\n                        \"secretName\": \"applicationuser\",\n                        \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        },\n                        {\n                            \"secretName\": \"AnotherSecrect\",\n                            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        }\n                    ]\n                }\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/2018-10-15-ARMTemplateParameter-VariableSetup.html#variables",
    "href": "Content/Journal/posts/2018-10-15-ARMTemplateParameter-VariableSetup.html#variables",
    "title": "ARM Template Parameter/Variable Setup",
    "section": "Variables",
    "text": "Variables\nAs you can see from the variables, I build up my resource names from the parameters. I also pull in values for the hostingplan and component identities, so they can be used easily with the resource definitions.\n\"variables\": {\n    \"var_env_region\": \"[concat(parameters('para_target_env'), '-', parameters('para_acronym_region'))]\",\n    \"var_public_url\": \"[concat(parameters('para_target_env'), '.', parameters('para_application_name'), '.', parameters('para_vanity_name'))]\",\n    \"var_str_name\": \"[concat(parameters('para_application_name'), parameters('para_acronym_stract'), parameters('para_target_env'), parameters('para_acronym_region'))]\",\n    \"var_str_resId\": \"[resourceId(resourceGroup().Name,'Microsoft.Storage/storageAccounts', variables('var_str_name'))]\",\n    \"var_kv_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_kv'), '-', variables('var_env_region'))]\",\n    \"var_azf_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_azfunc'),'-', variables('var_env_region'))]\",\n    \"var_appin_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appin'),'-', variables('var_env_region'))]\",\n    \"var_hstpln_group\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_resgrp'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_env\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appsvc'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_svcpln'), '-', variables('var_env_region'))]\",\n    \"var_webapp_name\": \"[concat(parameters('para_application_name'), '-' ,parameters('para_acronym_webapp'),'-', variables('var_env_region'))]\",\n    \"var_webapp_hstpln\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', variables('var_hstpln_group'), '/providers/Microsoft.Web/serverfarms/', variables('var_hstpln_name'))]\",\n    \"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\n},"
  },
  {
    "objectID": "Content/Current/todo.html",
    "href": "Content/Current/todo.html",
    "title": "To Do",
    "section": "",
    "text": "To Do‚Äôs\n\nTitle Service (Land Registry or Nimbus)\n\nStill not heard anything from Al concerning Nimbus\n\nIf we go with Nimbus, which sounds likely as the cost is lower, we will have to rename the project.\nIf we change the title to a different name, we should investigate using multiple services, so we can support Scotland and Northern Ireland and any other country as they come along.\n\nMyKf\n\nInvestigate how MyKf will save the data in the hub db\nInvestigate what MyKf will save in the database\nTalk to Katy to find out who is the developer\n\nRams Solaiappan\n\n\n\nQuarto\n\nPlantuml plugin\n\nDiagrams are loaded from files source\nAny raw plantuml diagram\nCreated a LUA PlantUML filter for Quarto\nAccepts a Puml file as a attribute\nAccepts inline PlantUML code\n\nCreates a local puml file if inline code is used\n\nRenders the PlantUML diagram as SVG\nRenders puml files in the current directory.\nWorks with subdirectories\nDirect where to store the generated SVG files\n\nQuarto and LUA processing executes relative to the location of the .qmd file. So we will need a method to identify which directory we are in so we can then redirect the output files to a specified subdirectory.\n\nCreate a GitHub repo for the filter\nPublish to Quarto extensions\nDocument the filter as a journal entry\nEcho script rather than create puml file.\n\nExecuting a ‚ÄòEcho‚Äô command terminates when it reaches a line break. So we will need to replace all line breaks with a special character sequence, e.g.¬†‚Äò‚Äô and then use a script to replace the special character sequence with a line break. Currently the directory is littered with intermediate .puml files which are not needed once the diagram has been created.\n\n\n\nAPI First Quarto Integration\n\nCreate a Journal page to document the process\nInvestigate the TypeSpec process\nInvestigate the AutoRest process\nCreate a End to End example\n\nCreate a GitHub repo to hold the example\nPublish the example to GitHub\n\nCreate a Quarto filter to automate the process\n\nCreate a GitHub repo to hold the filter\nPublish the filter to Quarto extensions\nDocument the filter as a journal entry\n\n\nSAD Website\n\nCreated a Frontify Quarto Book\nBook loaded into GitHub\nBook published to GitHub Pages\n\nBook ppublished as a website at https://newportg.github.io/Quarto-Frontify/\n\nRemove any references to KF"
  },
  {
    "objectID": "Content/Current/posts/2025-10-16/index.html",
    "href": "Content/Current/posts/2025-10-16/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-16\n\nDiabetes\n\nContinued the Diabetes online course.\n\nAPI First Quarto Integration\n\nCreated a Journal page to document the process\nInvestigate the TypeSpec process\nInstalled the TypeSpec CLI\nCreated a sample TypeSpec file\nGenerated the OpenAPI file\nISSUE: No scripting options seem to be acailable to automate the process"
  },
  {
    "objectID": "Content/Current/posts/2025-10-14/index.html",
    "href": "Content/Current/posts/2025-10-14/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-14\n\nDiabetes\n\nContinued the Diabetes online course.\n\nQuarto PlantUML filter\n\nCreated a Quarto filter to process PlantUML diagrams within Quarto documents.\nPublished it as a Quarto extension.\nDocumented the filter as a journal entry.\nNot able to direct the output files to a subdirectory yet.\nNeed to find a way to echo the script rather than create a puml file.\nCreated a GitHub repo to hold the filter:"
  },
  {
    "objectID": "Content/Current/posts/2025-10-10/index.html",
    "href": "Content/Current/posts/2025-10-10/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-10\n\nQuarto / Plantuml plugin\n\nThe plugin currently doesnt support standard includes !include  etc this needs to be fixed :(\n\nStarted a Quarto / TypeSpec plugin with copilot\n\nRan out of copilot tokens\nAim was to create a fensed typespec element which would convert a typespec into a openapi/swagger html page\n\nLand Registy\n\nStill not heard anything from Al concerning Nimbus\n\nIf we go with Nimbus, which sounds likely as the cost is lower, we will have to rename the project.\nIf we change the title to a different name, we should investigate using multiple services, so we can support Scotland and Northern Ireland and any other country as they come along.\n\nMyKf\n\nInvestigate how MyKf will save the data in the hub db\nInvestigate what MyKf will save in the database\nTalk to Katy to find out who is the developer\n\nRams Solaiappan\n\n\n\nQuarto-Frontify\n\nCreated a Frontify Quarto Book\nPlantuml Diagrams\n\nDiagrams are loaded from files source\nAny raw plantuml diagram\nAny diagram that includes a !Include statement :(\n\nBook loaded into GitHub\nBook published to GitHub Pages\n\nThe Quarto Publish command produced errors :("
  },
  {
    "objectID": "Content/Current/posts/2025-10-08/index.html",
    "href": "Content/Current/posts/2025-10-08/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "Created Quarto Projects folder for project wikis\n\nNeed to look at how I can make the project wiki‚Äôs usable with quarto\nNeed to get plantuml working with r markdown\n\n\nCreated Quarto Current foler for daily notes\nTech Review, walked through LR sofar."
  },
  {
    "objectID": "Content/Current/index.html",
    "href": "Content/Current/index.html",
    "title": "Current Backlog",
    "section": "",
    "text": "Order By\nDefault\n\n        Date - Oldest\n      \n\n        Date - Newest\n      \n\n        Title\n      \n\n        Author\n      \n\n    \n      \n      \n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\nOctober 8, 2025\n\n\nTo Do\n\n\nGary Newport\n\n\n\n\nNo matching items\n\n\n\n   \n    \n    Order By\nDefault\n\n        Date - Oldest\n      \n\n        Date - Newest\n      \n\n        Title\n      \n\n    \n      \n      \n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\nOctober 20, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 16, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 15, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 14, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 13, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 10, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 9, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 8, 2025\n\n\nDaily Note\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Content/Current/posts/2025-10-09/index.html",
    "href": "Content/Current/posts/2025-10-09/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "Quarto PlantUML diagram support\n\nTrying the published plantuml filters\n\n\n\ninline script"
  },
  {
    "objectID": "Content/Current/posts/2025-10-13/index.html",
    "href": "Content/Current/posts/2025-10-13/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-13\n\nDiabetes\n\nAsk about a holiday letter\nRecommended to do the X-pert Online course\nNHS Online Course https://app.changinghealth.com\nHad a look for insulin travel bag\n\nGP Appointment review\n\nWas Diabetic review with the nurse\nNurse took blood for chloesterol\nCancelled and rebooked Blood Test with St Peters this time.\n\nQuarto PlantUML filter\n\nTried to use the published PlantUML filters and could not get them to work.\nNeed to create my own filter."
  },
  {
    "objectID": "Content/Current/posts/2025-10-15/index.html",
    "href": "Content/Current/posts/2025-10-15/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-15\n\nDiabetes\n\nContinued the Diabetes online course.\n\nQuarto PlantUML filter\n\nContinued with the Quarto filter to process PlantUML diagrams within Quarto documents.\nWorked on placing all output files in a particular subdirectory.\nNeed to find a way to echo the script rather than create a puml file."
  },
  {
    "objectID": "Content/Current/posts/2025-10-20/index.html",
    "href": "Content/Current/posts/2025-10-20/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-20\n\nDiabetes\n\nContinued the Diabetes online course.\n\nQuarto\n\nCreated a example SAD Website - Quarto-Frontify\n\nJournal\n\nChanged the Journal template to create named directories for each post rather than date based directories. This makes it easier to manage posts with similar dates.\nOne thing to note is that the directory created includes spaces, which should be removed otherwise you have to replace them with ‚Äò%20‚Äô in any WikiLink :(\n\nLooking for a method to camelcase the ‚Äò${FOAM-TITLE}‚Äô variable."
  },
  {
    "objectID": "Content/Journal/index.html",
    "href": "Content/Journal/index.html",
    "title": "Journal",
    "section": "",
    "text": "October 20, 2025\n        \n        \n            Plantuml Quarto Filter\n\n            \n            \n                \n                \n                    Plantuml\n                \n                \n                \n                    Quarto Filter\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            October 16, 2025\n        \n        \n            API First\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n                \n                    API First\n                \n                \n                \n                    TypeSpec\n                \n                \n                \n                    AutoRest\n                \n                \n                \n                    Quarto\n                \n                \n                \n                    LUA\n                \n                \n                \n                    PowerShell\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            October 16, 2025\n        \n        \n            TypeSpec\n\n            \n            \n                \n                \n                    API First\n                \n                \n                \n                    TypeSpec\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            September 26, 2025\n        \n        \n            Diabetes\n\n            \n            \n                \n                \n                    Diabetes\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            May 1, 2025\n        \n        \n            Integration Testing Azure Functions\n\n            \n            \n                \n                \n                    Azure Functions\n                \n                \n                \n                    Testing\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 20, 2024\n        \n        \n            Data Annotation Vs FluentValidation\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Rules\n                \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 14, 2024\n        \n        \n            SOLID\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n                \n                    C#\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            September 27, 2023\n        \n        \n            Visual Studio Solution Template\n\n            \n            \n                \n                \n                    C#\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 5, 2023\n        \n        \n            Cast To a Reflected Type\n\n            \n            \n                \n                \n                    C#\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 22, 2023\n        \n        \n            Web UI\n\n            \n            \n                \n                \n                    Architecture\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 1, 2023\n        \n        \n            How to build Cloud components without any Azure Experience\n\n            \n            \n                \n                \n                    Architecture\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 20, 2023\n        \n        \n            SQL Change Tracking\n\n            \n            \n                \n                \n                    SQL\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 20, 2023\n        \n        \n            Event Notificiation\n\n            \n            \n                \n                \n                    Event Grid\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 19, 2023\n        \n        \n            Web Upload File AV Scanning\n\n            \n            \n                \n                \n                    Identity\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 10, 2022\n        \n        \n            Service Application Development\n\n            \n            \n                \n                \n                    Identity\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 21, 2022\n        \n        \n            User Assigned Managed Identities\n\n            \n            \n                \n                \n                    Identity\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 14, 2020\n        \n        \n            Adaptive Cards\n\n            \n            \n                \n                \n                    Json\n                \n                \n                \n                    Adaptive Card\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 20, 2020\n        \n        \n            Azure Pipeline Git Commit\n\n            \n            \n                \n                \n                    DevOps\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 19, 2020\n        \n        \n            Azure Functions Logging\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 17, 2020\n        \n        \n            Function Monkey\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 9, 2020\n        \n        \n            Rules\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Rules\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 1, 2019\n        \n        \n            Online Spreadsheet Database\n\n            \n            \n                \n                \n                    SQL\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            October 16, 2019\n        \n        \n            gRPC\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 8, 2019\n        \n        \n            EIP Loqate\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 8, 2019\n        \n        \n            EIP Content Based Routing\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 31, 2019\n        \n        \n            SAS Storage Account Access\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Account\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 25, 2019\n        \n        \n            Documentation\n\n            \n            \n                \n                \n                    Markdown\n                \n                \n                \n                    Documentation\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 22, 2019\n        \n        \n            MediatR\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 16, 2019\n        \n        \n            Azure Functions Dependency Injection\n\n            \n            \n                \n                \n                    Azure Functions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 16, 2019\n        \n        \n            Fluent Assertions\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Fluent Assertions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 16, 2019\n        \n        \n            IFrames\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Iframes\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 22, 2019\n        \n        \n            GOF Mediator Pattern\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 28, 2019\n        \n        \n            Getting Secrets from KeyVault in Azure Functions\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Key Vault\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 4, 2019\n        \n        \n            Storage Account Encryption\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Accounts\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 15, 2019\n        \n        \n            Storage Accounts\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Accounts\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 12, 2019\n        \n        \n            Certificate pfx to Base64\n\n            \n            \n                \n                \n                    Secuirty\n                \n                \n                \n                    Base64\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 11, 2019\n        \n        \n            Azure/C4\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    C4\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 11, 2019\n        \n        \n            KeyVault\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Key Vault\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 11, 2019\n        \n        \n            WOPI\n\n            \n            \n                \n                \n                    Office\n                \n                \n                \n                    WOPI\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 8, 2019\n        \n        \n            PlantUml\n\n            \n            \n                \n                \n                    PlantUml\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            October 15, 2018\n        \n        \n            ARM Template Parameter/Variable Setup\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Arm Templates\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            October 15, 2018\n        \n        \n            Azure Function Managed Service Identities\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Security\n                \n                \n                \n                    Managed Service Identities\n                \n                \n            \n            \n\n            Azure Function Managed Service Identities\n            \n        \n        \n    \n    \n    \n                  \n            August 14, 2018\n        \n        \n            Azure Application Hosting\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Hosting\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 14, 2018\n        \n        \n            Azure Static Websites\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Hosting\n                \n                \n                \n                    Static Websites\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "Content/Journal/posts/2018-10-15-AzureFunctionManagedServiceIdentities.html",
    "href": "Content/Journal/posts/2018-10-15-AzureFunctionManagedServiceIdentities.html",
    "title": "Azure Function Managed Service Identities",
    "section": "",
    "text": "Bootstrapping\nThe trouble with many security policies is that at least some element needs to know the password in order to instigate access to resources. That used to mean putting credentials into a configuration file or inserting them during a deployment process. The Manage Service Identities (MSI) facility has got around this by allowing all your resources to register a service principal with Active Directory, and then each resource grants the desired level of access to that service principal. By doing the security in this way, each of the resources never need to know credentials, they only request access and deal with the response. So, by removing credentials from the equation then there is no need to have to rotate passwords or update certs on a timely basis as they simply donÔøΩt exist between the resources.\n\n\nSo how do we accomplish this.\nWithin the azure function arm template declaration insert the following, this will register the function with your active directory.\n\"identity\": {\"type\": \"SystemAssigned\"},\nIn the variables section of the Arm Template, get the identity of the Azure Function. (replace ‚Äòvar_azf_name‚Äô with the name of your function)\n\"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\nWithin your Key Vault template your will need to add the functions access policy\n\"accessPolicies\": [{\"tenantId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').tenantId]\",\"objectId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').principalId]\",\"permissions\": {\"certificates\": [\"get\"],\"keys\": [\"get\"],\"secrets\": [\"get\"]}}}]"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-11-Azure-C4.html",
    "href": "Content/Journal/posts/2019-02-11-Azure-C4.html",
    "title": "Azure/C4",
    "section": "",
    "text": "I found a good resource, so I can include bot C4 diagrams and Azure icons within a PlantUML diagram. I‚Äôve done C4 within PlantUml before but it was a bit crude.\n![C4 flowchart](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/newportg/newportg.github.io/master/assets/C4/c4.puml)\n\n\n\nC4 flowchart\n\n\n![Azure flowchart](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/newportg/newportg.github.io/master/assets/C4/azure.puml)\n\n\n\nAzure flowchart\n\n\nhttps://github.com/RicardoNiepel/Azure-PlantUML"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-11-WOPI.html",
    "href": "Content/Journal/posts/2019-02-11-WOPI.html",
    "title": "WOPI",
    "section": "",
    "text": "For a full description of WOPI, please follow the link to the project. WOPI Project\n\n\n\nWopi Balanced"
  },
  {
    "objectID": "Content/Journal/posts/2019-02-15-StorageAccounts.html",
    "href": "Content/Journal/posts/2019-02-15-StorageAccounts.html",
    "title": "Storage Accounts",
    "section": "",
    "text": "The blob container resource is a sub-resource of the storage account. When you create a storage account, you can now specify an array of storages resources. We are then going to specify objects of type ‚ÄúblobServices/containers‚Äù, and make sure to use an API version of 2018-02-01 or later. In the example below, we are creating a storage account with two containers.\n{\n    \"type\": \"Microsoft.Storage/storageAccounts\",\n    \"apiVersion\": \"2018-02-01\",\n    \"name\": \"[parameters('StorageAccountName')]\",\n    \"location\": \"[resourceGroup().location]\",\n    \"tags\": {\n        \"displayName\": \"[parameters('StorageAccountName')]\"\n    },\n    \"sku\": {\n        \"name\": \"Standard_LRS\"\n    },\n    \"kind\": \"StorageV2\",\n    \"properties\": {},\n    \"resources\": [\n        {\n            \"type\": \"blobServices/containers\",\n            \"apiVersion\": \"2018-03-01-preview\",\n            \"name\": \"[concat('default/', parameters('Container1Name'))]\",\n            \"dependsOn\": [\n                \"[parameters('StorageAccountName')]\"\n            ],\n            \"properties\": {\n                \"publicAccess\": \"Container\"\n            }\n        },\n        {\n            \"type\": \"blobServices/containers\",\n            \"apiVersion\": \"2018-03-01-preview\",\n            \"name\": \"[concat('default/', parameters('Container2Name'))]\",\n            \"dependsOn\": [\n                \"[parameters('StorageAccountName')]\"\n            ],\n            \"properties\": {\n                \"publicAccess\": \"None\"\n            }\n        }\n    ]\n}\nWe can then go ahead and deploy this, and see the two containers being created. You can see in the template we are setting different Public Access properties on the two containers, you have a choice of 3 values here: None (private container) Container (the whole container is publically accessible) Blob (only Blobs are publically accessible)\nThe above example is fine, but you need to specify each of the containers as separate parameters, and remember to add the correct number of container clauses to the script.\nSo if we specify the containers as a array in the parameters file\n    \"para_storageObject\": {\n      \"value\": {\n        \"containers\": [\n          {\n            \"containerName\": \"files\"\n          },\n          {\n            \"containerName\": \"elephants\"\n          },\n          {\n            \"containerName\": \"bananas\"\n          }\n        ]\n      }\n    },\nThen we can use a copy clause to cycle over the array and create our containers in the deploy\n    {\n      \"apiVersion\": \"2018-03-01-preview\",\n      \"copy\": {\n        \"name\": \"containersCopy\",\n        \"count\": \"[length(parameters('para_storageObject').containers)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.Storage/storageAccounts/', variables('var_str_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_str_name'), '/default/', parameters('para_storageObject').containers[copyIndex()].containerName)]\",\n      \"properties\": {\n        \"publicAccess\": \"Container\"\n      },\n      \"tags\": {\n        \"displayName\": \"StorageAcct/Containers\"\n      },\n      \"type\": \"Microsoft.Storage/storageAccounts/blobServices/containers\"\n    },"
  },
  {
    "objectID": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html",
    "href": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "",
    "text": "KeyVault is a central resource where you should be storing all of your applications secrets/connection strings and config. The reason for using KeyVault rather than using a local config file, is really quite simple. config files dont exist for most azure artefacts, and if they did, it would encourage the sort of key management that has plagued .NET since day one. By using the KeyVault in this way means that the application code doesn‚Äôt need to know anything about the environment or its configuration. The build process should inject any secrets into the parameters file, which the ARM Template deployment consumes and deploys.\nSo How do we do it, in ARM Templates"
  },
  {
    "objectID": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html#arm-template-definition",
    "href": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html#arm-template-definition",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "ARM Template Definition",
    "text": "ARM Template Definition\n\nCreate a Azure Function\n\nIdentity should be ‚ÄòSystem Assigned‚Äô\nWEBSITE_ENABLE_SYNC_UPDATE_SITE\n\nN.B There is a reason for this, but I cant find it right now.\n\nAdd KeyVault Key names to App Settings\n\nThe KeyVault key is not a secret.\nThis creates a Appsetting which can be seen in the code, and assigns to it a KeyVault secret.\n\n\n\n\"name\": \"appsettings\",\n    \"properties\": {\n        \"applicationuser\": \"[concat('@Microsoft.KeyVault(SecretUri=', reference('applicationuser').secretUriWithVersion, ')')]\"\n    }\n\nDefine a KeyVault\n\nSet access policy\n\nTennant Id\nSystem Assigned Object Id\n\nLoad secrets etc into KeyVault\n\nFuture\n\nAssign keys to App Settings via a array copy structure\n\nThis would move the definition to the parameters file.\n\nThe issue with this approach is that the copy construct cannot operate on child resources (See Below).\n\n\n\n\n\"resources\": [\n  {\n    \"type\": \"{provider-namespace-and-type}\",\n    \"name\": \"parentResource\",\n    \"copy\": {  \n      /* yes, copy can be applied here */\n    },\n    \"properties\": {\n      \"exampleProperty\": {\n        /* no, copy cannot be applied here */\n      }\n    },\n    \"resources\": [\n      {\n        \"type\": \"{provider-type}\",\n        \"name\": \"childResource\",\n        /* copy can be applied if resource is promoted to top level */ \n      }\n    ]\n  }\n]"
  },
  {
    "objectID": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html#application-usage",
    "href": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html#application-usage",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "Application Usage",
    "text": "Application Usage\nAccess as you would a config variable\n        private static string StorageAccount = System.Environment.GetEnvironmentVariable(\"StorageAccount\");"
  },
  {
    "objectID": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html#code-snippets-from-integration",
    "href": "Content/Journal/posts/2019-03-28-AccessingKeyVaultSecrectsFromAzFunc.html#code-snippets-from-integration",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "Code snippets from integration",
    "text": "Code snippets from integration\n\nArm Parameters\n    \"para_kvSecretsObject\": {\n      \"value\": {\n        \"secrets\": [\n          {\n            \"secretName\": \"applicationuser\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          },\n          {\n            \"secretName\": \"StorageAccount\",\n            \"secretValue\": \"DefaultEndpointsProtocol=https;AccountName=kfaalphaneugeneralst01;AccountKey=&lt;ACCOUNTKEY&gt;;EndpointSuffix=core.windows.net\"\n          }\n        ]\n      }\n    }\n\n\nARM Template\n    {\n      \"apiVersion\": \"2016-08-01\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Storage/storageAccounts/', variables('var_str_name'))]\"\n      ],\n      \"identity\": {\n        \"type\": \"SystemAssigned\"\n      },\n      \"kind\": \"functionapp\",\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_azf_name')]\",\n      \"properties\": {\n        \"name\": \"[variables('var_azf_name')]\",\n        \"siteConfig\": {\n          \"appSettings\": [\n            {\n              \"name\": \"AzureWebJobsDashboard\",\n              \"value\": \"[concat('DefaultEndpointsProtocol=https;AccountName=',variables('var_str_name'),';AccountKey=',listKeys(variables('var_str_resId'),'2015-05-01-preview').key1) ]\"\n            },\n            {\n              \"name\": \"AzureWebJobsStorage\",\n              \"value\": \"[concat('DefaultEndpointsProtocol=https;AccountName=',variables('var_str_name'),';AccountKey=',listKeys(variables('var_str_resId'),'2015-05-01-preview').key1) ]\"\n            },\n            {\n              \"name\": \"APPINSIGHTS_INSTRUMENTATIONKEY\",\n              \"value\": \"[reference(concat('microsoft.insights/components/', variables('var_appin_name'))).InstrumentationKey]\"\n            },\n            {\n              \"WEBSITE_ENABLE_SYNC_UPDATE_SITE\": \"true\"\n            }\n          ],\n          \"alwaysOn\": false\n        },\n        \"clientAffinityEnabled\": false,\n        \"serverFarmId\": \"[variables('var_hstpln_name')]\",\n        \"hostingEnvironment\": \"[variables('var_hstpln_env')]\",\n        \"hostNameSslStates\": [\n        ]\n      },\n      \"resources\": [\n        {\n          \"apiVersion\": \"2015-08-01\",\n          \"dependsOn\": [\n            \"[resourceId('Microsoft.Web/sites', variables('var_azf_name'))]\",\n            \"[resourceId('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\",\n            \"secretsCopy\"\n          ],\n          \"name\": \"appsettings\",\n          \"properties\": {\n            \"applicationuser\": \"[concat('@Microsoft.KeyVault(SecretUri=', reference('applicationuser').secretUriWithVersion, ')')]\",\n            \"StorageAccount\": \"[concat( '@Microsoft.KeyVault(SecretUri=', reference('StorageAccount').secretUriWithVersion, ')')]\"\n          },\n          \"tags\": {\n            \"displayName\": \"AppSettings\"\n          },\n          \"type\": \"config\"\n        }\n      ],\n      \"tags\": {\n        \"displayName\": \"Az Function\"\n      },\n      \"type\": \"Microsoft.Web/sites\"\n    },\n    {\n      \"apiVersion\": \"2016-10-01\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Web/sites/', variables('var_azf_name'))]\"\n      ],\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_kv_name')]\",\n      \"properties\": {\n        \"sku\": {\n          \"family\": \"A\",\n          \"name\": \"Standard\"\n        },\n        \"tenantId\": \"[variables('var_ten_id')]\",\n        \"accessPolicies\": [\n          {\n            \"tenantId\": \"[variables('var_ten_id')]\",\n            \"objectId\": \"[reference(variables('var_svc_prin'), '2015-08-31-PREVIEW').principalId]\",\n            \"permissions\": {\n              \"keys\": [\n                \"Get\",\n                \"List\",\n                \"Update\",\n                \"Create\",\n                \"Import\",\n                \"Delete\",\n                \"Recover\",\n                \"Backup\",\n                \"Restore\",\n                \"Decrypt\",\n                \"Encrypt\",\n                \"UnwrapKey\",\n                \"WrapKey\",\n                \"Verify\",\n                \"Sign\",\n                \"Purge\"\n              ],\n              \"secrets\": [\n                \"Get\",\n                \"List\",\n                \"Set\",\n                \"Delete\",\n                \"Recover\",\n                \"Backup\",\n                \"Restore\",\n                \"Purge\"\n              ],\n              \"certificates\": [\n                \"Get\",\n                \"List\",\n                \"Update\",\n                \"Create\",\n                \"Import\",\n                \"Delete\",\n                \"Recover\",\n                \"Backup\",\n                \"Restore\",\n                \"ManageContacts\",\n                \"ManageIssuers\",\n                \"GetIssuers\",\n                \"ListIssuers\",\n                \"SetIssuers\",\n                \"DeleteIssuers\",\n                \"Purge\"\n              ]\n            }\n          }\n        ],\n            \"enabledForDeployment\": false,\n            \"enabledForDiskEncryption\": false,\n            \"enabledForTemplateDeployment\": false\n      },\n      \"scale\": null,\n      \"tags\": {\n        \"displayName\": \"Key Vault\"\n      },\n      \"type\": \"Microsoft.KeyVault/vaults\"\n    },\n    {\n      \"apiVersion\": \"2015-06-01\",\n      \"copy\": {\n        \"name\": \"secretsCopy\",\n        \"count\": \"[length(parameters('para_kvSecretsObject').secrets)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_kv_name'), '/', parameters('para_kvSecretsObject').secrets[copyIndex()].secretName)]\",\n      \"properties\": {\n        \"value\": \"[parameters('para_kvSecretsObject').secrets[copyIndex()].secretValue]\"\n      },\n      \"tags\": {\n        \"displayName\": \"Key Vault Secrets\"\n      },\n      \"type\": \"Microsoft.KeyVault/vaults/secrets\"\n    }"
  },
  {
    "objectID": "Content/Journal/posts/2019-07-16-AzureFunctionDI.html",
    "href": "Content/Journal/posts/2019-07-16-AzureFunctionDI.html",
    "title": "Azure Functions Dependency Injection",
    "section": "",
    "text": "Azure functions has always been missing the startup sequence enjoyed by other .net core applications. DI is now a first class member and the above post by microsoft explains whats needed well."
  },
  {
    "objectID": "Content/Journal/posts/2019-07-16-AzureFunctionDI.html#reference",
    "href": "Content/Journal/posts/2019-07-16-AzureFunctionDI.html#reference",
    "title": "Azure Functions Dependency Injection",
    "section": "Reference:",
    "text": "Reference:\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-dotnet-dependency-injection"
  },
  {
    "objectID": "Content/Journal/posts/2019-07-16-IFrames.html",
    "href": "Content/Journal/posts/2019-07-16-IFrames.html",
    "title": "IFrames",
    "section": "",
    "text": "What\n\nAn inline frame is used to embed another document within the current HTML document.\n\nHow\n\n&lt;iframe src=\"https://www.w3schools.com\"&gt;&lt;/iframe&gt;\nIFrames are not secure\nRight thats out of the way."
  },
  {
    "objectID": "Content/Journal/posts/2019-07-16-IFrames.html#iframes-and-security",
    "href": "Content/Journal/posts/2019-07-16-IFrames.html#iframes-and-security",
    "title": "IFrames",
    "section": "IFrames and Security",
    "text": "IFrames and Security\nThe element, by itself, is not a security risk. Unfortunately iframes have gotten a bad reputation because they can be used by malicious websites to include content that can infect a visitor‚Äôs computer without them seeing it on the page. This is done by having links point to the invisible scripts and those scripts set off malicious code. The user clicks the link and thinks that the link is broken because nothing appeared to happen, but a script was set off where they couldn‚Äôt see it.\nThe thing to remember when including an IFrame on your web page is that your users are only as safe as the content of all the sites you link to. If you have reason to feel a site is untrustworthy, don‚Äôt link to it in any fashion and most definitely don‚Äôt include its contents in an IFrame. Linking to your own pages within iframes, however, does not pose a security risk for you or your users."
  },
  {
    "objectID": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html",
    "href": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html",
    "title": "SAS Storage Account Access",
    "section": "",
    "text": "When you use shared access signatures in your applications, you need to be aware of two potential risks:\n\nIf a SAS is leaked, it can be used by anyone who obtains it, which can potentially compromise your storage account.\nIf a SAS provided to a client application expires and the application is unable to retrieve a new SAS from your service, then the application‚Äôs functionality may be hindered.\n\nThe following recommendations for using shared access signatures can help mitigate these risks:\n\nAlways use HTTPS to create or distribute a SAS. If a SAS is passed over HTTP and intercepted, an attacker performing a man-in-the-middle attack is able to read the SAS and then use it just as the intended user could have, potentially compromising sensitive data or allowing for data corruption by the malicious user.\nReference stored access policies where possible. Stored access policies give you the option to revoke permissions without having to regenerate the storage account keys. Set the expiration on these very far in the future (or infinite) and make sure it‚Äôs regularly updated to move it farther into the future.\nUse near-term expiration times on an ad hoc SAS. In this way, even if a SAS is compromised, it‚Äôs valid only for a short time. This practice is especially important if you cannot reference a stored access policy. Near-term expiration times also limit the amount of data that can be written to a blob by limiting the time available to upload to it.\nHave clients automatically renew the SAS if necessary. Clients should renew the SAS well before the expiration, in order to allow time for retries if the service providing the SAS is unavailable. If your SAS is meant to be used for a small number of immediate, short-lived operations that are expected to be completed within the expiration period, then this may be unnecessary as the SAS is not expected to be renewed. However, if you have client that is routinely making requests via SAS, then the possibility of expiration comes into play. The key consideration is to balance the need for the SAS to be short-lived (as previously stated) with the need to ensure that the client is requesting renewal early enough (to avoid disruption due to the SAS expiring prior to successful renewal). Be careful with SAS start time. If you set the start time for a SAS to now, then due to clock skew (differences in current time according to different machines), failures may be observed intermittently for the first few minutes. In general, set the start time to be at least 15 minutes in the past. Or, don‚Äôt set it at all, which will make it valid immediately in all cases. The same generally applies to expiry time as well‚Äìremember that you may observe up to 15 minutes of clock skew in either direction on any request. For clients using a REST version prior to 2012-02-12, the maximum duration for a SAS that does not reference a stored access policy is 1 hour, and any policies specifying longer term than that will fail.\nBe specific with the resource to be accessed. A security best practice is to provide a user with the minimum required privileges. If a user only needs read access to a single entity, then grant them read access to that single entity, and not read/write/delete access to all entities. This also helps lessen the damage if a SAS is compromised because the SAS has less power in the hands of an attacker.\nUnderstand that your account will be billed for any usage, including that done with SAS. If you provide write access to a blob, a user may choose to upload a 200GB blob. If you‚Äôve given them read access as well, they may choose to download it 10 times, incurring 2 TB in egress costs for you. Again, provide limited permissions to help mitigate the potential actions of malicious users. Use short-lived SAS to reduce this threat (but be mindful of clock skew on the end time).\nValidate data written using SAS. When a client application writes data to your storage account, keep in mind that there can be problems with that data. If your application requires that data be validated or authorized before it is ready to use, you should perform this validation after the data is written and before it is used by your application. This practice also protects against corrupt or malicious data being written to your account, either by a user who properly acquired the SAS, or by a user exploiting a leaked SAS.\nDon‚Äôt always use SAS. Sometimes the risks associated with a particular operation against your storage account outweigh the benefits of SAS. For such operations, create a middle-tier service that writes to your storage account after performing business rule validation, authentication, and auditing. Also, sometimes it‚Äôs simpler to manage access in other ways. For example, if you want to make all blobs in a container publicly readable, you can make the container Public, rather than providing a SAS to every client for access.\nUse Storage Analytics to monitor your application. You can use logging and metrics to observe any spike in authentication failures due to an outage in your SAS provider service or to the inadvertent removal of a stored access policy. See the Azure Storage Team Blog for additional information."
  },
  {
    "objectID": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html#best-practices-when-using-sas",
    "href": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html#best-practices-when-using-sas",
    "title": "SAS Storage Account Access",
    "section": "",
    "text": "When you use shared access signatures in your applications, you need to be aware of two potential risks:\n\nIf a SAS is leaked, it can be used by anyone who obtains it, which can potentially compromise your storage account.\nIf a SAS provided to a client application expires and the application is unable to retrieve a new SAS from your service, then the application‚Äôs functionality may be hindered.\n\nThe following recommendations for using shared access signatures can help mitigate these risks:\n\nAlways use HTTPS to create or distribute a SAS. If a SAS is passed over HTTP and intercepted, an attacker performing a man-in-the-middle attack is able to read the SAS and then use it just as the intended user could have, potentially compromising sensitive data or allowing for data corruption by the malicious user.\nReference stored access policies where possible. Stored access policies give you the option to revoke permissions without having to regenerate the storage account keys. Set the expiration on these very far in the future (or infinite) and make sure it‚Äôs regularly updated to move it farther into the future.\nUse near-term expiration times on an ad hoc SAS. In this way, even if a SAS is compromised, it‚Äôs valid only for a short time. This practice is especially important if you cannot reference a stored access policy. Near-term expiration times also limit the amount of data that can be written to a blob by limiting the time available to upload to it.\nHave clients automatically renew the SAS if necessary. Clients should renew the SAS well before the expiration, in order to allow time for retries if the service providing the SAS is unavailable. If your SAS is meant to be used for a small number of immediate, short-lived operations that are expected to be completed within the expiration period, then this may be unnecessary as the SAS is not expected to be renewed. However, if you have client that is routinely making requests via SAS, then the possibility of expiration comes into play. The key consideration is to balance the need for the SAS to be short-lived (as previously stated) with the need to ensure that the client is requesting renewal early enough (to avoid disruption due to the SAS expiring prior to successful renewal). Be careful with SAS start time. If you set the start time for a SAS to now, then due to clock skew (differences in current time according to different machines), failures may be observed intermittently for the first few minutes. In general, set the start time to be at least 15 minutes in the past. Or, don‚Äôt set it at all, which will make it valid immediately in all cases. The same generally applies to expiry time as well‚Äìremember that you may observe up to 15 minutes of clock skew in either direction on any request. For clients using a REST version prior to 2012-02-12, the maximum duration for a SAS that does not reference a stored access policy is 1 hour, and any policies specifying longer term than that will fail.\nBe specific with the resource to be accessed. A security best practice is to provide a user with the minimum required privileges. If a user only needs read access to a single entity, then grant them read access to that single entity, and not read/write/delete access to all entities. This also helps lessen the damage if a SAS is compromised because the SAS has less power in the hands of an attacker.\nUnderstand that your account will be billed for any usage, including that done with SAS. If you provide write access to a blob, a user may choose to upload a 200GB blob. If you‚Äôve given them read access as well, they may choose to download it 10 times, incurring 2 TB in egress costs for you. Again, provide limited permissions to help mitigate the potential actions of malicious users. Use short-lived SAS to reduce this threat (but be mindful of clock skew on the end time).\nValidate data written using SAS. When a client application writes data to your storage account, keep in mind that there can be problems with that data. If your application requires that data be validated or authorized before it is ready to use, you should perform this validation after the data is written and before it is used by your application. This practice also protects against corrupt or malicious data being written to your account, either by a user who properly acquired the SAS, or by a user exploiting a leaked SAS.\nDon‚Äôt always use SAS. Sometimes the risks associated with a particular operation against your storage account outweigh the benefits of SAS. For such operations, create a middle-tier service that writes to your storage account after performing business rule validation, authentication, and auditing. Also, sometimes it‚Äôs simpler to manage access in other ways. For example, if you want to make all blobs in a container publicly readable, you can make the container Public, rather than providing a SAS to every client for access.\nUse Storage Analytics to monitor your application. You can use logging and metrics to observe any spike in authentication failures due to an outage in your SAS provider service or to the inadvertent removal of a stored access policy. See the Azure Storage Team Blog for additional information."
  },
  {
    "objectID": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html#authorization",
    "href": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html#authorization",
    "title": "SAS Storage Account Access",
    "section": "Authorization",
    "text": "Authorization\n\nAzure Active Directory\n\nConsidered to be the most secure, as access is keyed to the users AD roles and permissions.\n\nShared Key\n\nThe Shared Key is the ‚ÄòRoot key‚Äô to the storage account, and has access to perform all operations.\nThis should not be shared or distributed.\n\nShared Access Signature\n\nSAS tokens can be generated, and allow access to resources. The SAS tokens can be limited to:-\nTime\nIP address\nUser\n\nAnonymous access to containers and blobs\n\nYou can set the storage account, so that anyone can access blobs or containers. This is obviously the least secure"
  },
  {
    "objectID": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html#reference",
    "href": "Content/Journal/posts/2019-07-31-SAS-StorageAccountAccess.html#reference",
    "title": "SAS Storage Account Access",
    "section": "Reference",
    "text": "Reference\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1"
  },
  {
    "objectID": "Content/Journal/posts/2019-10-16-gRPC.html",
    "href": "Content/Journal/posts/2019-10-16-gRPC.html",
    "title": "gRPC",
    "section": "",
    "text": "Overview\nThis started out as a google project, hence the ‚Äòg‚Äô, but the gRPC community is keen to distance itself from that. gRPC is quickly becoming a standard that getting wide adoption across the industry.\ngRPC makes use of the features of HTTP/2 such as protocol buffers to remotely calls methods deployed on other resources, such as other VM‚Äôs or on different cloud services.\ngRPC allows the developer to make calls to remote methods as if they are local resources, so there is no need for HTTP/REST infrastructure.\ngRPC uses HTTP/2 so takes advantage of the inbuilt TLS message security.\ngRPC is a protocol based service, where the user describes the methods, parameters and return objects, like swagger.\ngRPC application developer, working within a company, should develop a nuget client library, so the client developer doesn‚Äôt need to know that remote calls are happening.\ngRPC, Servers and Clients can be on vastly different architectures or environments.\n\n\nProtocol Buffers\nBy default gRPC works with protocol buffers as originally defined by google, but its equally at home with JSON.\nIf you use Protocol Buffers then you create a simple text file with a .proto extension,\nsyntax = \"proto3\";\n\nservice Greeter {\n  rpc SayHello (HelloRequest) returns (HelloReply);\n}\n\nmessage HelloRequest {\n  string name = 1;\n}\n\nmessage HelloReply {\n  string message = 1;\n}\nThen use the protoc compiler to generate data access classes in your desired language\n\n\nHTTP/2\n\nCompress Headers - HPACK\n\nEvery HTTP request sends headers. HTTP/2 now compresses those headers reducing the package size\n\nMultiplexing\n\nWeb pages can contain 100s of resources, in previous versions of HTTP each resource had to be requested individually in sequential order. With HTTP/2 your requests will be non blocking.\n\nSupported by all major browsers\n\nAll major browsers support HTTP/2, but if your browser doesnt then the protocol will be downgraded to HTTP 1.1\n\n\n\n\nReference\n\ngRPC\n\ngRPC.io\nGoogle gRPC\nMicrosoft gRPC\nYouTube - Blazor\n\nHTTP/2\n\nWikiPedia HTTP/2\nGoogle HTTP/2\nIETF HTTP Working Group\nYouTube - HTTP/2"
  },
  {
    "objectID": "Content/Journal/posts/2020-01-09-Rules.html",
    "href": "Content/Journal/posts/2020-01-09-Rules.html",
    "title": "Rules",
    "section": "",
    "text": "Splitting business rules away for the traffic flow of data has become commonplace in all development for the last few years. It has a lot of advantages as it crystallizes the rules for an object into a simple definable, readable and most of all testable item.\n\n\nBy concentrating the rules into a object it eliminates the need to have a data set in a certain way within a datastore, and then have to go through the procedure and resetting once your done. The developer can simply write a testcase which instantiates the object and load it with with data set in a certain way, and check the result of the rules."
  },
  {
    "objectID": "Content/Journal/posts/2020-01-09-Rules.html#testable",
    "href": "Content/Journal/posts/2020-01-09-Rules.html#testable",
    "title": "Rules",
    "section": "",
    "text": "By concentrating the rules into a object it eliminates the need to have a data set in a certain way within a datastore, and then have to go through the procedure and resetting once your done. The developer can simply write a testcase which instantiates the object and load it with with data set in a certain way, and check the result of the rules."
  },
  {
    "objectID": "Content/Journal/posts/2020-01-09-Rules.html#participants",
    "href": "Content/Journal/posts/2020-01-09-Rules.html#participants",
    "title": "Rules",
    "section": "Participants",
    "text": "Participants\nThe classes and objects participating in this pattern are:\n\nIRule‚Äì Defines the interface for all specific rules.\nIRuleResult‚Äì Defines the interface for the results of all specific rules.\nBaseRule‚Äì The base class provides basic functionality to all rules that inherit from it.\nRule‚Äì The class represents a concrete implementation of the BaseRule class.\nRulesChain‚Äì It is a helper class that contains the main rule for the current conditional statement and the rest of the conditional chain of rules.\nRulesEvaluator‚Äì This is the main class that supports the creation of readable rules and their relation. It evaluates the rules and returns their results."
  },
  {
    "objectID": "Content/Journal/posts/2020-11-19-AdaptiveCard.html",
    "href": "Content/Journal/posts/2020-11-19-AdaptiveCard.html",
    "title": "Adaptive Cards",
    "section": "",
    "text": "Adaptive Cards is a Json structure which describes how to display information in various Microsoft Applications. Currently the standard is at version 1.2, and the definition can be seen here https://AdaptiveCards.io"
  },
  {
    "objectID": "Content/Journal/posts/2020-11-19-AdaptiveCard.html#result",
    "href": "Content/Journal/posts/2020-11-19-AdaptiveCard.html#result",
    "title": "Adaptive Cards",
    "section": "Result",
    "text": "Result\n\n\n\nTeams-AdaptiveCard"
  },
  {
    "objectID": "Content/Journal/posts/2020-11-19-AdaptiveCard.html#json",
    "href": "Content/Journal/posts/2020-11-19-AdaptiveCard.html#json",
    "title": "Adaptive Cards",
    "section": "Json",
    "text": "Json\n{\n    \"type\": \"message\",\n    \"attachments\": [\n        {\n            \"contentType\": \"application/vnd.microsoft.card.adaptive\",\n            \"contentUrl\": null,\n            \"content\": {\n                \"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\",\n                \"type\": \"AdaptiveCard\",\n                \"version\": \"1.2\",\n                \"body\": [\n                    {\n                        \"type\": \"TextBlock\",\n                        \"size\": \"Medium\",\n                        \"weight\": \"Bolder\",\n                        \"text\": \"Image WaterMark Exception\"\n                    },\n                    {\n                        \"type\": \"ColumnSet\",\n                        \"columns\": [\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"Image\",\n                                        \"style\": \"Default\",\n                                        \"url\": \"https://www.knightfrank.com/library/v3.0/images/knightfranklogo.png\",\n                                        \"size\": \"Large\"\n                                    }\n                                ],\n                                \"width\": \"auto\"\n                            },\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"TextBlock\",\n                                        \"weight\": \"Bolder\",\n                                        \"text\": \"WaterMark Validator\",\n                                        \"wrap\": true\n                                    },\n                                    {\n                                        \"type\": \"TextBlock\",\n                                        \"spacing\": \"None\",\n                                        \"text\": \"Created {{DATE(2017-02-14T06:08:39Z,SHORT)}}\",\n                                        \"isSubtle\": true,\n                                        \"wrap\": true\n                                    }\n                                ],\n                                \"width\": \"stretch\"\n                            }\n                        ]\n                    },\n                    {\n                        \"type\": \"ColumnSet\",\n                        \"columns\": [\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"Image\",\n                                        \"style\": \"Default\",\n                                        \"url\": \"https://content.knightfrank.com/property/cbm190150/images/b2bee1c5-3dfa-4bbd-9d13-94cde2044822-0.jpg?cio=true&w=730\",\n                                        \"size\": \"Large\"\n                                    }\n                                ],\n                                \"width\": \"auto\"\n                            },\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"TextBlock\",\n                                        \"text\": \"A Watermark has been detected in this image\",\n                                        \"wrap\": true\n                                    }\n                                ],\n                                \"width\": \"stretch\"\n                            }\n                        ]\n                    }\n                ],\n                \"actions\": [\n                    {\n                        \"type\": \"Action.OpenUrl\",\n                        \"title\": \"View\",\n                        \"url\": \"https://hub.knightfrank.com/#/app/activity/view/60c1d8fb-1bd7-ea11-a95a-000d3ab2efee?tabname=Marketing\"\n\n                    }\n                ]\n            }\n        }\n    ]\n}"
  },
  {
    "objectID": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html",
    "href": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html",
    "title": "User Assigned Managed Identities",
    "section": "",
    "text": "A Managed Identity is a Azure resource which comprises two components. * A resource. A resource will create a Identity in Azure AD * An Azure Role\nTherefore by giving a AD registered App the Managed Identity Role, when the app trys to connect Ad will give permission without the need for any auth flow credentials to be passed.\nManaged identities eradicate the need to manage credentials within your code. A components identity is lodged within Active Directory, and makes available a AD Token for other componets to consume. The AD token gives the consuming compoent access rights to the original resource.\nThere are two types of Mangaged Identities, System assigned and User assigned. System assigned identities are tied to the resource that creates them, so when the resource is deleted, so is the identity, whereas user assigned identities exist idependantly whithin AD and can be assign to several resources.\ne.g As in the example below. A User Assigned managed identity is created and both a Azure Function and Azure Sql Server share the identity."
  },
  {
    "objectID": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html#variables",
    "href": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html#variables",
    "title": "User Assigned Managed Identities",
    "section": "Variables",
    "text": "Variables\n    \"var_id_name\": \"[concat(variables('namingConvention').prefixes.Identity, '-', variables('var_application_name_delim'),  parameters('para_acronym_region'))]\",\n    \"var_uaid_name\": \"[concat('/subscriptions/',variables('var_sub_id'),'/resourcegroups/', resourceGroup().Name, '/providers/Microsoft.ManagedIdentity/userAssignedIdentities/', tolower(variables('var_id_name')))]\""
  },
  {
    "objectID": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html#ctrate-resources",
    "href": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html#ctrate-resources",
    "title": "User Assigned Managed Identities",
    "section": "Ctrate Resources",
    "text": "Ctrate Resources\nThe following ARM snippets show how you define and assign a User Assigned Identity to a Azure Function and Azure Sql Server, this will allow the Azure function to access the server.\n\nCreate a User Assigned Managed Identity\n    {\n      \"type\": \"Microsoft.ManagedIdentity/userAssignedIdentities\",\n      \"apiVersion\": \"2018-11-30\",\n      \"name\": \"[variables('var_id_name')]\",\n      \"location\": \"[resourceGroup().location]\",\n      \"tags\": {\n        \"displayName\": \"Managed Identity\"\n      }\n    },\n\n\nCreate a Azure Function, with both a System and User Assigned Identity\n    {\n      \"apiVersion\": \"2021-02-01\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Web/serverfarms/', variables('var_svcpln_name'))]\"\n      ],\n      \"identity\": {\n        \"type\": \"SystemAssigned, UserAssigned\",\n        \"userAssignedIdentities\": {\n          \"[variables('var_uaid_name')]\": {}\n        }\n      },\n      \"kind\": \"functionapp\",\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_azf_name')]\",\n      \"properties\": {\n        \"state\": \"[parameters('para_funcState')]\",\n        \"name\": \"[variables('var_azf_name')]\",\n        \"siteConfig\": {\n          \"alwaysOn\": \"[parameters('para_alwaysOn')]\"\n        },\n        \"clientAffinityEnabled\": false,\n        \"serverFarmId\": \"[variables('var_svcpln_name')]\",\n        \"hostNameSslStates\": [],\n        \"httpsOnly\": true\n      },\n      \"resources\": [\n      ],\n      \"tags\": {\n        \"displayName\": \"Az Function\"\n      },\n      \"type\": \"Microsoft.Web/sites\"\n    },\n\n\nCreate a Azure Sql Server with User Assigned Identity\n    {\n      \"type\": \"Microsoft.Sql/servers\",\n      \"apiVersion\": \"2021-02-01-preview\",\n      \"dependsOn\": [\n        \"[resourceId('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\"\n      ],\n      \"name\": \"[variables('var_sql_name')]\",\n      \"location\": \"eastus\",\n      \"kind\": \"v12.0\",\n      \"identity\": {\n        \"type\": \"UserAssigned\",\n        \"userAssignedIdentities\": {\n          \"[variables('var_uaid_name')]\": {}\n        }\n      },\n      \"properties\": {\n        \"administratorLogin\": \"[parameters('para_dbUsr')]\",\n        \"administratorLoginPassword\": \"[parameters('para_dbPwd')]\",\n        \"version\": \"12.0\",\n        \"minimalTlsVersion\": \"1.2\",\n        \"publicNetworkAccess\": \"Enabled\",\n        \"primaryUserAssignedIdentityId\": \"[variables('var_uaid_name')]\",\n        \"administrators\": {\n          \"administratorType\": \"ActiveDirectory\",\n          \"principalType\": \"Group\",\n          \"login\": \"Hub-Sec-DB-Grp\",\n          \"sid\": \"1b2a41cc-232c-4d73-9b30-9159697bec2d\",\n          \"tenantId\": \"55a71488-bbff-4451-a18d-a1bfa479293b\"\n        },\n        \"restrictOutboundNetworkAccess\": \"Disabled\"\n      },\n      \"tags\": {\n        \"displayName\": \"Sql Server\"\n      }\n    },"
  },
  {
    "objectID": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html#azure-function",
    "href": "Content/Journal/posts/2022-07-21-UserAssignManagedIdentities.html#azure-function",
    "title": "User Assigned Managed Identities",
    "section": "Azure Function",
    "text": "Azure Function\nThe following snippet shows how you can access the Azure Sql Server from a Azure Function. notice how the connection string indicated that authentication is handled by the managed identity.\nSo on a connection being made Sql with take a Identity token pased by the function and validate it with identity in AD.\npublic static class Function1\n{\n    private static SqlConnection connection = new SqlConnection();\n\n    [FunctionName(\"Function1\")]\n    public static async Task&lt;IActionResult&gt; Run(\n        [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)] HttpRequest req,\n        ILogger log)\n    {\n        try\n        {\n            connection.ConnectionString = \"Server=sql-mifunctest-vse-ne.database.windows.net; Authentication=Active Directory Managed Identity; Database=MiFuncTest\";\n            await connection.OpenAsync();\n            var cmd = connection.CreateCommand();\n            cmd.CommandText = \"select * from [dbo].[WebEnquiry]\";\n\n            var response = await cmd.ExecuteReaderAsync();\n\n            var result = \"\";\n            while (response.Read())\n            { \n                Console.WriteLine(response[\"Id\"].ToString());\n                result += response[\"Id\"].ToString() + \"\\n\";\n            }\n            response.Close();\n\n            return new OkObjectResult($\"The database connection is: {connection.State}  Result {result}\");\n        }\n        catch (SqlException sqlex)\n        {\n            return new OkObjectResult($\"The following SqlException happened: {sqlex.Message}\");\n        }\n        catch (Exception ex)\n        {\n            return new OkObjectResult($\"The following Exception happened: {ex.Message}\");\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/2023-01-19-Upload AV File Scanning.html",
    "href": "Content/Journal/posts/2023-01-19-Upload AV File Scanning.html",
    "title": "Web Upload File AV Scanning",
    "section": "",
    "text": "Description\nWeb file uploads within a company has been a neglected space for a while now, as any file the appears on a laptop or via email is scanned by the inbuild Anti Virus software, but with new projects opening up internal strorage to external entities via web portals we have to consider the fact that somone could upload a file containing a virus and we would be none the wiser. For us to write a Anti Virus scanning tool is out of the question, even employing a piece of opensource software is not advisable, as we have no idea how upto date the virus templates are, or even if they are valid.\nA solution would be to send any prospective file to a external third party who specialises in file scanning and verification. A possible solution is descibed below.\n\n\n\nA Solution\n\nA file is uploaded via a a Wb Upload tool.\nThe web application registers its interest in file upload statuses with the SignalR handler\nThe file is passed to the Document API\nThe Document API passes the document to a Third Party AV scanning company.\n\nBecause a document could contain more than one file, It could take the Third Party a while to scan the complete document, so they normally make use of a Webhook interface so they can asynchronously send back a response.\n\nThe Document API receives a response and posts a success or fail message to the Event Grid File Upload Topic.\nIf the AV response was a success the file is written to the blob storage.\n\nThe notification pattern of EventGrid Topic/ Azure Function / Azure SignalR is a standard pattern. There is also a pattern for securing webhooks, although webhooks are not loved by all."
  },
  {
    "objectID": "Content/Journal/posts/2023-01-20-Event Notification.html",
    "href": "Content/Journal/posts/2023-01-20-Event Notification.html",
    "title": "Event Notificiation",
    "section": "",
    "text": "Azure Event Grid with Topics is a enterprise grade notification engine which mimics Azure Service Bus in many ways, Where it differes from Service Bus is that it is a serverless component, and has no physical storage of messages, so cannot, in the event of a incident, gaurentee delivery. What id does however offer in a normal operating environment is a fast, low latency delivery of messages that can be consumed easily.\nThe following description shows a particular implementation of event grid messaging, which is used in conjunction with Azure SignalR so that a web GUI can receive notifications.\n\n\nThe Web page gets the SignalR connection from the Azure Function.\nThe Event Source publishes a message to a Event Grid Topic.\nThe Azure Function subscribes to a particular topic, the Event Grid can have many topics.\nThe Azure Function publishes the message to SignalR\nThe Web page receive the message from the signalR WebSocket."
  },
  {
    "objectID": "Content/Journal/posts/2023-01-20-Event Notification.html#azure-function",
    "href": "Content/Journal/posts/2023-01-20-Event Notification.html#azure-function",
    "title": "Event Notificiation",
    "section": "Azure Function",
    "text": "Azure Function\n    public static class CloudEventSubscription\n    {\n        // Azure Function for handling negotation protocol for SignalR. It returns a connection info\n        // that will be used by Client applications to connect to the SignalR service.\n        // It is recommended to authenticate this Function in production environments.\n        [FunctionName(\"negotiate\")]\n        public static SignalRConnectionInfo GetSignalRInfo(\n            [HttpTrigger(AuthorizationLevel.Anonymous, \"post\")] HttpRequest req,\n            [SignalRConnectionInfo(HubName = \"cloudEventSchemaHub\")] SignalRConnectionInfo connectionInfo,\n            ILogger log)\n        {\n            log.LogInformation(\"negotiate\");\n            return connectionInfo;\n        }\n\n        // Azure Function for handling Event Grid events using CloudEventSchema v1.0 \n        // (see CloudEvents Specification: https://github.com/cloudevents/spec)\n        [FunctionName(\"EventSubscription\")]\n        public static async Task&lt;IActionResult&gt; Run(\n            [HttpTrigger(AuthorizationLevel.Anonymous, \"POST\", \"OPTIONS\", Route = null)] HttpRequest req,\n            [SignalR(HubName = \"cloudEventSchemaHub\")] IAsyncCollector&lt;SignalRMessage&gt; signalRMessages,\n            ILogger log)\n        {\n            log.LogInformation(\"CloudEventSubscription\");\n\n            // Handle EventGrid subscription validation for CloudEventSchema v1.0.\n            // It sets the header response `Webhook-Allowed-Origin` with the value from \n            // the header request `Webhook-Request-Origin` \n            // (see: https://docs.microsoft.com/en-us/azure/event-grid/cloudevents-schema#use-with-azure-functions)\n            if (HttpMethods.IsOptions(req.Method))\n            {\n                log.LogInformation(\"CloudEventSubscription - Options\");\n                if (req.Headers.TryGetValue(\"Webhook-Request-Origin\", out var headerValues))\n                {\n                    log.LogInformation(\"CloudEventSubscription - Webhook-Request-Origin\");\n                    var originValue = headerValues.FirstOrDefault();\n                    if(!string.IsNullOrEmpty(originValue))\n                    {\n                        req.HttpContext.Response.Headers.Add(\"Webhook-Allowed-Origin\", originValue);\n                        return new OkResult();\n                    }\n                    log.LogInformation(\"CloudEventSubscription - Missing Webhook-Request-Origin\");\n                    return new BadRequestObjectResult(\"Missing 'Webhook-Request-Origin' header when validating\");\n                }\n            }\n            \n            // Handle an event received from EventGrid. It reads the event from the request payload and send \n            // it to the SignalR serverless service using the Azure Function output binding\n            if(HttpMethods.IsPost(req.Method)) \n            {\n                string @event = await new StreamReader(req.Body).ReadToEndAsync();\n                await signalRMessages.AddAsync(new SignalRMessage\n                {\n                    Target = \"newEvent\",\n                    Arguments = new[] { @event }\n                });\n            }\n\n            log.LogInformation(\"CloudEventSubscription - SignalR Post\");\n            return new OkResult();\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/2023-01-20-Event Notification.html#angular",
    "href": "Content/Journal/posts/2023-01-20-Event Notification.html#angular",
    "title": "Event Notificiation",
    "section": "Angular",
    "text": "Angular\nimport { Component } from '@angular/core';\nimport * as SignalR from '@microsoft/signalr';\n\n@Component({\n  selector: 'app-root',\n  templateUrl: './app.component.html',\n  styleUrls: ['./app.component.css']\n})\nexport class AppComponent {\n\n  title = 'viewer-app';\n  events: string[] = [];\n\n  private hubConnection: SignalR.HubConnection;\n\n  constructor() {\n    // Create connection\n    this.hubConnection = new SignalR.HubConnectionBuilder()\n      .withUrl(\"https://func-pocegsr-vse-ne.azurewebsites.net/api/\")\n      .build();\n\n    // Start connection. This will call negotiate endpoint\n    this.hubConnection\n      .start();\n\n    // Handle incoming events for the specific target\n    this.hubConnection.on(\"newEvent\", (event) =&gt; {\n      this.events.push(event);\n    });\n  }\n}"
  },
  {
    "objectID": "Content/Journal/posts/2023-01-20-Event Notification.html#arm-template",
    "href": "Content/Journal/posts/2023-01-20-Event Notification.html#arm-template",
    "title": "Event Notificiation",
    "section": "Arm Template",
    "text": "Arm Template\n    {\n      \"type\": \"Microsoft.SignalRService/SignalR\",\n      \"apiVersion\": \"2022-02-01\",\n      \"name\": \"[variables('var_sr_name')]\",\n      \"location\": \"[resourceGroup().location]\",\n      \"sku\": {\n        \"name\": \"Standard_S1\",\n        \"tier\": \"Standard\",\n        \"size\": \"S1\",\n        \"capacity\": 1\n      },\n      \"kind\": \"SignalR\",\n      \"properties\": {\n        \"tls\": {\n          \"clientCertEnabled\": false\n        },\n        \"features\": [\n          {\n            \"flag\": \"ServiceMode\",\n            \"value\": \"Serverless\",\n            \"properties\": {}\n          },\n          {\n            \"flag\": \"EnableConnectivityLogs\",\n            \"value\": \"True\",\n            \"properties\": {}\n          }\n        ],\n        \"cors\": {\n          \"allowedOrigins\": [\n            \"*\"\n          ]\n        },\n        \"upstream\": {},\n        \"networkACLs\": {\n          \"defaultAction\": \"Deny\",\n          \"publicNetwork\": {\n            \"allow\": [\n              \"ServerConnection\",\n              \"ClientConnection\",\n              \"RESTAPI\",\n              \"Trace\"\n            ]\n          },\n          \"privateEndpoints\": []\n        },\n        \"publicNetworkAccess\": \"Enabled\",\n        \"disableLocalAuth\": false,\n        \"disableAadAuth\": false\n      },\n      \"tags\": {\n        \"displayName\": \"SignalR\"\n      }\n    },\n    {\n      \"type\": \"Microsoft.EventGrid/topics\",\n      \"apiVersion\": \"2021-12-01\",\n      \"name\": \"[variables('var_egt_name')]\",\n      \"location\": \"[resourceGroup().location]\",\n      \"identity\": {\n        \"type\": \"None\"\n      },\n      \"properties\": {\n        \"inputSchema\": \"CloudEventSchemaV1_0\",\n        \"publicNetworkAccess\": \"Enabled\",\n        \"inboundIpRules\": [],\n        \"disableLocalAuth\": false\n      },\n      \"tags\": {\n        \"displayName\": \"Event Grid Topic\"\n      }\n    },"
  },
  {
    "objectID": "Content/Journal/posts/2023-06-22-WebUI.html",
    "href": "Content/Journal/posts/2023-06-22-WebUI.html",
    "title": "Web UI",
    "section": "",
    "text": "A web application, also known as a web app, is a software application that runs on web browsers and is accessed over the internet. It is designed to provide interactive and dynamic functionality to users, like traditional desktop applications, but with the advantage of being platform-independent and accessible from any device with a web browser.\nWeb applications are typically built using web technologies such as HTML (Hypertext Markup Language), CSS (Cascading Style Sheets), and JavaScript, which enable the creation of interactive user interfaces and the manipulation of data. The application‚Äôs logic is executed on the server and/or client side, depending on the architecture employed.\nWeb applications can vary widely in their complexity and functionality, ranging from simple websites with basic forms and interactivity to more advanced applications that involve complex data processing, real-time communication, and collaboration. Examples of web applications include online banking systems, social media platforms, e-commerce websites, project management tools, and web-based email clients.\nOne of the key advantages of web applications is their ability to deliver updates and improvements seamlessly, as they are centrally hosted and accessed over the internet. This eliminates the need for users to manually install software updates, resulting in a more efficient and user-friendly experience.\n\n\nSSR (Server-Side Rendering), SSG (Static Site Generation), SPA (Single-Page Application), PWA (Progressive Web Application), Isomorphic, and Micro Frontend architectures are different approaches to building web applications, each with its own characteristics and benefits.\n\n\nSSR is an approach where the server generates the complete HTML for each request and sends it to the client. The client receives a fully rendered page, including content and data. SSR is beneficial for improving initial page load time and search engine optimization (SEO) since search engines can easily crawl and index the content.\n\n\n\n\nSSG is a method of generating static HTML files during the build process based on predefined templates and data. These static files can then be served to clients without the need for server-side processing. SSG provides fast page loads, reduces server load, and enables caching benefits. It is well-suited for content-based websites and blogs with less dynamic content.\n\n\n\n\nSPA is an application that loads a single HTML page initially and then dynamically updates its content using JavaScript, typically through API calls. The client-side JavaScript handles the rendering of views and the manipulation of data. SPAs provide a smooth and responsive user experience, as they avoid full page reloads. Examples include Gmail and Trello.\n\n\n\n\nPWA is an application that combines the features of web and mobile apps. PWAs are built using web technologies and provide an app-like experience to users, including offline capabilities, push notifications, and access to device features. PWAs can be installed on the user‚Äôs device and are accessible through the home screen, bridging the gap between web and native apps.\n\n\n\n\nIsomorphic applications aim to create a consistent codebase that can run both on the server and the client. The goal is to share as much code as possible between the server and the client, enabling server-side rendering for improved initial load times and search engine friendliness while still providing interactive client-side functionality.\n\n\n\n\nMicro Frontend is an architectural approach that focuses on breaking down a web application into smaller, self-contained frontend modules. Each module can be developed, tested, and deployed independently, often by different teams. Micro Frontend allows for better scalability, flexibility, and autonomy in the frontend development process.\n\nIt‚Äôs important to note that these architectures are not mutually exclusive, and elements of each can be combined to suit specific project requirements. The choice of architecture depends on factors such as application complexity, performance needs, SEO requirements, development team structure, and user experience goals.\n\n\n\n\nThe web has got more complicated, having a development team that can keep pace with the development and testing approaches, code structures and framework evolutions is difficult and probably very expensive. Multiple teams, different focuses Having one team develop a web application where everyone uses one JavaScript framework for the GUI and .NET for the APIs can have several benefits: 1. Consistency: Using a single JavaScript framework for the GUI ensures consistent code style, architecture, and patterns across the frontend components. This simplifies collaboration, code reviews, and maintenance tasks within the development team. 2. Efficient Communication: When the GUI and APIs are developed by the same team, communication between frontend and backend developers becomes more seamless. Developers can coordinate closely, align requirements, and address issues promptly. 3. Shared Expertise: By focusing on a single JavaScript framework, team members can deepen their expertise in that particular framework. This can result in faster development, efficient problem-solving, and improved overall quality of the frontend code. 4. Seamless Integration: .NET is a versatile framework that offers excellent integration capabilities with various APIs and services. When the same team handles both the frontend and backend, they can ensure smooth communication and data flow between the GUI and the APIs, leading to a better overall user experience. 5. Faster Development Iterations: With a unified team and shared knowledge, development iterations can be accelerated. Developers can iterate and deploy changes more quickly, as they have a deep understanding of both the frontend and backend components. This agility can be beneficial for meeting project deadlines and adapting to evolving requirements. However, there are some considerations to keep in mind: 1. Expertise Balance: It‚Äôs important to ensure that the team has a balanced skill set, with expertise in both the chosen JavaScript framework and .NET for APIs. If the team members are primarily skilled in one area and lack proficiency in the other, it could lead to inefficiencies or a compromise in the quality of either the frontend or backend development. 2. Scalability: As the application grows and becomes more complex, a single team may face challenges in managing both the frontend and backend aspects simultaneously. In such cases, it may be necessary to divide the team or consider specialized roles to ensure efficient development and maintenance. 3. Flexibility and Adaptability: Relying on a single JavaScript framework and .NET for APIs may limit the team‚Äôs flexibility when it comes to incorporating new technologies or frameworks that may better suit specific requirements. It‚Äôs important to periodically evaluate and adapt the technology stack to stay up-to-date with industry trends and evolving project needs. Ultimately, the decision to have one team develop both the GUI and APIs using specific technologies depends on factors such as the team‚Äôs expertise, project requirements, timelines, and scalability considerations.\nIn Hub 1.0 we have encountered all the negative considerations detailed above, so I‚Äôd advise against having multi-disciplinary developers, rather we should have Web GUI and API developers who specialise in their areas."
  },
  {
    "objectID": "Content/Journal/posts/2023-06-22-WebUI.html#web-ui-architectures",
    "href": "Content/Journal/posts/2023-06-22-WebUI.html#web-ui-architectures",
    "title": "Web UI",
    "section": "",
    "text": "SSR (Server-Side Rendering), SSG (Static Site Generation), SPA (Single-Page Application), PWA (Progressive Web Application), Isomorphic, and Micro Frontend architectures are different approaches to building web applications, each with its own characteristics and benefits.\n\n\nSSR is an approach where the server generates the complete HTML for each request and sends it to the client. The client receives a fully rendered page, including content and data. SSR is beneficial for improving initial page load time and search engine optimization (SEO) since search engines can easily crawl and index the content.\n\n\n\n\nSSG is a method of generating static HTML files during the build process based on predefined templates and data. These static files can then be served to clients without the need for server-side processing. SSG provides fast page loads, reduces server load, and enables caching benefits. It is well-suited for content-based websites and blogs with less dynamic content.\n\n\n\n\nSPA is an application that loads a single HTML page initially and then dynamically updates its content using JavaScript, typically through API calls. The client-side JavaScript handles the rendering of views and the manipulation of data. SPAs provide a smooth and responsive user experience, as they avoid full page reloads. Examples include Gmail and Trello.\n\n\n\n\nPWA is an application that combines the features of web and mobile apps. PWAs are built using web technologies and provide an app-like experience to users, including offline capabilities, push notifications, and access to device features. PWAs can be installed on the user‚Äôs device and are accessible through the home screen, bridging the gap between web and native apps.\n\n\n\n\nIsomorphic applications aim to create a consistent codebase that can run both on the server and the client. The goal is to share as much code as possible between the server and the client, enabling server-side rendering for improved initial load times and search engine friendliness while still providing interactive client-side functionality.\n\n\n\n\nMicro Frontend is an architectural approach that focuses on breaking down a web application into smaller, self-contained frontend modules. Each module can be developed, tested, and deployed independently, often by different teams. Micro Frontend allows for better scalability, flexibility, and autonomy in the frontend development process.\n\nIt‚Äôs important to note that these architectures are not mutually exclusive, and elements of each can be combined to suit specific project requirements. The choice of architecture depends on factors such as application complexity, performance needs, SEO requirements, development team structure, and user experience goals."
  },
  {
    "objectID": "Content/Journal/posts/2023-06-22-WebUI.html#people",
    "href": "Content/Journal/posts/2023-06-22-WebUI.html#people",
    "title": "Web UI",
    "section": "",
    "text": "The web has got more complicated, having a development team that can keep pace with the development and testing approaches, code structures and framework evolutions is difficult and probably very expensive. Multiple teams, different focuses Having one team develop a web application where everyone uses one JavaScript framework for the GUI and .NET for the APIs can have several benefits: 1. Consistency: Using a single JavaScript framework for the GUI ensures consistent code style, architecture, and patterns across the frontend components. This simplifies collaboration, code reviews, and maintenance tasks within the development team. 2. Efficient Communication: When the GUI and APIs are developed by the same team, communication between frontend and backend developers becomes more seamless. Developers can coordinate closely, align requirements, and address issues promptly. 3. Shared Expertise: By focusing on a single JavaScript framework, team members can deepen their expertise in that particular framework. This can result in faster development, efficient problem-solving, and improved overall quality of the frontend code. 4. Seamless Integration: .NET is a versatile framework that offers excellent integration capabilities with various APIs and services. When the same team handles both the frontend and backend, they can ensure smooth communication and data flow between the GUI and the APIs, leading to a better overall user experience. 5. Faster Development Iterations: With a unified team and shared knowledge, development iterations can be accelerated. Developers can iterate and deploy changes more quickly, as they have a deep understanding of both the frontend and backend components. This agility can be beneficial for meeting project deadlines and adapting to evolving requirements. However, there are some considerations to keep in mind: 1. Expertise Balance: It‚Äôs important to ensure that the team has a balanced skill set, with expertise in both the chosen JavaScript framework and .NET for APIs. If the team members are primarily skilled in one area and lack proficiency in the other, it could lead to inefficiencies or a compromise in the quality of either the frontend or backend development. 2. Scalability: As the application grows and becomes more complex, a single team may face challenges in managing both the frontend and backend aspects simultaneously. In such cases, it may be necessary to divide the team or consider specialized roles to ensure efficient development and maintenance. 3. Flexibility and Adaptability: Relying on a single JavaScript framework and .NET for APIs may limit the team‚Äôs flexibility when it comes to incorporating new technologies or frameworks that may better suit specific requirements. It‚Äôs important to periodically evaluate and adapt the technology stack to stay up-to-date with industry trends and evolving project needs. Ultimately, the decision to have one team develop both the GUI and APIs using specific technologies depends on factors such as the team‚Äôs expertise, project requirements, timelines, and scalability considerations.\nIn Hub 1.0 we have encountered all the negative considerations detailed above, so I‚Äôd advise against having multi-disciplinary developers, rather we should have Web GUI and API developers who specialise in their areas."
  },
  {
    "objectID": "Content/Journal/posts/2023-09-27-VisualStudioSolutioTemplate.html",
    "href": "Content/Journal/posts/2023-09-27-VisualStudioSolutioTemplate.html",
    "title": "Visual Studio Solution Template",
    "section": "",
    "text": "Create a new Solution\nCreate your directory structure\nAdd relervant Projects in the directory structure\n\nExport each project using the Project-&gt;Export Template menu item\n\nThis will create a zip file.\n\nRepeat for all Projects.\nGo to the directory where the zips have been created\n\nC:{user}- Knight FrankStudio {version}Exported Templates\n\nIn this directory create your folder structure\n\n\n\n\n\n\n\n\n\nExtract the Zip files into the relervant directory.\nIn the empty directories create a Read.Me text file.\n\nIf you dont then the directory will not be preserved.\n\nin the root create a .vstemplate xml file\nAdd the following\n\nUpdate the Name\nDescription\nProject Type, if necceary\nAdd any new solution folders\nAdd any new vstemplates\n\n\n&lt;VSTemplate Version=\"2.0.0\" Type=\"ProjectGroup\"\n    xmlns=\"http://schemas.microsoft.com/developer/vstemplate/2005\"&gt;\n    &lt;TemplateData&gt;\n        &lt;Name&gt;CSharp, Az Function, Blazor&lt;/Name&gt;\n        &lt;Description&gt;Blazor, Azure Function Template&lt;/Description&gt;\n        &lt;Icon&gt;__TemplateIcon.PNG&lt;/Icon&gt;\n        &lt;ProjectType&gt;CSharp&lt;/ProjectType&gt;\n    &lt;/TemplateData&gt;\n    &lt;TemplateContent&gt;\n        &lt;ProjectCollection&gt;\n            &lt;SolutionFolder Name=\"Build\"&gt;\n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Deploy\"&gt;\n                &lt;ProjectTemplateLink ProjectName=\"Deploy\"&gt;\n                    Deploy\\MyTemplate.vstemplate\n                &lt;/ProjectTemplateLink&gt;          \n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Src\"&gt;\n                &lt;SolutionFolder Name=\"API\"&gt;\n                    &lt;ProjectTemplateLink ProjectName=\"API\"&gt;\n                        Src\\API\\MyTemplate.vstemplate\n                    &lt;/ProjectTemplateLink&gt;                  \n                &lt;/SolutionFolder&gt;   \n                &lt;SolutionFolder Name=\"Client\"&gt;\n                    &lt;ProjectTemplateLink ProjectName=\"Client\"&gt;\n                        Src\\Client\\MyTemplate.vstemplate\n                    &lt;/ProjectTemplateLink&gt;                  \n                &lt;/SolutionFolder&gt;                   \n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Tests\"&gt;\n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Wiki\"&gt;\n            &lt;/SolutionFolder&gt;\n        &lt;/ProjectCollection&gt;\n    &lt;/TemplateContent&gt;\n&lt;/VSTemplate&gt;\n\nSelect all the files and save as a compressed ZIP\nCopy the file Zip file to C:{user}- Knight FrankStudio {version}"
  },
  {
    "objectID": "Content/Journal/posts/2024-06-14-SOLID.html",
    "href": "Content/Journal/posts/2024-06-14-SOLID.html",
    "title": "SOLID",
    "section": "",
    "text": "S - Single responsibility\nO - Open Closed\nL - Liskov Substitution\nI - Interface Segregation\nD - Dependency Inversion\n\n\nSingle Responsibility\nA class should have one and only one reason to change, meaning that a class should have only one job.\n\n\nOpen Closed\nObjects or entities should be open for extension but closed for modification.\n\n\nLiskov Substitution\nLet q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T.\nThis means that every subclass or derived class should be substitutable for their base or parent class.\n\n\nInterface Segregation\nA client should never be forced to implement an interface that it doesn‚Äôt use, or clients shouldn‚Äôt be forced to depend on methods they do not use.\n\n\nDependency Inversion\nEntities must depend on abstractions, not on concretions. It states that the high-level module must not depend on the low-level module, but they should depend on abstractions."
  },
  {
    "objectID": "Content/Journal/posts/2025-09-26-Diabetes.html",
    "href": "Content/Journal/posts/2025-09-26-Diabetes.html",
    "title": "Diabetes",
    "section": "",
    "text": "Been trying for years to loose weight, but this summer I had to stop exercising as I had a sciatic nerve issue, twice, which took some time to recover from. So by september I‚Äôd lost 10 Kg, no big thing, over the a four month time period thats what all the diets aim for isnt it. So the doctor sent me for some blood tests and I went on holiday. My holiday was interrupted by calls from the GP to get in contact, which I did on return.\nMy HbA1C levels where at 106, normal levels are below 46 mmol/mol. So I‚Äôm diabetic :(\nNow I have this !! \nNot sure which way is up anymore."
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/TypeSpec.html",
    "href": "Content/Journal/posts/APIFirst/TypeSpec.html",
    "title": "TypeSpec",
    "section": "",
    "text": "TypeSpec is a powerful tool for designing and generating APIs. It allows developers to define the structure and behavior of their APIs using a simple and intuitive syntax. TypeSpec supports a wide range of features, including:\n\nDefining endpoints and routes\n\nSpecifying request and response formats\nAdding authentication and authorization\nGenerating client and server code in multiple programming languages\nCreating interactive documentation for APIs\nValidating API definitions against industry standards such as OpenAPI and Swagger\nExtending TypeSpec with custom plugins and extensions\n\nTypeSpec is designed to be easy to use and understand, making it accessible to developers of all skill levels. It also integrates seamlessly with other tools and frameworks, such as AutoRest, making it a versatile choice for API development.\nTypeSpec can be used in a variety of scenarios, including:\n\nBuilding RESTful APIs for web and mobile applications\nCreating microservices and serverless functions\nDesigning APIs for data analytics and machine learning applications\nGenerating SDKs and client libraries for third-party developers\nAutomating API documentation and testing processes\nPrototyping and iterating on API designs quickly and efficiently\nIntegrating with CI/CD pipelines for continuous delivery of APIs\nEnsuring compliance with industry regulations and standards for API security and privacy\nFacilitating API versioning and backward compatibility for long-term maintenance\nSupporting multiple API styles, including REST, GraphQL, and gRPC\nEnabling rapid development and deployment of APIs in cloud-native environments"
  },
  {
    "objectID": "Content/Journal/posts/AzureStaticWebsites/index.html",
    "href": "Content/Journal/posts/AzureStaticWebsites/index.html",
    "title": "Azure Static Websites",
    "section": "",
    "text": "Microsoft announced that you can now enable static websites on a storage account. This will generate a new URL for your site, and enable read access to any static html files within the blob storage. There‚Äôs a link to Microsoft‚Äôs preview announcement here. https://azure.microsoft.com/en-us/blog/azure-storage-static-web-hosting-public-preview/\nAt the moment I prefer the approach made by Anthony Chu in his blog post https://anthonychu.ca/post/azure-functions-static-file-server/\nHe hosts a index.html file within a Azure function Http Trigger request. Although this only returns a static file, it does allow you to create a on demand website. The Html file you serve up can should only contain links to CDN resources or to readable JS or other files within your storage account blob storage. By proxying the azure function then everything could be accessed via the same URL.\nusing System.IO;\nusing System.Linq;\nusing System.Net;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Threading.Tasks;\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.Azure.WebJobs.Extensions.Http;\nusing Microsoft.Azure.WebJobs.Host;\nusing MimeTypes;\n\nnamespace Counterflip\n{\n    public static class WebSite\n    {\n        [FunctionName(\"WebSite\")]\n        public static async Task&lt;HttpResponseMessage&gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)]HttpRequestMessage req, TraceWriter log)\n        {\n            log.Info(\"C# HTTP trigger function processed a request.\");\n\n            try\n            {\n                var response = new HttpResponseMessage(HttpStatusCode.OK);\n                var stream = new FileStream(@\"www\\index.html\", FileMode.Open);\n                response.Content = new StreamContent(stream);\n                response.Content.Headers.ContentType =\n                    new MediaTypeHeaderValue(GetMimeType(@\"www\\index.html\"));\n                return response;\n            }\n            catch\n            {\n                return new HttpResponseMessage(HttpStatusCode.NotFound);\n            }\n        }\n\n        private static string GetMimeType(string filePath)\n        {\n            var fileInfo = new FileInfo(filePath);\n            return MimeTypeMap.GetMimeType(fileInfo.Extension);\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/FunctionMonkey/index.html",
    "href": "Content/Journal/posts/FunctionMonkey/index.html",
    "title": "Function Monkey",
    "section": "",
    "text": "Function Monkey Hello World\n\n\n\nReferences\n\nFunction Monkey\nJason Roberts"
  },
  {
    "objectID": "Content/Journal/posts/PlantumlQuartoFilter/index.html",
    "href": "Content/Journal/posts/PlantumlQuartoFilter/index.html",
    "title": "Plantuml Quarto Filter",
    "section": "",
    "text": "For many tasks a diagram is required to illustrate a concept or process. This Quarto filter allows PlantUML diagrams to be created directly within Quarto documents. The filter supports both inline PlantUML code and loading PlantUML code from external .puml files."
  },
  {
    "objectID": "Content/Journal/posts/PlantumlQuartoFilter/index.html#file-source",
    "href": "Content/Journal/posts/PlantumlQuartoFilter/index.html#file-source",
    "title": "Plantuml Quarto Filter",
    "section": "File Source",
    "text": "File Source\nN.B. The path to the puml file is relative to the location of the .qmd file being processed.\nN.B. The plantuml script mush be stored in a file with a ‚Äò.puml‚Äô extension.\nN.B. The pumlfile attribute must not include a file extension.\n```{.plantuml pumlfile=\"diagram1\"}\n```\nOr for inline PlantUML code:\n```{.plantuml}\n@startuml   \n    Alice -&gt; Bob: Hello\n@enduml\n```"
  },
  {
    "objectID": "Content/Projects/posts/Frontify.html",
    "href": "Content/Projects/posts/Frontify.html",
    "title": "Frontify Marketing Platform",
    "section": "",
    "text": "This project was integration between a internal system and the Frontify Marketing platform.\nFrontify"
  },
  {
    "objectID": "docs/Content/Journal/posts/Test-Note/index.html",
    "href": "docs/Content/Journal/posts/Test-Note/index.html",
    "title": "Test Note",
    "section": "",
    "text": "Hello World! This is a test note."
  }
]