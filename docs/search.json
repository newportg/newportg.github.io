[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gary Newport MBCS",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Content/Research/articles/AzureApplicationHosting/index.html",
    "href": "Content/Research/articles/AzureApplicationHosting/index.html",
    "title": "Azure Application Hosting",
    "section": "",
    "text": "Each component is a separately deploy-able artefact, but we need a coherent single URL to link them all. The normal method would be to deploy out each individual component to Azure and each would get its own ‚Äòaurewebsites.com‚Äô URL. This approach would lead to confusion, as it would mean you would need to keep lists of URL‚Äôs By using the proxy feature of Azure Functions we can define routes to each of the installed artefacts while preserving a single URL for the application. So:-\n\n\n\nRoute\nResult\n\n\n\n\nzoomalong.co.uk\nWebsite\n\n\nzoomalong.co.uk/api\nAzure functions\n\n\nzoomalong.co.uk/static\nAzure Storage Account\n\n\n\nUsed to store any files or images etc. Because both the API and Web Application exist on the same URL then we won‚Äôt run into any CORS issues. Remember to bind the DNS Cname to the Azure function proxy and not the website.\n\n\n\naah1\n\n\nHow ? In your Azure Functions Project Create a files called proxies.json and insert the following code\nproxies.json\n{\n  \"$schema\": \"http://json.schemastore.org/proxies\",\n    \"proxies\": {\n      \"api\": {\n        \"matchCondition\": {\n          \"route\": \"/api/{*url}\"\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"app\": {\n        \"matchCondition\": {\n          \"route\": \"{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"appResources\": {\n        \"matchCondition\": {\n          \"route\": \"/static/{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      }\n    }\n  }\nChange in the build. * Build should include Azure App Service Deploy V3 or greater * Update this section with the file to be changed. *  * Add your substitutions to the Variables section * As Path to variable to be replace element.element.element * Value to be replaced. * as below\n\n\n\n\n\n\n\nProxy\nURL\n\n\n\n\nproxies.api.backendUri\nhttps://azure website url\n\n\nproxies.app.backendUri\nhttps://azure function url\n\n\nproxies.appResources.backendUri\nhttps:// azure storage account blob strorage\n\n\n\n\nRepeat in the release."
  },
  {
    "objectID": "Content/Research/articles/APIFirst/index.html",
    "href": "Content/Research/articles/APIFirst/index.html",
    "title": "API First",
    "section": "",
    "text": "API First Process"
  },
  {
    "objectID": "Content/Research/articles/APIFirst/index.html#description",
    "href": "Content/Research/articles/APIFirst/index.html#description",
    "title": "API First",
    "section": "Description",
    "text": "Description\nAPI First is an approach to software development that prioritizes the design and implementation of APIs before building the underlying application or service. This approach emphasizes the importance of APIs as the primary means of communication between different components of a system, as well as between different systems.\nBy designing APIs first, developers can ensure that the APIs are well-defined, consistent, and easy to use. This can lead to better collaboration between teams, faster development cycles, and more efficient integration with third-party services.\nThe approach detailed here takes the responsibility for creating the API definition away from the developer and places it in the hands of a specialist tool. This ensures that the API definition is created in a consistent manner and adheres to best practices. The API definition is then used to generate the client and server code, which can be integrated into the application or service. This approach can help to reduce errors and inconsistencies in the code, as well as improve the overall quality of the API. The API First approach can be particularly beneficial in complex systems where multiple teams are working on different components. By defining the APIs first, teams can work independently while still ensuring that their components will integrate seamlessly with the rest of the system. The API First approach can also help to improve the overall user experience of the application or service. By designing APIs that are easy to use and understand, developers can create applications that are more intuitive and user-friendly. Overall, the API First approach is a powerful way to improve the development process and create high-quality applications and services that are built around well-designed APIs."
  },
  {
    "objectID": "Content/Research/articles/APIFirst/index.html#tools",
    "href": "Content/Research/articles/APIFirst/index.html#tools",
    "title": "API First",
    "section": "Tools",
    "text": "Tools\nThe tools used in this process are:\n\nTypeSpec - A tool for designing and generating APIs\nAutoRest - A tool for generating client and server code from API definitions\nQuarto - A tool for creating and publishing documents\nLUA - A lightweight scripting language used for automation\nPowerShell - A task automation and configuration management framework\nC# - A programming language used for building applications and services\nAzure Functions - A serverless compute service used for building and deploying applications and services"
  },
  {
    "objectID": "Content/Research/articles/APIFirst/index.html#process",
    "href": "Content/Research/articles/APIFirst/index.html#process",
    "title": "API First",
    "section": "Process",
    "text": "Process\nThe process for creating an API using the API First approach is as follows:\n\nDesign the API - Use TypeSpec to design the API, including the endpoints, request and response formats, and any authentication or authorization requirements.\nGenerate the API definition - Use TypeSpec to generate the API definition in a standard format, such as OpenAPI or Swagger.\nGenerate the client and server code - Use AutoRest to generate the client and server code from the API definition.\nIntegrate the code - Integrate the generated code into the application or service.\nTest the API - Test the API to ensure that it works as expected and meets the requirements.\n\nDocument the API - Use Quarto to create documentation for the API, including usage examples and any relevant information for developers.\nAutomate the process - Use LUA and PowerShell to automate the process of generating the API definition, client and server code, and documentation."
  },
  {
    "objectID": "Content/Research/articles/APIFirst/index.html#benefits",
    "href": "Content/Research/articles/APIFirst/index.html#benefits",
    "title": "API First",
    "section": "Benefits",
    "text": "Benefits\nThe benefits of using the API First approach include:\n\n\n\n\n\n\n\nCategory\nQualities\n\n\n\n\nüßë‚Äçüíª Developer Experience & Productivity\n- Improved developer productivity and satisfaction  - Easier onboarding of new developers and teams  - Reduced errors and inconsistencies in the code  - Easier maintenance and updates to the API  - Improved versioning and backward compatibility of the API  - Better integration with DevOps and CI/CD processes\n\n\nüöÄ Development Efficiency & Technical Excellence\n- Faster development cycles  - More efficient integration with third-party services  - Consistent and well-defined APIs  - Better quality APIs  - Better support for multiple platforms and devices  - More effective monitoring and analytics of API usage  - More effective management of API lifecycle and governance\n\n\nüåê Business Alignment & Strategic Impact\n- Better alignment with business goals and requirements  - Increased flexibility and scalability of the application or service  - Enhanced ability to adapt to changing market and customer needs  - Greater overall success and impact of the application or service in the market  - Increased revenue and business opportunities through the API  - Better alignment with industry trends and best practices in API development\n\n\nü§ù Collaboration & Communication\n- Improved collaboration between teams  - Improved communication and transparency with stakeholders and customers\n\n\nüé® Innovation & Design\n- Greater innovation and creativity in API design  - Improved user experience\n\n\nüîí Trust, Security & Adoption\n- Enhanced security and compliance with industry standards  - Greater adoption and usage of the API by developers and users  - Enhanced reputation and credibility of the organization providing the API"
  },
  {
    "objectID": "Content/Research/articles/APIFirst/index.html#summary",
    "href": "Content/Research/articles/APIFirst/index.html#summary",
    "title": "API First",
    "section": "Summary",
    "text": "Summary\nThe API First approach is a powerful way to improve the development process and create high-quality applications and services that are built around well-designed APIs. By prioritizing the design and implementation of APIs, developers can ensure that their applications are more efficient, flexible, and user-friendly. The tools and processes outlined in this document can help to streamline the API First approach and make it easier for developers to create and maintain high-quality APIs."
  },
  {
    "objectID": "Content/Journal/posts/WebUI/index.html",
    "href": "Content/Journal/posts/WebUI/index.html",
    "title": "Web UI",
    "section": "",
    "text": "A web application, also known as a web app, is a software application that runs on web browsers and is accessed over the internet. It is designed to provide interactive and dynamic functionality to users, like traditional desktop applications, but with the advantage of being platform-independent and accessible from any device with a web browser.\nWeb applications are typically built using web technologies such as HTML (Hypertext Markup Language), CSS (Cascading Style Sheets), and JavaScript, which enable the creation of interactive user interfaces and the manipulation of data. The application‚Äôs logic is executed on the server and/or client side, depending on the architecture employed.\nWeb applications can vary widely in their complexity and functionality, ranging from simple websites with basic forms and interactivity to more advanced applications that involve complex data processing, real-time communication, and collaboration. Examples of web applications include online banking systems, social media platforms, e-commerce websites, project management tools, and web-based email clients.\nOne of the key advantages of web applications is their ability to deliver updates and improvements seamlessly, as they are centrally hosted and accessed over the internet. This eliminates the need for users to manually install software updates, resulting in a more efficient and user-friendly experience.\n\n\nSSR (Server-Side Rendering), SSG (Static Site Generation), SPA (Single-Page Application), PWA (Progressive Web Application), Isomorphic, and Micro Frontend architectures are different approaches to building web applications, each with its own characteristics and benefits.\n\n\nSSR is an approach where the server generates the complete HTML for each request and sends it to the client. The client receives a fully rendered page, including content and data. SSR is beneficial for improving initial page load time and search engine optimization (SEO) since search engines can easily crawl and index the content.\n\n\n\nSSR\n\n\n\n\n\nSSG is a method of generating static HTML files during the build process based on predefined templates and data. These static files can then be served to clients without the need for server-side processing. SSG provides fast page loads, reduces server load, and enables caching benefits. It is well-suited for content-based websites and blogs with less dynamic content.\n\n\n\nSSG\n\n\n\n\n\nSPA is an application that loads a single HTML page initially and then dynamically updates its content using JavaScript, typically through API calls. The client-side JavaScript handles the rendering of views and the manipulation of data. SPAs provide a smooth and responsive user experience, as they avoid full page reloads. Examples include Gmail and Trello.\n\n\n\nSPA\n\n\n\n\n\nPWA is an application that combines the features of web and mobile apps. PWAs are built using web technologies and provide an app-like experience to users, including offline capabilities, push notifications, and access to device features. PWAs can be installed on the user‚Äôs device and are accessible through the home screen, bridging the gap between web and native apps.\n\n\n\nPWA\n\n\n\n\n\nIsomorphic applications aim to create a consistent codebase that can run both on the server and the client. The goal is to share as much code as possible between the server and the client, enabling server-side rendering for improved initial load times and search engine friendliness while still providing interactive client-side functionality.\n\n\n\nISO\n\n\n\n\n\nMicro Frontend is an architectural approach that focuses on breaking down a web application into smaller, self-contained frontend modules. Each module can be developed, tested, and deployed independently, often by different teams. Micro Frontend allows for better scalability, flexibility, and autonomy in the frontend development process.\n\n\n\nmicrofrontend\n\n\nIt‚Äôs important to note that these architectures are not mutually exclusive, and elements of each can be combined to suit specific project requirements. The choice of architecture depends on factors such as application complexity, performance needs, SEO requirements, development team structure, and user experience goals.\n\n\n\n\nThe web has got more complicated, having a development team that can keep pace with the development and testing approaches, code structures and framework evolutions is difficult and probably very expensive. Multiple teams, different focuses Having one team develop a web application where everyone uses one JavaScript framework for the GUI and .NET for the APIs can have several benefits:\n\nConsistency: Using a single JavaScript framework for the GUI ensures consistent code style, architecture, and patterns across the frontend components. This simplifies collaboration, code reviews, and maintenance tasks within the development team.\nEfficient Communication: When the GUI and APIs are developed by the same team, communication between frontend and backend developers becomes more seamless. Developers can coordinate closely, align requirements, and address issues promptly.\nShared Expertise: By focusing on a single JavaScript framework, team members can deepen their expertise in that particular framework. This can result in faster development, efficient problem-solving, and improved overall quality of the frontend code.\nSeamless Integration: .NET is a versatile framework that offers excellent integration capabilities with various APIs and services. When the same team handles both the frontend and backend, they can ensure smooth communication and data flow between the GUI and the APIs, leading to a better overall user experience.\nFaster Development Iterations: With a unified team and shared knowledge, development iterations can be accelerated. Developers can iterate and deploy changes more quickly, as they have a deep understanding of both the frontend and backend components. This agility can be beneficial for meeting project deadlines and adapting to evolving requirements.\n\nHowever, there are some considerations to keep in mind:\n\nExpertise Balance: It‚Äôs important to ensure that the team has a balanced skill set, with expertise in both the chosen JavaScript framework and .NET for APIs. If the team members are primarily skilled in one area and lack proficiency in the other, it could lead to inefficiencies or a compromise in the quality of either the frontend or backend development.\nScalability: As the application grows and becomes more complex, a single team may face challenges in managing both the frontend and backend aspects simultaneously. In such cases, it may be necessary to divide the team or consider specialized roles to ensure efficient development and maintenance.\nFlexibility and Adaptability: Relying on a single JavaScript framework and .NET for APIs may limit the team‚Äôs flexibility when it comes to incorporating new technologies or frameworks that may better suit specific requirements. It‚Äôs important to periodically evaluate and adapt the technology stack to stay up-to-date with industry trends and evolving project needs. Ultimately, the decision to have one team develop both the GUI and APIs using specific technologies depends on factors such as the team‚Äôs expertise, project requirements, timelines, and scalability considerations.\n\nIn Hub 1.0 we have encountered all the negative considerations detailed above, so I‚Äôd advise against having multi-disciplinary developers, rather we should have Web GUI and API developers who specialise in their areas."
  },
  {
    "objectID": "Content/Journal/posts/WebUI/index.html#web-ui-architectures",
    "href": "Content/Journal/posts/WebUI/index.html#web-ui-architectures",
    "title": "Web UI",
    "section": "",
    "text": "SSR (Server-Side Rendering), SSG (Static Site Generation), SPA (Single-Page Application), PWA (Progressive Web Application), Isomorphic, and Micro Frontend architectures are different approaches to building web applications, each with its own characteristics and benefits.\n\n\nSSR is an approach where the server generates the complete HTML for each request and sends it to the client. The client receives a fully rendered page, including content and data. SSR is beneficial for improving initial page load time and search engine optimization (SEO) since search engines can easily crawl and index the content.\n\n\n\nSSR\n\n\n\n\n\nSSG is a method of generating static HTML files during the build process based on predefined templates and data. These static files can then be served to clients without the need for server-side processing. SSG provides fast page loads, reduces server load, and enables caching benefits. It is well-suited for content-based websites and blogs with less dynamic content.\n\n\n\nSSG\n\n\n\n\n\nSPA is an application that loads a single HTML page initially and then dynamically updates its content using JavaScript, typically through API calls. The client-side JavaScript handles the rendering of views and the manipulation of data. SPAs provide a smooth and responsive user experience, as they avoid full page reloads. Examples include Gmail and Trello.\n\n\n\nSPA\n\n\n\n\n\nPWA is an application that combines the features of web and mobile apps. PWAs are built using web technologies and provide an app-like experience to users, including offline capabilities, push notifications, and access to device features. PWAs can be installed on the user‚Äôs device and are accessible through the home screen, bridging the gap between web and native apps.\n\n\n\nPWA\n\n\n\n\n\nIsomorphic applications aim to create a consistent codebase that can run both on the server and the client. The goal is to share as much code as possible between the server and the client, enabling server-side rendering for improved initial load times and search engine friendliness while still providing interactive client-side functionality.\n\n\n\nISO\n\n\n\n\n\nMicro Frontend is an architectural approach that focuses on breaking down a web application into smaller, self-contained frontend modules. Each module can be developed, tested, and deployed independently, often by different teams. Micro Frontend allows for better scalability, flexibility, and autonomy in the frontend development process.\n\n\n\nmicrofrontend\n\n\nIt‚Äôs important to note that these architectures are not mutually exclusive, and elements of each can be combined to suit specific project requirements. The choice of architecture depends on factors such as application complexity, performance needs, SEO requirements, development team structure, and user experience goals."
  },
  {
    "objectID": "Content/Journal/posts/WebUI/index.html#people",
    "href": "Content/Journal/posts/WebUI/index.html#people",
    "title": "Web UI",
    "section": "",
    "text": "The web has got more complicated, having a development team that can keep pace with the development and testing approaches, code structures and framework evolutions is difficult and probably very expensive. Multiple teams, different focuses Having one team develop a web application where everyone uses one JavaScript framework for the GUI and .NET for the APIs can have several benefits:\n\nConsistency: Using a single JavaScript framework for the GUI ensures consistent code style, architecture, and patterns across the frontend components. This simplifies collaboration, code reviews, and maintenance tasks within the development team.\nEfficient Communication: When the GUI and APIs are developed by the same team, communication between frontend and backend developers becomes more seamless. Developers can coordinate closely, align requirements, and address issues promptly.\nShared Expertise: By focusing on a single JavaScript framework, team members can deepen their expertise in that particular framework. This can result in faster development, efficient problem-solving, and improved overall quality of the frontend code.\nSeamless Integration: .NET is a versatile framework that offers excellent integration capabilities with various APIs and services. When the same team handles both the frontend and backend, they can ensure smooth communication and data flow between the GUI and the APIs, leading to a better overall user experience.\nFaster Development Iterations: With a unified team and shared knowledge, development iterations can be accelerated. Developers can iterate and deploy changes more quickly, as they have a deep understanding of both the frontend and backend components. This agility can be beneficial for meeting project deadlines and adapting to evolving requirements.\n\nHowever, there are some considerations to keep in mind:\n\nExpertise Balance: It‚Äôs important to ensure that the team has a balanced skill set, with expertise in both the chosen JavaScript framework and .NET for APIs. If the team members are primarily skilled in one area and lack proficiency in the other, it could lead to inefficiencies or a compromise in the quality of either the frontend or backend development.\nScalability: As the application grows and becomes more complex, a single team may face challenges in managing both the frontend and backend aspects simultaneously. In such cases, it may be necessary to divide the team or consider specialized roles to ensure efficient development and maintenance.\nFlexibility and Adaptability: Relying on a single JavaScript framework and .NET for APIs may limit the team‚Äôs flexibility when it comes to incorporating new technologies or frameworks that may better suit specific requirements. It‚Äôs important to periodically evaluate and adapt the technology stack to stay up-to-date with industry trends and evolving project needs. Ultimately, the decision to have one team develop both the GUI and APIs using specific technologies depends on factors such as the team‚Äôs expertise, project requirements, timelines, and scalability considerations.\n\nIn Hub 1.0 we have encountered all the negative considerations detailed above, so I‚Äôd advise against having multi-disciplinary developers, rather we should have Web GUI and API developers who specialise in their areas."
  },
  {
    "objectID": "Content/Journal/posts/UserAssignManagedIdentities/index.html",
    "href": "Content/Journal/posts/UserAssignManagedIdentities/index.html",
    "title": "User Assigned Managed Identities",
    "section": "",
    "text": "A Managed Identity is a Azure resource which comprises two components. * A resource. A resource will create a Identity in Azure AD * An Azure Role\nTherefore by giving a AD registered App the Managed Identity Role, when the app trys to connect Ad will give permission without the need for any auth flow credentials to be passed.\nManaged identities eradicate the need to manage credentials within your code. A components identity is lodged within Active Directory, and makes available a AD Token for other componets to consume. The AD token gives the consuming compoent access rights to the original resource.\nThere are two types of Mangaged Identities, System assigned and User assigned. System assigned identities are tied to the resource that creates them, so when the resource is deleted, so is the identity, whereas user assigned identities exist idependantly whithin AD and can be assign to several resources.\ne.g As in the example below. A User Assigned managed identity is created and both a Azure Function and Azure Sql Server share the identity."
  },
  {
    "objectID": "Content/Journal/posts/UserAssignManagedIdentities/index.html#variables",
    "href": "Content/Journal/posts/UserAssignManagedIdentities/index.html#variables",
    "title": "User Assigned Managed Identities",
    "section": "Variables",
    "text": "Variables\n    \"var_id_name\": \"[concat(variables('namingConvention').prefixes.Identity, '-', variables('var_application_name_delim'),  parameters('para_acronym_region'))]\",\n    \"var_uaid_name\": \"[concat('/subscriptions/',variables('var_sub_id'),'/resourcegroups/', resourceGroup().Name, '/providers/Microsoft.ManagedIdentity/userAssignedIdentities/', tolower(variables('var_id_name')))]\""
  },
  {
    "objectID": "Content/Journal/posts/UserAssignManagedIdentities/index.html#ctrate-resources",
    "href": "Content/Journal/posts/UserAssignManagedIdentities/index.html#ctrate-resources",
    "title": "User Assigned Managed Identities",
    "section": "Ctrate Resources",
    "text": "Ctrate Resources\nThe following ARM snippets show how you define and assign a User Assigned Identity to a Azure Function and Azure Sql Server, this will allow the Azure function to access the server.\n\nCreate a User Assigned Managed Identity\n    {\n      \"type\": \"Microsoft.ManagedIdentity/userAssignedIdentities\",\n      \"apiVersion\": \"2018-11-30\",\n      \"name\": \"[variables('var_id_name')]\",\n      \"location\": \"[resourceGroup().location]\",\n      \"tags\": {\n        \"displayName\": \"Managed Identity\"\n      }\n    },\n\n\nCreate a Azure Function, with both a System and User Assigned Identity\n    {\n      \"apiVersion\": \"2021-02-01\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Web/serverfarms/', variables('var_svcpln_name'))]\"\n      ],\n      \"identity\": {\n        \"type\": \"SystemAssigned, UserAssigned\",\n        \"userAssignedIdentities\": {\n          \"[variables('var_uaid_name')]\": {}\n        }\n      },\n      \"kind\": \"functionapp\",\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_azf_name')]\",\n      \"properties\": {\n        \"state\": \"[parameters('para_funcState')]\",\n        \"name\": \"[variables('var_azf_name')]\",\n        \"siteConfig\": {\n          \"alwaysOn\": \"[parameters('para_alwaysOn')]\"\n        },\n        \"clientAffinityEnabled\": false,\n        \"serverFarmId\": \"[variables('var_svcpln_name')]\",\n        \"hostNameSslStates\": [],\n        \"httpsOnly\": true\n      },\n      \"resources\": [\n      ],\n      \"tags\": {\n        \"displayName\": \"Az Function\"\n      },\n      \"type\": \"Microsoft.Web/sites\"\n    },\n\n\nCreate a Azure Sql Server with User Assigned Identity\n    {\n      \"type\": \"Microsoft.Sql/servers\",\n      \"apiVersion\": \"2021-02-01-preview\",\n      \"dependsOn\": [\n        \"[resourceId('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\"\n      ],\n      \"name\": \"[variables('var_sql_name')]\",\n      \"location\": \"eastus\",\n      \"kind\": \"v12.0\",\n      \"identity\": {\n        \"type\": \"UserAssigned\",\n        \"userAssignedIdentities\": {\n          \"[variables('var_uaid_name')]\": {}\n        }\n      },\n      \"properties\": {\n        \"administratorLogin\": \"[parameters('para_dbUsr')]\",\n        \"administratorLoginPassword\": \"[parameters('para_dbPwd')]\",\n        \"version\": \"12.0\",\n        \"minimalTlsVersion\": \"1.2\",\n        \"publicNetworkAccess\": \"Enabled\",\n        \"primaryUserAssignedIdentityId\": \"[variables('var_uaid_name')]\",\n        \"administrators\": {\n          \"administratorType\": \"ActiveDirectory\",\n          \"principalType\": \"Group\",\n          \"login\": \"Hub-Sec-DB-Grp\",\n          \"sid\": \"1b2a41cc-232c-4d73-9b30-9159697bec2d\",\n          \"tenantId\": \"55a71488-bbff-4451-a18d-a1bfa479293b\"\n        },\n        \"restrictOutboundNetworkAccess\": \"Disabled\"\n      },\n      \"tags\": {\n        \"displayName\": \"Sql Server\"\n      }\n    },"
  },
  {
    "objectID": "Content/Journal/posts/UserAssignManagedIdentities/index.html#azure-function",
    "href": "Content/Journal/posts/UserAssignManagedIdentities/index.html#azure-function",
    "title": "User Assigned Managed Identities",
    "section": "Azure Function",
    "text": "Azure Function\nThe following snippet shows how you can access the Azure Sql Server from a Azure Function. notice how the connection string indicated that authentication is handled by the managed identity.\nSo on a connection being made Sql with take a Identity token pased by the function and validate it with identity in AD.\npublic static class Function1\n{\n    private static SqlConnection connection = new SqlConnection();\n\n    [FunctionName(\"Function1\")]\n    public static async Task&lt;IActionResult&gt; Run(\n        [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)] HttpRequest req,\n        ILogger log)\n    {\n        try\n        {\n            connection.ConnectionString = \"Server=sql-mifunctest-vse-ne.database.windows.net; Authentication=Active Directory Managed Identity; Database=MiFuncTest\";\n            await connection.OpenAsync();\n            var cmd = connection.CreateCommand();\n            cmd.CommandText = \"select * from [dbo].[WebEnquiry]\";\n\n            var response = await cmd.ExecuteReaderAsync();\n\n            var result = \"\";\n            while (response.Read())\n            { \n                Console.WriteLine(response[\"Id\"].ToString());\n                result += response[\"Id\"].ToString() + \"\\n\";\n            }\n            response.Close();\n\n            return new OkObjectResult($\"The database connection is: {connection.State}  Result {result}\");\n        }\n        catch (SqlException sqlex)\n        {\n            return new OkObjectResult($\"The following SqlException happened: {sqlex.Message}\");\n        }\n        catch (Exception ex)\n        {\n            return new OkObjectResult($\"The following Exception happened: {ex.Message}\");\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/TestingAzureFunctions/index.html",
    "href": "Content/Journal/posts/TestingAzureFunctions/index.html",
    "title": "Integration Testing Azure Functions",
    "section": "",
    "text": "Integration testing of azure functions is essential when demostrating that a service does what its supposed to do, and also responds in the way you expect. This post shows how you can construct a unit test, in this case Specflow, so that it excutes the function as a background process, so that you can test it using standard http calls.\nThis process uses the Azure Functions Core Tools cli interface https://go.microsoft.com/fwlink/?linkid=2174087 which should be installed. This should also be installed as part of the build process.\n  - task: PowerShell@2\n    inputs:\n      targetType: 'inline'\n      script: 'choco install azure-functions-core-tools -y'\nThis snippet demostrates how to instatiate the Azure function at the start of the unit test and kill it off after the test has completed. N.B the Directory Info object you pass is the directory of your azure function source code.\nnamespace ImageResizerTests\n{\n    [Binding]\n    public class ResizerSteps\n    {\n        private string request;\n        private IFlurlResponse response;\n        private static TemporaryAzureFunctionsApplication azfunc;\n\n        [BeforeTestRun(Order = 1)]\n        public static void Before()\n        {\n            var dirInfo = new DirectoryInfo(\"..\\\\..\\\\..\\\\..\\\\..\\\\src\\\\Application\\\\KnightFrank.Hub.Watermark\");\n            Console.WriteLine($\"Directory : {dirInfo.ToString()}\");\n\n            azfunc = TemporaryAzureFunctionsApplication.StartNewAsync(dirInfo).Result;\n        }\n\n        [AfterTestRun]\n        public static void After()\n        {\n            azfunc.DisposeAsync();\n        }\n\n        [When(@\"the URI is evaluated\")]\n        public void WhenTheURIIsEvaluated()\n        {\n            try\n            {\n                response = \"http://localhost:7071/api/Resize\"\n                    .WithHeader(\"Accept\", \"*/*\")\n                    .WithHeader(\"Content_Type\", \"application/json\")\n                    .AllowAnyHttpStatus()\n                    .PostJsonAsync(new { Size = \"SmallThumbnail\", InputUri = request }).Result;\n            }\n            catch(Exception ex)\n            {\n                Console.WriteLine(ex.Message);\n            }\n        }\n    }\n}\nCopy the following into its own file, as is.\nnamespace ImageResizerTests\n{\n    using Polly;\n    using Polly.Retry;\n    using System;\n    using System.Diagnostics;\n    using System.IO;\n    using System.Net.Http;\n    using System.Threading.Tasks;\n\n    public class TemporaryAzureFunctionsApplication : IAsyncDisposable\n    {\n        private readonly Process _application;\n        private static readonly HttpClient HttpClient = new HttpClient();\n\n        private TemporaryAzureFunctionsApplication(Process application)\n        {\n            _application = application;\n        }\n\n        public static async Task&lt;TemporaryAzureFunctionsApplication&gt; StartNewAsync(DirectoryInfo projectDirectory)\n        {\n            int port = 7071;\n            Process app = StartApplication(port, projectDirectory);\n            await WaitUntilTriggerIsAvailableAsync($\"http://localhost:{port}/\");\n\n            return new TemporaryAzureFunctionsApplication(app);\n        }\n\n        private static Process StartApplication(int port, DirectoryInfo projectDirectory)\n        {\n            var appInfo = new ProcessStartInfo(\"func\", $\"start --port {port} --prefix bin/Debug/net8.0\")\n            {\n                UseShellExecute = false,\n                CreateNoWindow = false,\n                WorkingDirectory = projectDirectory.FullName\n            };\n\n            var app = new Process { StartInfo = appInfo };\n            app.Start();\n            return app;\n        }\n\n        private static async Task WaitUntilTriggerIsAvailableAsync(string endpoint)\n        {\n            AsyncRetryPolicy retryPolicy =\n                    Policy.Handle&lt;Exception&gt;()\n                          .WaitAndRetryForeverAsync(index =&gt; TimeSpan.FromMilliseconds(500));\n\n            PolicyResult&lt;HttpResponseMessage&gt; result =\n                await Policy.TimeoutAsync(TimeSpan.FromSeconds(30))\n                            .WrapAsync(retryPolicy)\n                            .ExecuteAndCaptureAsync(() =&gt; HttpClient.GetAsync(endpoint));\n\n            if (result.Outcome == OutcomeType.Failure)\n            {\n                throw new InvalidOperationException(\n                    \"The Azure Functions project doesn't seem to be running, \"\n                    + \"please check any build or runtime errors that could occur during startup\");\n            }\n        }\n\n        public ValueTask DisposeAsync()\n        {\n            if (!_application.HasExited)\n            {\n                _application.Kill(entireProcessTree: true);\n            }\n\n            _application.Dispose();\n            return ValueTask.CompletedTask;\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/StorageAccountEncryption/index.html",
    "href": "Content/Journal/posts/StorageAccountEncryption/index.html",
    "title": "Storage Account Encryption",
    "section": "",
    "text": "By default storage accounts are encrypted, and Microsoft holds the keys. The encryption that is used is AES 256 bit, as it is one of the strongest ciphers currently available.\nThe one big issue with this is that Microsoft owns the encryption key and potentially has unrestricted access to the users data. They have included the facility for the user to specify an encryption key, so you can meet your individual company security or regulatory compliance needs. To use your own key, you will need. * A Key Vault * Needs to be in the same region as the storage * Does not need to be in the same subscription * Storage needs permissions to access your Key Vault. * Need to grant wrapKey, unwrapKey privileges"
  },
  {
    "objectID": "Content/Journal/posts/StorageAccountEncryption/index.html#data-encryption",
    "href": "Content/Journal/posts/StorageAccountEncryption/index.html#data-encryption",
    "title": "Storage Account Encryption",
    "section": "Data Encryption",
    "text": "Data Encryption\nA key point to understand, the data itself is not encrypted with the key. Microsoft employs a two stage encryption process which involves a DEK (Data Encryption Key) and a KEK (Key Encryption Key). The DEK is generated when the storage account is created, and is used to encrypt the data. The DEK is it self is encrypted with a key that Microsoft holds (KEK), Its this Microsoft key that can be replaced with the users own key."
  },
  {
    "objectID": "Content/Journal/posts/StorageAccountEncryption/index.html#key-rotation",
    "href": "Content/Journal/posts/StorageAccountEncryption/index.html#key-rotation",
    "title": "Storage Account Encryption",
    "section": "Key Rotation",
    "text": "Key Rotation\nThis is currently under development by Microsoft. Key rotation is a process where the KEK (see Data Encryption above) is rotated every 90 days, this involves decrypting the existing key and re-encrypting it with a newly generated key."
  },
  {
    "objectID": "Content/Journal/posts/StorageAccountEncryption/index.html#references",
    "href": "Content/Journal/posts/StorageAccountEncryption/index.html#references",
    "title": "Storage Account Encryption",
    "section": "References",
    "text": "References\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-service-encryption"
  },
  {
    "objectID": "Content/Journal/posts/SOLID/index.html",
    "href": "Content/Journal/posts/SOLID/index.html",
    "title": "SOLID",
    "section": "",
    "text": "S - Single responsibility\nO - Open Closed\nL - Liskov Substitution\nI - Interface Segregation\nD - Dependency Inversion\n\n\nSingle Responsibility\nA class should have one and only one reason to change, meaning that a class should have only one job.\n\n\nOpen Closed\nObjects or entities should be open for extension but closed for modification.\n\n\nLiskov Substitution\nLet q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T.\nThis means that every subclass or derived class should be substitutable for their base or parent class.\n\n\nInterface Segregation\nA client should never be forced to implement an interface that it doesn‚Äôt use, or clients shouldn‚Äôt be forced to depend on methods they do not use.\n\n\nDependency Inversion\nEntities must depend on abstractions, not on concretions. It states that the high-level module must not depend on the low-level module, but they should depend on abstractions."
  },
  {
    "objectID": "Content/Journal/posts/SAS-StorageAccountAccess/index.html",
    "href": "Content/Journal/posts/SAS-StorageAccountAccess/index.html",
    "title": "SAS Storage Account Access",
    "section": "",
    "text": "When you use shared access signatures in your applications, you need to be aware of two potential risks:\n\nIf a SAS is leaked, it can be used by anyone who obtains it, which can potentially compromise your storage account.\nIf a SAS provided to a client application expires and the application is unable to retrieve a new SAS from your service, then the application‚Äôs functionality may be hindered.\n\nThe following recommendations for using shared access signatures can help mitigate these risks:\n\nAlways use HTTPS to create or distribute a SAS. If a SAS is passed over HTTP and intercepted, an attacker performing a man-in-the-middle attack is able to read the SAS and then use it just as the intended user could have, potentially compromising sensitive data or allowing for data corruption by the malicious user.\nReference stored access policies where possible. Stored access policies give you the option to revoke permissions without having to regenerate the storage account keys. Set the expiration on these very far in the future (or infinite) and make sure it‚Äôs regularly updated to move it farther into the future.\nUse near-term expiration times on an ad hoc SAS. In this way, even if a SAS is compromised, it‚Äôs valid only for a short time. This practice is especially important if you cannot reference a stored access policy. Near-term expiration times also limit the amount of data that can be written to a blob by limiting the time available to upload to it.\nHave clients automatically renew the SAS if necessary. Clients should renew the SAS well before the expiration, in order to allow time for retries if the service providing the SAS is unavailable. If your SAS is meant to be used for a small number of immediate, short-lived operations that are expected to be completed within the expiration period, then this may be unnecessary as the SAS is not expected to be renewed. However, if you have client that is routinely making requests via SAS, then the possibility of expiration comes into play. The key consideration is to balance the need for the SAS to be short-lived (as previously stated) with the need to ensure that the client is requesting renewal early enough (to avoid disruption due to the SAS expiring prior to successful renewal). Be careful with SAS start time. If you set the start time for a SAS to now, then due to clock skew (differences in current time according to different machines), failures may be observed intermittently for the first few minutes. In general, set the start time to be at least 15 minutes in the past. Or, don‚Äôt set it at all, which will make it valid immediately in all cases. The same generally applies to expiry time as well‚Äìremember that you may observe up to 15 minutes of clock skew in either direction on any request. For clients using a REST version prior to 2012-02-12, the maximum duration for a SAS that does not reference a stored access policy is 1 hour, and any policies specifying longer term than that will fail.\nBe specific with the resource to be accessed. A security best practice is to provide a user with the minimum required privileges. If a user only needs read access to a single entity, then grant them read access to that single entity, and not read/write/delete access to all entities. This also helps lessen the damage if a SAS is compromised because the SAS has less power in the hands of an attacker.\nUnderstand that your account will be billed for any usage, including that done with SAS. If you provide write access to a blob, a user may choose to upload a 200GB blob. If you‚Äôve given them read access as well, they may choose to download it 10 times, incurring 2 TB in egress costs for you. Again, provide limited permissions to help mitigate the potential actions of malicious users. Use short-lived SAS to reduce this threat (but be mindful of clock skew on the end time).\nValidate data written using SAS. When a client application writes data to your storage account, keep in mind that there can be problems with that data. If your application requires that data be validated or authorized before it is ready to use, you should perform this validation after the data is written and before it is used by your application. This practice also protects against corrupt or malicious data being written to your account, either by a user who properly acquired the SAS, or by a user exploiting a leaked SAS.\nDon‚Äôt always use SAS. Sometimes the risks associated with a particular operation against your storage account outweigh the benefits of SAS. For such operations, create a middle-tier service that writes to your storage account after performing business rule validation, authentication, and auditing. Also, sometimes it‚Äôs simpler to manage access in other ways. For example, if you want to make all blobs in a container publicly readable, you can make the container Public, rather than providing a SAS to every client for access.\nUse Storage Analytics to monitor your application. You can use logging and metrics to observe any spike in authentication failures due to an outage in your SAS provider service or to the inadvertent removal of a stored access policy. See the Azure Storage Team Blog for additional information."
  },
  {
    "objectID": "Content/Journal/posts/SAS-StorageAccountAccess/index.html#best-practices-when-using-sas",
    "href": "Content/Journal/posts/SAS-StorageAccountAccess/index.html#best-practices-when-using-sas",
    "title": "SAS Storage Account Access",
    "section": "",
    "text": "When you use shared access signatures in your applications, you need to be aware of two potential risks:\n\nIf a SAS is leaked, it can be used by anyone who obtains it, which can potentially compromise your storage account.\nIf a SAS provided to a client application expires and the application is unable to retrieve a new SAS from your service, then the application‚Äôs functionality may be hindered.\n\nThe following recommendations for using shared access signatures can help mitigate these risks:\n\nAlways use HTTPS to create or distribute a SAS. If a SAS is passed over HTTP and intercepted, an attacker performing a man-in-the-middle attack is able to read the SAS and then use it just as the intended user could have, potentially compromising sensitive data or allowing for data corruption by the malicious user.\nReference stored access policies where possible. Stored access policies give you the option to revoke permissions without having to regenerate the storage account keys. Set the expiration on these very far in the future (or infinite) and make sure it‚Äôs regularly updated to move it farther into the future.\nUse near-term expiration times on an ad hoc SAS. In this way, even if a SAS is compromised, it‚Äôs valid only for a short time. This practice is especially important if you cannot reference a stored access policy. Near-term expiration times also limit the amount of data that can be written to a blob by limiting the time available to upload to it.\nHave clients automatically renew the SAS if necessary. Clients should renew the SAS well before the expiration, in order to allow time for retries if the service providing the SAS is unavailable. If your SAS is meant to be used for a small number of immediate, short-lived operations that are expected to be completed within the expiration period, then this may be unnecessary as the SAS is not expected to be renewed. However, if you have client that is routinely making requests via SAS, then the possibility of expiration comes into play. The key consideration is to balance the need for the SAS to be short-lived (as previously stated) with the need to ensure that the client is requesting renewal early enough (to avoid disruption due to the SAS expiring prior to successful renewal). Be careful with SAS start time. If you set the start time for a SAS to now, then due to clock skew (differences in current time according to different machines), failures may be observed intermittently for the first few minutes. In general, set the start time to be at least 15 minutes in the past. Or, don‚Äôt set it at all, which will make it valid immediately in all cases. The same generally applies to expiry time as well‚Äìremember that you may observe up to 15 minutes of clock skew in either direction on any request. For clients using a REST version prior to 2012-02-12, the maximum duration for a SAS that does not reference a stored access policy is 1 hour, and any policies specifying longer term than that will fail.\nBe specific with the resource to be accessed. A security best practice is to provide a user with the minimum required privileges. If a user only needs read access to a single entity, then grant them read access to that single entity, and not read/write/delete access to all entities. This also helps lessen the damage if a SAS is compromised because the SAS has less power in the hands of an attacker.\nUnderstand that your account will be billed for any usage, including that done with SAS. If you provide write access to a blob, a user may choose to upload a 200GB blob. If you‚Äôve given them read access as well, they may choose to download it 10 times, incurring 2 TB in egress costs for you. Again, provide limited permissions to help mitigate the potential actions of malicious users. Use short-lived SAS to reduce this threat (but be mindful of clock skew on the end time).\nValidate data written using SAS. When a client application writes data to your storage account, keep in mind that there can be problems with that data. If your application requires that data be validated or authorized before it is ready to use, you should perform this validation after the data is written and before it is used by your application. This practice also protects against corrupt or malicious data being written to your account, either by a user who properly acquired the SAS, or by a user exploiting a leaked SAS.\nDon‚Äôt always use SAS. Sometimes the risks associated with a particular operation against your storage account outweigh the benefits of SAS. For such operations, create a middle-tier service that writes to your storage account after performing business rule validation, authentication, and auditing. Also, sometimes it‚Äôs simpler to manage access in other ways. For example, if you want to make all blobs in a container publicly readable, you can make the container Public, rather than providing a SAS to every client for access.\nUse Storage Analytics to monitor your application. You can use logging and metrics to observe any spike in authentication failures due to an outage in your SAS provider service or to the inadvertent removal of a stored access policy. See the Azure Storage Team Blog for additional information."
  },
  {
    "objectID": "Content/Journal/posts/SAS-StorageAccountAccess/index.html#authorization",
    "href": "Content/Journal/posts/SAS-StorageAccountAccess/index.html#authorization",
    "title": "SAS Storage Account Access",
    "section": "Authorization",
    "text": "Authorization\n\nAzure Active Directory\n\nConsidered to be the most secure, as access is keyed to the users AD roles and permissions.\n\nShared Key\n\nThe Shared Key is the ‚ÄòRoot key‚Äô to the storage account, and has access to perform all operations.\nThis should not be shared or distributed.\n\nShared Access Signature\n\nSAS tokens can be generated, and allow access to resources. The SAS tokens can be limited to:-\nTime\nIP address\nUser\n\nAnonymous access to containers and blobs\n\nYou can set the storage account, so that anyone can access blobs or containers. This is obviously the least secure"
  },
  {
    "objectID": "Content/Journal/posts/SAS-StorageAccountAccess/index.html#reference",
    "href": "Content/Journal/posts/SAS-StorageAccountAccess/index.html#reference",
    "title": "SAS Storage Account Access",
    "section": "Reference",
    "text": "Reference\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1"
  },
  {
    "objectID": "Content/Journal/posts/QuartoWebBook/index.html",
    "href": "Content/Journal/posts/QuartoWebBook/index.html",
    "title": "Chapter2",
    "section": "",
    "text": "This is the introduction to my long blog post.",
    "crumbs": [
      "Now!",
      "Quarto Web Book Introduction"
    ]
  },
  {
    "objectID": "Content/Journal/posts/QuartoWebBook/index.html#introduction",
    "href": "Content/Journal/posts/QuartoWebBook/index.html#introduction",
    "title": "Chapter2",
    "section": "",
    "text": "This is the introduction to my long blog post.",
    "crumbs": [
      "Now!",
      "Quarto Web Book Introduction"
    ]
  },
  {
    "objectID": "Content/Journal/posts/QuartoWebBook/index.html#conclusion",
    "href": "Content/Journal/posts/QuartoWebBook/index.html#conclusion",
    "title": "Chapter2",
    "section": "Conclusion",
    "text": "Conclusion\nNeither of these solutions is very satisfactory, as Quarto seems to forget the basic tenants of Wiki‚Äôs. Method 1 is useful if you only have simple documents. Method 2 is useful but you will have issues with duplication of the home page of the book, or do you make the website blog post just a link ?",
    "crumbs": [
      "Now!",
      "Quarto Web Book Introduction"
    ]
  },
  {
    "objectID": "Content/Journal/posts/QuartoWebBook/chapter1SubPage.html",
    "href": "Content/Journal/posts/QuartoWebBook/chapter1SubPage.html",
    "title": "Chapter1 Sub Page",
    "section": "",
    "text": "Chapter 1 SubPage\nContent for chapter 1 Sub Page"
  },
  {
    "objectID": "Content/Journal/posts/PlantumlQuartoFilter/index.html",
    "href": "Content/Journal/posts/PlantumlQuartoFilter/index.html",
    "title": "Plantuml Quarto Filter",
    "section": "",
    "text": "For many tasks a diagram is required to illustrate a concept or process. This Quarto filter allows PlantUML diagrams to be created directly within Quarto documents. The filter supports both inline PlantUML code and loading PlantUML code from external .puml files."
  },
  {
    "objectID": "Content/Journal/posts/PlantumlQuartoFilter/index.html#file-source",
    "href": "Content/Journal/posts/PlantumlQuartoFilter/index.html#file-source",
    "title": "Plantuml Quarto Filter",
    "section": "File Source",
    "text": "File Source\nN.B. The path to the puml file is relative to the location of the .qmd file being processed.\nN.B. The plantuml script mush be stored in a file with a ‚Äò.puml‚Äô extension.\nN.B. The pumlfile attribute must not include a file extension.\n```{.plantuml pumlfile=\"diagram1\"}\n```\nOr for inline PlantUML code:\n```{.plantuml}\n@startuml   \n    Alice -&gt; Bob: Hello\n@enduml\n```"
  },
  {
    "objectID": "Content/Journal/posts/PfxBase64/index.html",
    "href": "Content/Journal/posts/PfxBase64/index.html",
    "title": "Certificate pfx to Base64",
    "section": "",
    "text": "To convert certificate that is in .pfx to base64 format in PowerShell, you can use .NET namespace available in PowerShell to convert. I had a scenario where I was required to use base64 encoding to upload certificate to Azure to secure communication to backend instance. Since Microsoft Azure provides rich API to work with. I was able to make a patch request and push certificate to Azure. In this tutorial, I will show you how to convert certificate from .pfx to base64. Open PowerShell as an administrator. Now that we have PowerShell console opened. Let‚Äôs first load the content into a variable.\nParam([string] $file) $file = '.\\certificate.pfx' $pfxFileBytes = get-content $file -Encoding Byte [System.Convert]::ToBase64String($pfxFileBytes) | Out-File 'PfxFileBytes-Base64.txt'\nOnce you have your Base64 string you can insert it in the ARM template as shown below."
  },
  {
    "objectID": "Content/Journal/posts/PfxBase64/index.html#azuredeploy.parameters.json",
    "href": "Content/Journal/posts/PfxBase64/index.html#azuredeploy.parameters.json",
    "title": "Certificate pfx to Base64",
    "section": "Azuredeploy.parameters.json",
    "text": "Azuredeploy.parameters.json\n    \"cert\": {\"value\": \"MIIT...\"},\n    \"certPass\": { \"value\": \"password\" }"
  },
  {
    "objectID": "Content/Journal/posts/PfxBase64/index.html#azuredeploy.json",
    "href": "Content/Journal/posts/PfxBase64/index.html#azuredeploy.json",
    "title": "Certificate pfx to Base64",
    "section": "Azuredeploy.json",
    "text": "Azuredeploy.json\n  \"parameters\": {\n      \"cert\": {\n      \"type\": \"securestring\"\n    },\n    \"certPass\": {\n      \"type\": \"securestring\"\n    }\n  }\n  \"variables\": {\n    \"var_cert_name\": \"[concat( tolower(parameters('para_application_name')), uniqueString(resourceGroup().id))]\",\n  }\n  \"resources\": [\n      {\n      \"apiVersion\": \"2015-08-01\",\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_cert_name')]\",\n      \"properties\": {\n        \"pfxBlob\": \"[parameters('cert')]\",\n        \"password\": \"[parameters('certPass')]\"\n      },\n      \"scale\": null,\n      \"tags\": {\n        \"displayName\": \"Certificate\"\n      },\n      \"type\": \"Microsoft.Web/certificates\"\n    }\n  ]"
  },
  {
    "objectID": "Content/Journal/posts/OnlineSpreadsheetDatabase/index.html",
    "href": "Content/Journal/posts/OnlineSpreadsheetDatabase/index.html",
    "title": "Online Spreadsheet Database",
    "section": "",
    "text": "Description\nA while a go I was working on a spreadsheet style applicaiton for the web, and we needed a method to store the 2 dimentional data in a 1 deminentional database. This was the resultant database structure, which is a linked list, which allowed both data and formula‚Äôs to included. There was either a template structure so you could load predefined sheets. I‚Äôll get round to recreating the web page‚Ä¶. eventually.\n\n\n\nGreenDog\n\n\n\nTables\n\nR2Data\nR2Item\nR3Data\nR6TemplateItem\n\n\nItemId, ParentId, GdDdId, Format, DivisionId, Depth, ProcessId, Attribute\nCREATE TABLE R6TemplateItem (\n       R6TemplateItemId INT NOT NULL IDENTITY PRIMARY KEY\n     , R6ParentId INT\n     , R6GdDdId VARCHAR(50)\n     , R6Depth INT\n     , R6Lineage VARCHAR(100)\n     , R6Format VARCHAR(50)\n     , R6Row INT\n     , R6Col INT\n     , R6Value VARCHAR(250)\n     , R6SeqNo INT\n     , R6ItemLinkId INT\n     , CONSTRAINT FK_R6TemplateItem_1 FOREIGN KEY (R6ParentId)\n                  REFERENCES R6TemplateItem (R6TemplateItemId)\n);\n\nCREATE TABLE R5Division (\n       R5DivisionId INT NOT NULL IDENTITY PRIMARY KEY\n     , R5ProcessId INT\n     , R5CountryId INT\n     , R5ItemId INT\n     , R5Name VARCHAR(50)\n     , R5RecipientId INT\n     , R5Seq INT\n     , R5Creator BIT\n     , R5Valid BIT\n     , R5Deleted BIT\n     , CONSTRAINT FK_R5Division_1 FOREIGN KEY (R5ProcessId)\n                  REFERENCES Q4Process (Q4ProcessId)\n     , CONSTRAINT FK_R5Division_3 FOREIGN KEY (R5CountryId)\n                  REFERENCES P3Country (P3CountryId)\n     , CONSTRAINT FK_R5Division_2 FOREIGN KEY (R5ItemId)\n                  REFERENCES R2Item (R2ItemId) ON DELETE CASCADE\n);\n\nCREATE TABLE R3Data (\n       R3DataId INT NOT NULL IDENTITY PRIMARY KEY\n     , R3ItemId INT NOT NULL\n     , R3DivisionId INT\n     , R3Value VARCHAR(250)\n     , R3ItemLinkId INT\n     , R3TaxPackageRef VARCHAR(50)\n     , R3ProcessId INT\n     , R3Attribute INT\n     , CONSTRAINT FK_R4Data_4 FOREIGN KEY (R3ItemId)\n                  REFERENCES R2Item (R2ItemId) ON DELETE CASCADE\n     , CONSTRAINT FK_R3Data_3 FOREIGN KEY (R3DivisionId)\n                  REFERENCES R5Division (R5DivisionId)\n);\n\nCREATE TABLE R1IpackTemplate (\n       R1IpackTemplateId INT NOT NULL IDENTITY PRIMARY KEY\n     , R1Name VARCHAR(20)\n     , R1TemplateItemId INT\n     , R1Deleted BIT\n     , CONSTRAINT FK_R1IpackTemplate_2 FOREIGN KEY (R1TemplateItemId)\n                  REFERENCES R6TemplateItem (R6TemplateItemId) ON DELETE CASCADE\n);\n\n\nGet Item Tree\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_getItemTree'\n       AND    type = 'P')\n    DROP PROCEDURE gd_getItemTree\nGO\n\nCREATE PROCEDURE gd_getItemTree\n    @ItemId int = NULL,\n    @DivisionId int = NULL\nAS\n\n    IF @ItemId IS NOT NULL and @DivisionId = -1\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            a.R2GdDdId AS GdDdId,\n            a.R2Format AS Format,\n            b.R3Value AS Value,\n            b.R3DivisionId AS DivisionId,\n            a.R2Depth AS Depth,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b\n        WHERE a.R2Lineage LIKE '%'+cast( @ItemId AS varchar(10)) +'%'\n            and a.R2GdDdId IN ( SELECT R6GdDdId FROM R6TemplateItem WHERE R6GdDdId = a.R2GdDdId and R6ParentId = 0)\n            and b.R3ItemId = a.R2ItemId\n            and b.R3DivisionId IS NULL\n        ORDER BY a.R2SeqNo\n    END\n    ELSE IF @ItemId IS NOT NULL and @DivisionId IS NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            a.R2GdDdId AS GdDdId,\n            a.R2Format AS Format,\n            b.R3Value AS Value,\n            b.R3DivisionId AS DivisionId,\n            a.R2Depth AS Depth,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b\n        WHERE a.R2Lineage LIKE '%'+cast( @ItemId AS varchar(10)) +'%'\n            and a.R2GdDdId IN ( SELECT R6GdDdId FROM R6TemplateItem WHERE R6GdDdId = a.R2GdDdId and R6ParentId = 0)\n            and b.R3ItemId = a.R2ItemId\n            and (b.R3DivisionId IS NULL\n            or b.R3DivisionId = (\n                SELECT R5DivisionId\n                FROM R5Division\n                WHERE b.R3DivisionId IS NOT NULL\n                and R5DivisionId = b.R3DivisionId\n                and R5Deleted IS NULL))\n        ORDER BY a.R2SeqNo\n    END\n    ELSE IF @ItemId IS NOT NULL and @DivisionId IS NOT NULL\n    BEGIN\n        -- Make sure its not marked as deleted.\n        IF( SELECT R5DivisionId FROM R5Division WHERE R5DivisionId = @DivisionId and R5Deleted IS NULL) &gt; 0\n        BEGIN\n            SELECT a.R2ItemId AS ItemId,\n                a.R2ParentId AS ParentId,\n                a.R2GdDdId AS GdDdId,\n                a.R2Format AS Format,\n                b.R3Value AS Value,\n                b.R3DivisionId AS DivisionId,\n                a.R2Depth AS Depth,\n                b.R3ProcessId AS ProcessId,\n                b.R3Attribute AS Attribute\n            FROM R2Item a, R3Data b\n            WHERE a.R2Lineage LIKE '%'+cast( @ItemId AS varchar(10)) +'%'\n                and a.R2GdDdId IN ( SELECT R6GdDdId FROM R6TemplateItem WHERE R6GdDdId = a.R2GdDdId and R6ParentId = 0)\n                and b.R3ItemId = a.R2ItemId\n                and (b.R3DivisionId = @DivisionId or b.R3DivisionId IS NULL)\n            ORDER BY a.R2SeqNo\n        END\n    END\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n-- EXECUTE gd_getItemTree 1803, 167\nGO\n\n\nGet Item\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_getItem'\n       AND    type = 'P')\n    DROP PROCEDURE gd_getItem\nGO\n\nCREATE PROCEDURE gd_getItem\n    @ItemId int = NULL,\n    @DivisionId int = NULL\nAS\n\n    DECLARE @TemplateParentId int;\n    SET @TemplateParentId = (SELECT a.R6TemplateItemId FROM R6TemplateItem a\n                WHERE a.R6GdDdId = (SELECT a.R2GdDdId AS GdDdId FROM R2Item a WHERE a.R2ItemId = @ItemId))\n\n    IF @ItemId IS NOT NULL and @DivisionId IS NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            b.R3DataId AS DataId,\n            a.R2GdDdId AS GdDdId,\n            c.R6Format AS Format,\n            c.R6Row AS Row,\n            c.R6Col AS Col,\n            a.R2SeqNo AS SeqNo,\n            c.R6SeqNo AS TemplateSeqNo,\n            b.R3DivisionId AS DivisionId,\n            b.R3Value AS Value,\n            b.R3ItemLinkId AS ItemLinkId,\n            b.R3TaxPackageRef AS TaxPackageRef,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b, R6TemplateItem c\n        WHERE ( a.R2ItemId = @ItemId or a.R2ParentId = @ItemId\n            or a.R2ParentId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId))\n            or a.R2ItemId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId)))\n            and b.R3ItemId = a.R2ItemId\n            and a.R2GdDdId = c.R6GdDdId\n            and (c.R6ParentId = @TemplateParentId\n                or c.R6TemplateItemId = @TemplateParentId)\n            order by c.R6Row, c.R6Col\n    END\n    IF @ItemId IS NOT NULL and @DivisionId IS NOT NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            b.R3DataId AS DataId,\n            a.R2GdDdId AS GdDdId,\n            c.R6Format AS Format,\n            c.R6Row AS Row,\n            c.R6Col AS Col,\n            a.R2SeqNo AS SeqNo,\n            c.R6SeqNo AS TemplateSeqNo,\n            b.R3DivisionId AS DivisionId,\n            b.R3Value AS Value,\n            b.R3ItemLinkId AS ItemLinkId,\n            b.R3TaxPackageRef AS TaxPackageRef,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute\n        FROM R2Item a, R3Data b, R6TemplateItem c\n        WHERE ( a.R2ItemId = @ItemId or a.R2ParentId = @ItemId\n            or a.R2ParentId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId))\n            or a.R2ItemId = ( SELECT R2ItemId FROM R2Item WHERE R2ParentId = 0\n                and R2GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId)))\n            and b.R3ItemId = a.R2ItemId\n            and a.R2GdDdId = c.R6GdDdId\n            and (c.R6ParentId = @TemplateParentId\n                or c.R6TemplateItemId = @TemplateParentId)\n            and ( b.R3DivisionId = @DivisionId or b.R3DivisionId IS NULL)\n            order by c.R6Row, c.R6Col\n    END\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--EXECUTE gd_getItem 74, 165\n--GO\nselect * from R2Item\n--select * from R6TemplateItem\n\n\nSet Item\n-- =============================================\n-- Create procedure gd_setItem\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_setItem'\n       AND    type = 'P')\n    DROP PROCEDURE gd_setItem\nGO\n\nCREATE PROCEDURE gd_setItem\n    @ItemId int = NULL,\n    @ParentId int = NULL,\n    @GdDdId varchar(30) = NULL,\n    @Format varchar(50) = NULL,\n    @Row int = NULL,\n    @Col int = NULL,\n    @SeqNo int = NULL,\n    @DivisionId int = NULL,\n    @Value varchar(250) = NULL,\n    @ItemLinkId int = NULL,\n    @TaxPackageRef varchar(50) = NULL,\n    @ProcessId int = NULL,\n    @Attribute int = NULL\n\nAS\nSET ROWCOUNT 0\nDECLARE @ErrorMsgID int\nDECLARE @DataListId int\nDECLARE @div int\nDECLARE @DataId int\n\n-- A Item conists of two parts \n-- 1, its navigation, specified by the R2Item table\n-- 2, its data, specified by the R3Data table\n-- There can only ever be one navigation record \n-- There can be many Data records.\n-- A Item can be uniquly identified by its @ParentId, @GdDdId, @SeqNo and @DivisionId\n\nBEGIN TRAN T1\n\n    IF @ItemId IS NULL \n    BEGIN\n        -- Does a Item exist for this record\n        SET @ItemId = ( SELECT R2ItemId \n                        FROM R2Item \n                        WHERE R2ParentId = @ParentId and R2GdDdId = @GdDdId and R2SeqNo = @SeqNo)\n    END\n                \n    IF @ItemId IS NULL or @ItemId = 0 or @ItemId = -1\n    BEGIN\n        -- Create a new item record\n        -- Create a data record\n\n        DECLARE @depth int;\n        DECLARE @lineage varchar(100);\n\n        SELECT @depth=R2Depth, @lineage=R2Lineage FROM R2Item where R2ItemId = @ParentId\n        SET @depth = @depth + 1;\n        SET @lineage = @lineage + LTRIM(Str(@ParentId, 6,0)) + '.'\n\n        INSERT R2Item ( R2ParentId, R2GdDdId, R2Format, R2Row, R2Col, R2SeqNo, R2Depth, R2Lineage)\n        VALUES( @ParentId, @GdDdId, @Format, @Row, @Col, @SeqNo, @depth, @lineage)\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_setItem: Unable to Insert a new Navigation Item',10,1)\n            RETURN -1\n        END\n\n        SET @ItemId = @@IDENTITY;\n\n        INSERT R3Data( R3ItemId, R3DivisionId, R3Value, R3ItemLinkId, R3TaxPackageRef, R3ProcessId, R3Attribute)\n        VALUES( @ItemId, @DivisionId, @Value, @ItemLinkId, @TaxPackageRef, @ProcessId, @Attribute)\n        \n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_setItem: Unable to Insert a new Data Item',10,1)\n            RETURN -1\n        END\n\n\n        -- Update Depth and lineage\n        IF (SELECT COUNT( R2ItemId) FROM R2Item) = 1\n        BEGIN\n            UPDATE R2Item\n            SET R2Depth = 0 , R2Lineage = '.'\n            WHERE R2ItemId = 1\n        END\n    END\n    ELSE IF @ItemId IS NOT NULL\n    BEGIN\n        -- Update the item record\n\n        UPDATE R2Item\n        SET R2ParentId = @ParentId\n            , R2GdDdId = @GdDdId\n            , R2Format = @Format\n            , R2Row = @Row\n            , R2Col = @Col\n            , R2SeqNo = @SeqNo\n        WHERE R2ItemId = @ItemId\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_setItem: Unable to Update a Item',10,1)\n            RETURN -1\n        END\n \n        -- Does the data record exist\n\n        IF @DivisionId IS NULL\n        BEGIN\n            SET @DataId = ( SELECT R3DataId \n                            FROM R3Data\n                            WHERE R3ItemId = @ItemId and R3DivisionId is null)\n        END\n        ELSE\n        BEGIN\n            SET @DataId = ( SELECT R3DataId \n                            FROM R3Data\n                            WHERE R3ItemId = @ItemId and R3DivisionId = @DivisionId)\n        END\n\n        IF @DataId IS NULL\n        BEGIN\n            -- Create a new Data Record\n\n            INSERT R3Data( R3ItemId, R3DivisionId, R3Value, R3ItemLinkId, R3ProcessId, R3Attribute)\n            VALUES( @ItemId, @DivisionId, @Value, @ItemLinkId, @ProcessId, @Attribute)\n\n            SET @ErrorMsgID =@@ERROR\n            IF @ErrorMsgID &lt;&gt;0\n            BEGIN\n                ROLLBACK TRAN T1\n                RAISERROR ('gd_setItem: Unable to Insert a new Data Item',10,1)\n                RETURN -1\n            END\n        END\n        ELSE\n        BEGIN\n            -- Update the Data Record\n\n            UPDATE R3Data\n            SET R3DivisionId = @DivisionId\n                , R3Value = @Value\n                , R3ItemLinkId = @ItemLinkId\n                , R3TaxPackageRef = @TaxPackageRef\n                , R3ProcessId = @ProcessId\n                , R3Attribute = @Attribute\n            WHERE R3DataId = @DataId\n\n            SET @ErrorMsgID =@@ERROR\n            IF @ErrorMsgID &lt;&gt;0\n            BEGIN\n                ROLLBACK TRAN T1\n                RAISERROR ('gd_setItem: Unable to Update a Data Item',10,1)\n                RETURN -1\n            END\n        END\n    END\n    ELSE\n    BEGIN\n        ROLLBACK TRAN T1\n        RAISERROR ('gd_setItem: Should not of got here',10,1)\n        RETURN -1\n    END\n\n    COMMIT TRAN T1\n    RETURN @ItemId\nGO  \n\n\nDelete Item\n-- =============================================\n-- Create procedure gd_delItem\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_delItem' \n       AND    type = 'P')\n    DROP PROCEDURE gd_delItem\nGO\n\n\nCREATE PROCEDURE gd_delItem \n    @ItemId int = NULL\nAS\n    DECLARE @ErrorMsgID int\n\n    -- Delete a item and all the rows that refer to it.\n\n    BEGIN TRAN T1\n        \n        -- Delete children of this Item\n        DELETE \n        FROM R2Item \n        WHERE R2Lineage like '%.'+cast( @ItemId AS varchar(10)) +'.%'\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_delItem: Unable to Delete Item',10,1)\n            RETURN -1\n        END\n\n        -- Clear any links to this Item\n        UPDATE R3Data\n        SET R3ItemLinkId = NULL\n        WHERE R3ItemLinkId = @ItemId\n\n\n        -- Delete this Item\n        DELETE \n        FROM R2Item\n        WHERE R2ItemId = @ItemId\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_delItem: Unable to Delete Item',10,1)\n            RETURN -1\n        END\n\n    COMMIT TRAN T1\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--gd_delItem 13\n\n\nDelete Item Child\n-- =============================================\n-- Create procedure gd_delItemChild\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_delItemChild' \n       AND    type = 'P')\n    DROP PROCEDURE gd_delItemChild\nGO\n\n\nCREATE PROCEDURE gd_delItemChild \n    @ItemId int = NULL\nAS\n    DECLARE @ErrorMsgID int\n\n    -- Delete a item and all the rows that refer to it.\n\n    BEGIN TRAN T1\n\n        DELETE\n        FROM R2Item \n        WHERE R2ParentId = @ItemId\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN T1\n            RAISERROR ('gd_delItem: Unable to Delete Items Children',10,1)\n            RETURN -1\n        END\n\n    COMMIT TRAN T1\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--gd_delItem 10067\n\n\nGet TemplateItem\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_getTemplateItem' \n       AND    type = 'P')\n    DROP PROCEDURE gd_getTemplateItem\nGO\n\nCREATE PROCEDURE gd_getTemplateItem \n    @ItemId int = NULL\nAS\n\n    IF @ItemId IS NOT NULL\n    BEGIN\n        SELECT a.R6TemplateItemId AS ItemId, a.R6ParentId AS ParentId, a.R6GdDdId AS GdDdId,\n            a.R6Format AS Format, a.R6Row AS Row, a.R6Col AS Col, a.R6SeqNo AS SeqNo,\n            a.R6Value AS Value, a.R6ItemLinkId AS ItemLinkId\n        FROM R6TemplateItem a\n        WHERE (a.R6TemplateItemId = ( SELECT R6TemplateItemId FROM R6TemplateItem WHERE R6ParentId = 0 \n                and R6GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId))\n            or a.R6ParentId = ( SELECT R6TemplateItemId FROM R6TemplateItem WHERE R6ParentId = 0 \n                and R6GdDdId = ( SELECT R2GdDdId FROM R2Item WHERE R2ItemId = @ItemId)))\n        ORDER BY a.R6ParentId, a.R6Row, a.R6Col\n    END\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\nEXECUTE gd_getTemplateItem 10\nGO\n\n\nGet Data Item\n-- =============================================\n-- Create procedure basic template\n-- =============================================\n-- creating the store procedure\nIF EXISTS (SELECT name\n       FROM   sysobjects\n       WHERE  name = N'gd_getDataItem'\n       AND    type = 'P')\n    DROP PROCEDURE gd_getDataItem\nGO\n\nCREATE PROCEDURE gd_getDataItem\n    @ItemId int = NULL,\n    @DivisionId int = NULL\nAS\n\n\n    IF @ItemId IS NOT NULL and @DivisionId IS NULL\n    BEGIN\n        SELECT a.R2ItemId AS ItemId,\n            a.R2ParentId AS ParentId,\n            a.R2GdDdId AS GdDdId,\n            a.R2Format AS Format,\n            a.R2Row AS Row,\n            a.R2Col AS Col,\n            a.R2SeqNo AS SeqNo,\n        a.R2Depth AS Depth,\n            b.R3DivisionId AS DivisionId,\n            b.R3Value AS Value,\n            b.R3ItemLinkId AS ItemLinkId,\n            b.R3TaxPackageRef AS TaxPackageRef,\n            b.R3ProcessId AS ProcessId,\n            b.R3Attribute AS Attribute,\n            b.R3DataId AS DataId\n        FROM R2Item a, R3Data b\n        WHERE (a.R2ItemId = @ItemId or a.R2ParentId = @ItemId)\n        and b.R3ItemId = a.R2ItemId\n    and (b.R3DivisionId IS NULL\n    or b.R3DivisionId = (\n            SELECT R5DivisionId\n            FROM R5Division\n            WHERE b.R3DivisionId IS NOT NULL\n            and R5DivisionId = b.R3DivisionId\n            and R5Deleted IS NULL))\n        ORDER BY a.R2ParentId,a.R2GdDdId,a.R2SeqNo\n    END\n    ELSE IF @ItemId IS NOT NULL and @DivisionId IS NOT NULL\n    BEGIN\n    -- Make sure its not marked as deleted.\n    IF( SELECT R5DivisionId FROM R5Division WHERE R5DivisionId = @DivisionId and R5Deleted IS NULL) &gt; 0\n    BEGIN\n            SELECT a.R2ItemId AS ItemId,\n                a.R2ParentId AS ParentId,\n                b.R3DataId AS DataId,\n                a.R2GdDdId AS GdDdId,\n                a.R2Format AS Format,\n                a.R2Row AS Row,\n                a.R2Col AS Col,\n                a.R2SeqNo AS SeqNo,\n                a.R2Depth AS Depth,\n                b.R3DivisionId AS DivisionId,\n                b.R3Value AS Value,\n                b.R3ItemLinkId AS ItemLinkId,\n                b.R3TaxPackageRef AS TaxPackageRef,\n                b.R3ProcessId AS ProcessId,\n                b.R3Attribute AS Attribute,\n                b.R3DataId AS DataId\n               FROM R2Item a, R3Data b\n            WHERE (a.R2ItemId = @ItemId or a.R2ParentId = @ItemId)\n            and b.R3ItemId = a.R2ItemId\n        and (b.R3DivisionId = @DivisionId or b.R3DivisionId IS NULL)\n            ORDER BY a.R2ParentId,a.R2GdDdId,a.R2SeqNo\n    END\n    END\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\nEXECUTE gd_getDataItem 1803, 167\nGO\n\n\nSet Template Item\n-- =============================================\n-- Create procedure gd_setTemplateItem\n-- =============================================\n\n-- creating the store procedure\nIF EXISTS (SELECT name \n       FROM   sysobjects \n       WHERE  name = N'gd_setTemplateItem' \n       AND    type = 'P')\n    DROP PROCEDURE gd_setTemplateItem\nGO\n\nCREATE PROCEDURE gd_setTemplateItem \n    @ItemId int = NULL,\n    @ParentId int = NULL,\n    @GdDdId varchar(30) = NULL,\n    @Format varchar(50) = NULL,\n    @Row int = NULL, \n    @Col int = NULL,\n    @SeqNo int = NULL,\n    @DivisionId int = NULL,\n    @Value varchar(50) = NULL,\n    @ItemLinkId int = NULL\nAS\n    SET ROWCOUNT 0\n    DECLARE @ErrorMsgID int\n    DECLARE @DataListId int\n    DECLARE @div int\n\n    BEGIN TRAN Template\n\n    IF @ItemId IS NULL\n    BEGIN\n        INSERT R6TemplateItem ( R6ParentId, R6GdDdId, R6Format, R6Row, R6Col, R6SeqNo, R6Value, R6ItemLinkId ) \n        VALUES( @ParentId, @GdDdId, @Format, @Row, @Col, @SeqNo, @Value, @ItemLinkId)\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN Template\n            RAISERROR ('gd_setTemplateItem: Unable to Insert a new Item',10,1)\n            RETURN -1\n        END\n\n        SET @ItemId = @@IDENTITY;\n\n        -- Update Depth and lineage\n        IF (SELECT COUNT( R6TemplateItemId) FROM R6TemplateItem) = 1\n        BEGIN\n            UPDATE R6TemplateItem\n            SET R6Depth = 0 , R6Lineage = '.'\n            WHERE R6TemplateItemId = 1\n        END\n\n        WHILE EXISTS (SELECT * FROM R6Item WHERE R6Depth IS NULL)\n        UPDATE T SET T.R6Depth = P.R6Depth +1, \n                T.R6Lineage = P.R2Lineage + LTRIM(Str(T.R6ParentId, 6,0)) + '.'\n        FROM R2Item As T\n            INNER JOIN R6Item AS P on (T.R6ParentId=P.R6ItemId)\n        WHERE P.R2Depth &gt;=0\n            AND P.R6Lineage IS NOT NULL\n            AND T.R6Depth IS NULL\n\n    END\n    ELSE IF @ItemId IS NOT NULL\n    BEGIN\n        UPDATE R6TemplateItem\n        SET R6ParentId = @ParentId, R6GdDdId = @GdDdId, \n            R6Format = @Format, R6Row = @Row, R6Col = @Col, R6SeqNo = @SeqNo, R6Value = @Value, R6ItemLinkId = @ItemLinkId\n        WHERE R6TemplateItemId = @ItemId\n\n        SET @ErrorMsgID =@@ERROR\n        IF @ErrorMsgID &lt;&gt;0\n        BEGIN\n            ROLLBACK TRAN Template\n            RAISERROR ('gd_setTemplateItem: Unable to Update a Item',10,1)\n            RETURN -1\n        END\n    \n\n    END\n    ELSE\n    BEGIN\n        ROLLBACK TRAN Template\n        RAISERROR ('gd_setTemplateItem: Should not of got here',10,1)\n        RETURN -1\n    END\n\n    COMMIT TRAN Template\n\n    -- SET the OUTPUT Parameter\n--  SET @RTN = @ItemId;\n\n--  EXECUTE gd_getTemplateItem @ItemId;\n    RETURN @ItemId\n\nGO\n\nGO\n\n-- =============================================\n-- example to execute the store procedure\n-- =============================================\n--gd_setTemplateItem @ParentId = 1, @GdDdId = 'fred', @Format = 'xxx', @Row = 3, @Col = 2, @Value = '54'\n\n--gd_getItem 10001\n--gd_getTemplateItem 25990\n--select * from gd_vItem\n\n-- set a new template item\n-- gd_setTemplateItem @ParentId = NULL, @GdDdId = '', @Format = '', @Row= NULL, @Col = NULL, @Value = NULL\n\n-- update a template item\n-- gd_setTemplateItem @ItemId = NULL, @ParentId = NULL, @GdDdId = '', @Format = '', @Row= NULL, @Col = NULL, @Value = NULL\n\n\n\n\nView Data Item\n-- =============================================\n-- Create view gd_vDataItem\n-- =============================================\nIF EXISTS (SELECT TABLE_NAME\n       FROM   INFORMATION_SCHEMA.VIEWS\n       WHERE  TABLE_NAME = N'gd_vDataItem')\n    DROP VIEW gd_vDataItem\nGO\n\nCREATE VIEW gd_vDataItem\nAS\n    SELECT a.R2ItemId AS ItemId,\n        a.R2ParentId AS ParentId,\n        a.R2GdDdId AS GdDdId,\n        a.R2Depth AS Depth,\n        c.R6Format AS Format,\n        c.R6Row AS Row,\n        c.R6Col AS Col,\n        a.R2SeqNo AS SeqNo,\n        c.R6SeqNo AS TemplateSeqNo,\n        b.R3DivisionId AS DivisionId,\n        b.R3Value AS Value,\n        b.R3ItemLinkId AS ItemLinkId,\n        b.R3TaxPackageRef AS TaxPackageRef,\n        b.R3ProcessId AS ProcessId,\n        b.R3Attribute AS Attribute\n    FROM R2Item a, R3Data b, R6TemplateItem c\n    WHERE a.R2ItemId = b.R3ItemId\n        and a.R2GdDdId = c.R6GdDdId\n\nGO\nselect * from gd_vDataItem\n\n\nView Template Item\n-- =============================================\n-- Create view gd_vTemplateItem\n-- =============================================\nIF EXISTS (SELECT TABLE_NAME \n       FROM   INFORMATION_SCHEMA.VIEWS \n       WHERE  TABLE_NAME = N'gd_vTemplateItem')\n    DROP VIEW gd_vTemplateItem\nGO\n\nCREATE VIEW gd_vTemplateItem\nAS \n    SELECT R6TemplateItemId AS ItemId\n        , R6ParentId AS ParentId\n        , R6GdDdId AS GdDdId\n        , R6Depth AS Depth\n        , R6Lineage AS Lineage\n        , R6Format AS Format\n        , R6Row AS Row\n        , R6Col AS Col\n        , R6Value AS Value\n        , R6SeqNo AS SeqNo\n        , R6ItemLinkId AS ItemLinkId\n    FROM R6TemplateItem ti\n\n\nGO\nselect * from gd_vTemplateItem \n\n\nView Template Tree\n-- =============================================\n-- Create view gd_vTemplateTree\n-- =============================================\nIF EXISTS (SELECT TABLE_NAME \n       FROM   INFORMATION_SCHEMA.VIEWS \n       WHERE  TABLE_NAME = N'gd_vTemplateTree')\n    DROP VIEW gd_vTemplateTree\nGO\n\nCREATE VIEW gd_vTemplateTree\nAS \n    SELECT R6TemplateItemId AS ItemId\n        , R6ParentId AS ParentId\n        , R6GdDdId AS GdDdId\n        , R6Depth AS Depth\n        , R6Lineage AS Lineage\n        , R6Format AS Format\n        , R6Row AS Row\n        , R6Col AS Col\n        , R6Value AS Value\n        , R6SeqNo AS SeqNo\n        , R6ItemLinkId AS ItemLinkId\n    FROM R6TemplateItem ti\n    WHERE R6ParentId = 0\n\nGO\nselect * from gd_vTemplateTree"
  },
  {
    "objectID": "Content/Journal/posts/MediatorPattern/index.html#good-for",
    "href": "Content/Journal/posts/MediatorPattern/index.html#good-for",
    "title": "GOF Mediator Pattern",
    "section": "Good For",
    "text": "Good For\n\nLoose coupling\nDelegates the interaction between the objects to a separate mediation object."
  },
  {
    "objectID": "Content/Journal/posts/MediatorPattern/index.html#references",
    "href": "Content/Journal/posts/MediatorPattern/index.html#references",
    "title": "GOF Mediator Pattern",
    "section": "References",
    "text": "References\n\nhttps://www.dofactory.com/net/mediator-design-pattern\nhttps://en.wikipedia.org/wiki/Mediator_pattern"
  },
  {
    "objectID": "Content/Journal/posts/KeyVault/index.html",
    "href": "Content/Journal/posts/KeyVault/index.html",
    "title": "KeyVault",
    "section": "",
    "text": "I‚Äôm always looking for ways to simply automate deployments, so I don‚Äôt have to hard code, or repeat myself. This blog is looking at Key Vault deployments, and its use."
  },
  {
    "objectID": "Content/Journal/posts/KeyVault/index.html#parameters",
    "href": "Content/Journal/posts/KeyVault/index.html#parameters",
    "title": "KeyVault",
    "section": "Parameters",
    "text": "Parameters\nFor Key Vaults I don‚Äôt like to specify the normal key/value pairs, as it would mean that I have to specify the name of the secret within the deployment script. The obvious draw back here is, as the code evolves and the secret changes then you would need to update both the parameters plus the deployment script. So first off in the parameters file put the secrets into an array.\n    \"para_kvSecretsObject\": {\n      \"value\": {\n        \"secrets\": [\n          {\n            \"secretName\": \"applicationuser\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          },\n          {\n            \"secretName\": \"AnotherSecret\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          }\n        ]\n      }\nStraight away you can see that the actual secret is injected from the CI tool, this is so you can inject a different secret depending on the environment. Putting the secrets into an array in this way means that we can cycle through the array in the deployment script without actually referring to the secret itself, only using the array index."
  },
  {
    "objectID": "Content/Journal/posts/KeyVault/index.html#load-secrets",
    "href": "Content/Journal/posts/KeyVault/index.html#load-secrets",
    "title": "KeyVault",
    "section": "Load Secrets",
    "text": "Load Secrets\nLoading of secrets is normally achieved via the actual KeyVault deployment specifying each secret in turn, but as our secrets are specified as part of an array, we will need to apply them in a slightly different way. At the resource level we create a new feature which has a type of Microsoft.KeyVault/vaults/secrets. In the following code block the lines to look out for are the Copy/Name and Value properties. * The Copy property looks at the array and determines the number of items we need to process. * The Name property set the secret name to the ‚ÄòsecrectName‚Äô value in the array and it is indexed by the copyIndex() parameter * The Value is specified in the same way as the Name just using the secretValue array name.\nOne important line which is often overlooked is the DependsOn property. Each resource in the deployment file can be applied by Azure in parallel so by specifying the DependsOn property stops the deployment until the dependant resource exists.\n    {\n      \"apiVersion\": \"2015-06-01\",\n      \"copy\": {\n        \"name\": \"secretsCopy\",\n        \"count\": \"[length(parameters('para_kvSecretsObject').secrets)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_kv_name'), '/', parameters('para_kvSecretsObject').secrets[copyIndex()].secretName)]\",\n      \"properties\": {\n        \"value\": \"[parameters('para_kvSecretsObject').secrets[copyIndex()].secretValue]\"\n      },\n      \"tags\": {\n        \"displayName\": \"Key Vault Secrets\"\n      },\n      \"type\": \"Microsoft.KeyVault/vaults/secrets\"\n    }"
  },
  {
    "objectID": "Content/Journal/posts/KeyVault/index.html#accessing-the-key-vault",
    "href": "Content/Journal/posts/KeyVault/index.html#accessing-the-key-vault",
    "title": "KeyVault",
    "section": "Accessing the Key Vault",
    "text": "Accessing the Key Vault\nA Key Vault cannot exist on its own, when it is deployed it needs the service principal, at the moment the service principal can only be obtained from an AD associated application, such as a web app/api app or function."
  },
  {
    "objectID": "Content/Journal/posts/gRPC/index.html",
    "href": "Content/Journal/posts/gRPC/index.html",
    "title": "gRPC",
    "section": "",
    "text": "Overview\nThis started out as a google project, hence the ‚Äòg‚Äô, but the gRPC community is keen to distance itself from that. gRPC is quickly becoming a standard that getting wide adoption across the industry.\ngRPC makes use of the features of HTTP/2 such as protocol buffers to remotely calls methods deployed on other resources, such as other VM‚Äôs or on different cloud services.\ngRPC allows the developer to make calls to remote methods as if they are local resources, so there is no need for HTTP/REST infrastructure.\ngRPC uses HTTP/2 so takes advantage of the inbuilt TLS message security.\ngRPC is a protocol based service, where the user describes the methods, parameters and return objects, like swagger.\ngRPC application developer, working within a company, should develop a nuget client library, so the client developer doesn‚Äôt need to know that remote calls are happening.\ngRPC, Servers and Clients can be on vastly different architectures or environments.\n\n\nProtocol Buffers\nBy default gRPC works with protocol buffers as originally defined by google, but its equally at home with JSON.\nIf you use Protocol Buffers then you create a simple text file with a .proto extension,\nsyntax = \"proto3\";\n\nservice Greeter {\n  rpc SayHello (HelloRequest) returns (HelloReply);\n}\n\nmessage HelloRequest {\n  string name = 1;\n}\n\nmessage HelloReply {\n  string message = 1;\n}\nThen use the protoc compiler to generate data access classes in your desired language\n\n\nHTTP/2\n\nCompress Headers - HPACK\n\nEvery HTTP request sends headers. HTTP/2 now compresses those headers reducing the package size\n\nMultiplexing\n\nWeb pages can contain 100s of resources, in previous versions of HTTP each resource had to be requested individually in sequential order. With HTTP/2 your requests will be non blocking.\n\nSupported by all major browsers\n\nAll major browsers support HTTP/2, but if your browser doesnt then the protocol will be downgraded to HTTP 1.1\n\n\n\n\nReference\n\ngRPC\n\ngRPC.io\nGoogle gRPC\nMicrosoft gRPC\nYouTube - Blazor\n\nHTTP/2\n\nWikiPedia HTTP/2\nGoogle HTTP/2\nIETF HTTP Working Group\nYouTube - HTTP/2"
  },
  {
    "objectID": "Content/Journal/posts/Frontify/index.html",
    "href": "Content/Journal/posts/Frontify/index.html",
    "title": "Frontify Marketing Platform",
    "section": "",
    "text": "This project was integration between a internal system and the Frontify Marketing platform.\nFrontify"
  },
  {
    "objectID": "Content/Journal/posts/EventNotification/index.html",
    "href": "Content/Journal/posts/EventNotification/index.html",
    "title": "Event Notificiation",
    "section": "",
    "text": "Azure Event Grid with Topics is a enterprise grade notification engine which mimics Azure Service Bus in many ways, Where it differes from Service Bus is that it is a serverless component, and has no physical storage of messages, so cannot, in the event of a incident, gaurentee delivery. What id does however offer in a normal operating environment is a fast, low latency delivery of messages that can be consumed easily.\nThe following description shows a particular implementation of event grid messaging, which is used in conjunction with Azure SignalR so that a web GUI can receive notifications.\n\n\n\nEventGridSignalR\n\n\n\nThe Web page gets the SignalR connection from the Azure Function.\nThe Event Source publishes a message to a Event Grid Topic.\nThe Azure Function subscribes to a particular topic, the Event Grid can have many topics.\nThe Azure Function publishes the message to SignalR\nThe Web page receive the message from the signalR WebSocket."
  },
  {
    "objectID": "Content/Journal/posts/EventNotification/index.html#azure-function",
    "href": "Content/Journal/posts/EventNotification/index.html#azure-function",
    "title": "Event Notificiation",
    "section": "Azure Function",
    "text": "Azure Function\n    public static class CloudEventSubscription\n    {\n        // Azure Function for handling negotation protocol for SignalR. It returns a connection info\n        // that will be used by Client applications to connect to the SignalR service.\n        // It is recommended to authenticate this Function in production environments.\n        [FunctionName(\"negotiate\")]\n        public static SignalRConnectionInfo GetSignalRInfo(\n            [HttpTrigger(AuthorizationLevel.Anonymous, \"post\")] HttpRequest req,\n            [SignalRConnectionInfo(HubName = \"cloudEventSchemaHub\")] SignalRConnectionInfo connectionInfo,\n            ILogger log)\n        {\n            log.LogInformation(\"negotiate\");\n            return connectionInfo;\n        }\n\n        // Azure Function for handling Event Grid events using CloudEventSchema v1.0 \n        // (see CloudEvents Specification: https://github.com/cloudevents/spec)\n        [FunctionName(\"EventSubscription\")]\n        public static async Task&lt;IActionResult&gt; Run(\n            [HttpTrigger(AuthorizationLevel.Anonymous, \"POST\", \"OPTIONS\", Route = null)] HttpRequest req,\n            [SignalR(HubName = \"cloudEventSchemaHub\")] IAsyncCollector&lt;SignalRMessage&gt; signalRMessages,\n            ILogger log)\n        {\n            log.LogInformation(\"CloudEventSubscription\");\n\n            // Handle EventGrid subscription validation for CloudEventSchema v1.0.\n            // It sets the header response `Webhook-Allowed-Origin` with the value from \n            // the header request `Webhook-Request-Origin` \n            // (see: https://docs.microsoft.com/en-us/azure/event-grid/cloudevents-schema#use-with-azure-functions)\n            if (HttpMethods.IsOptions(req.Method))\n            {\n                log.LogInformation(\"CloudEventSubscription - Options\");\n                if (req.Headers.TryGetValue(\"Webhook-Request-Origin\", out var headerValues))\n                {\n                    log.LogInformation(\"CloudEventSubscription - Webhook-Request-Origin\");\n                    var originValue = headerValues.FirstOrDefault();\n                    if(!string.IsNullOrEmpty(originValue))\n                    {\n                        req.HttpContext.Response.Headers.Add(\"Webhook-Allowed-Origin\", originValue);\n                        return new OkResult();\n                    }\n                    log.LogInformation(\"CloudEventSubscription - Missing Webhook-Request-Origin\");\n                    return new BadRequestObjectResult(\"Missing 'Webhook-Request-Origin' header when validating\");\n                }\n            }\n            \n            // Handle an event received from EventGrid. It reads the event from the request payload and send \n            // it to the SignalR serverless service using the Azure Function output binding\n            if(HttpMethods.IsPost(req.Method)) \n            {\n                string @event = await new StreamReader(req.Body).ReadToEndAsync();\n                await signalRMessages.AddAsync(new SignalRMessage\n                {\n                    Target = \"newEvent\",\n                    Arguments = new[] { @event }\n                });\n            }\n\n            log.LogInformation(\"CloudEventSubscription - SignalR Post\");\n            return new OkResult();\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/EventNotification/index.html#angular",
    "href": "Content/Journal/posts/EventNotification/index.html#angular",
    "title": "Event Notificiation",
    "section": "Angular",
    "text": "Angular\nimport { Component } from '@angular/core';\nimport * as SignalR from '@microsoft/signalr';\n\n@Component({\n  selector: 'app-root',\n  templateUrl: './app.component.html',\n  styleUrls: ['./app.component.css']\n})\nexport class AppComponent {\n\n  title = 'viewer-app';\n  events: string[] = [];\n\n  private hubConnection: SignalR.HubConnection;\n\n  constructor() {\n    // Create connection\n    this.hubConnection = new SignalR.HubConnectionBuilder()\n      .withUrl(\"https://func-pocegsr-vse-ne.azurewebsites.net/api/\")\n      .build();\n\n    // Start connection. This will call negotiate endpoint\n    this.hubConnection\n      .start();\n\n    // Handle incoming events for the specific target\n    this.hubConnection.on(\"newEvent\", (event) =&gt; {\n      this.events.push(event);\n    });\n  }\n}"
  },
  {
    "objectID": "Content/Journal/posts/EventNotification/index.html#arm-template",
    "href": "Content/Journal/posts/EventNotification/index.html#arm-template",
    "title": "Event Notificiation",
    "section": "Arm Template",
    "text": "Arm Template\n    {\n      \"type\": \"Microsoft.SignalRService/SignalR\",\n      \"apiVersion\": \"2022-02-01\",\n      \"name\": \"[variables('var_sr_name')]\",\n      \"location\": \"[resourceGroup().location]\",\n      \"sku\": {\n        \"name\": \"Standard_S1\",\n        \"tier\": \"Standard\",\n        \"size\": \"S1\",\n        \"capacity\": 1\n      },\n      \"kind\": \"SignalR\",\n      \"properties\": {\n        \"tls\": {\n          \"clientCertEnabled\": false\n        },\n        \"features\": [\n          {\n            \"flag\": \"ServiceMode\",\n            \"value\": \"Serverless\",\n            \"properties\": {}\n          },\n          {\n            \"flag\": \"EnableConnectivityLogs\",\n            \"value\": \"True\",\n            \"properties\": {}\n          }\n        ],\n        \"cors\": {\n          \"allowedOrigins\": [\n            \"*\"\n          ]\n        },\n        \"upstream\": {},\n        \"networkACLs\": {\n          \"defaultAction\": \"Deny\",\n          \"publicNetwork\": {\n            \"allow\": [\n              \"ServerConnection\",\n              \"ClientConnection\",\n              \"RESTAPI\",\n              \"Trace\"\n            ]\n          },\n          \"privateEndpoints\": []\n        },\n        \"publicNetworkAccess\": \"Enabled\",\n        \"disableLocalAuth\": false,\n        \"disableAadAuth\": false\n      },\n      \"tags\": {\n        \"displayName\": \"SignalR\"\n      }\n    },\n    {\n      \"type\": \"Microsoft.EventGrid/topics\",\n      \"apiVersion\": \"2021-12-01\",\n      \"name\": \"[variables('var_egt_name')]\",\n      \"location\": \"[resourceGroup().location]\",\n      \"identity\": {\n        \"type\": \"None\"\n      },\n      \"properties\": {\n        \"inputSchema\": \"CloudEventSchemaV1_0\",\n        \"publicNetworkAccess\": \"Enabled\",\n        \"inboundIpRules\": [],\n        \"disableLocalAuth\": false\n      },\n      \"tags\": {\n        \"displayName\": \"Event Grid Topic\"\n      }\n    },"
  },
  {
    "objectID": "Content/Journal/posts/Documentation/index.html",
    "href": "Content/Journal/posts/Documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation is task that is best shared among all the relevant people, each supplying enough information that satisfies there understanding of the issue.\n\nBusiness User\n\nWhat they need\nWhat would they recognize as success\nWhat would indicate a failure\n\nBusiness Analyst\n\nGather\n\nGather requirements from the business user\nGather Success and Failure criteria from the business user\n\nRefactor\n\nBreak down the requirements into distinct items\nChallenge contradictions and Remove duplications\n\nAgainst previous requirements\n\n\nReview\nInterpret the Business Users requirements\nMake sure it doesn‚Äôt conflict with previous requirements\nGenerate Feature/Stories\nWith Acceptance Criteria !!\n\nDeveloper\n\nCreate a BDD Feature File\nGenerate scenarios to satisfy the story\n\nTest\nCode\nRefactor\n\n\nThe same principals of Test,Code,Refactor should be applied to the Requirement gathering process"
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/MainArchitecture.html",
    "href": "Content/Journal/posts/DiagramFirstCoding/MainArchitecture.html",
    "title": "Main Architecture Page (index.md)",
    "section": "",
    "text": "This page contains the C4 model script. You can render this using the Structurizr CLI or the Structurizr Lite Docker container.\n&lt;size:24&gt;Container View: Order Management System&lt;/size&gt;Container View: Order Management SystemOrder Management System[Software System]API¬†Service[Container:¬†Node.js/TypeScript]¬†Handles¬†incoming¬†requestsRules¬†Engine[Container:¬†Java/Drools]¬†Processes¬†business¬†logicUser[Person]¬†A¬†customer¬†of¬†the¬†systemMakes API calls to[JSON/HTTPS]Validates rules via[gRPC]\n\n\n\nReceive Request: Validate JSON schema against TypeSpec model.\nAuth Check: Verify JWT via Identity Provider.\nRule Delegation: Forward payload to the Rules Engine.\nPersistence: Save result to the database."
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/MainArchitecture.html#workflow",
    "href": "Content/Journal/posts/DiagramFirstCoding/MainArchitecture.html#workflow",
    "title": "Main Architecture Page (index.md)",
    "section": "",
    "text": "Receive Request: Validate JSON schema against TypeSpec model.\nAuth Check: Verify JWT via Identity Provider.\nRule Delegation: Forward payload to the Rules Engine.\nPersistence: Save result to the database."
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/api-service.html",
    "href": "Content/Journal/posts/DiagramFirstCoding/api-service.html",
    "title": "Component: API Service (api-service.md)",
    "section": "",
    "text": "Component: API Service (api-service.md)\nThis sub-page defines the interface contract using TypeSpec. API Specification (TypeSpec)\nimport \"@typespec/http\";\nimport \"@typespec/rest\";\n\nusing Http;\n\n@service({ title: \"Order Service\" })\nnamespace OrderSystem;\n\nmodel Order {\n  @key id: string;\n  customerName: string;\n  amount: float64;\n  status: \"pending\" | \"approved\" | \"rejected\";\n}\n\n@route(\"/orders\")\ninterface Orders {\n  @post create(@body order: Order): Order | Error;\n  @get read(@path id: string): Order | Error;\n}"
  },
  {
    "objectID": "Content/Journal/posts/CastToReflectedType/index.html",
    "href": "Content/Journal/posts/CastToReflectedType/index.html",
    "title": "Cast To a Reflected Type",
    "section": "",
    "text": "How many times do you find yourselves not knowing the type of a object, and wanting to cast it.\nIn my senario, I was working in a table storage library, and I was trying to merge json objects. The issue being, in the library you have no visibility of the data structure, so you have to use reflection.\nentity is the object to be merged. mergedJson is the merged json object.\nThe issue here is the Update method takes a object of TableEntity. If you simply map to a table entity then your object will be missing the data. So in this case you get the type of the passed ‚Äòentity‚Äô in this case AddressDto after you have merged the json you need to deserailise it back into a object. The issue is the deserialise object is of type object and not TableEntity. The ObjectExtensions code was obtain, and performs the cast correctly and creates a eek object of type AddressDto, which is good to update.\n    Type objectType = entity.GetType();\n    var merged2 = JsonConvert.DeserializeObject(mergedJson, objectType);\n    var eek = ObjectExtensions.CastToReflected(merged2, objectType);\n    res = Update(eek);\n\n\n    public static class ObjectExtensions\n    {\n        public static T CastTo&lt;T&gt;(this object o) =&gt; (T)o;\n\n        public static dynamic CastToReflected(this object o, Type type)\n        {\n            var methodInfo = typeof(ObjectExtensions).GetMethod(nameof(CastTo), BindingFlags.Static | BindingFlags.Public);\n            var genericArguments = new[] { type };\n            var genericMethodInfo = methodInfo?.MakeGenericMethod(genericArguments);\n            return genericMethodInfo?.Invoke(null, new[] { o });\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/AzurePipelineGitCommit/index.html",
    "href": "Content/Journal/posts/AzurePipelineGitCommit/index.html",
    "title": "Azure Pipeline Git Commit",
    "section": "",
    "text": "If you are trying to use git within a devops pipeline then you will need to enable the pipeline the bility to commit.\nYou need to grant the permissions to Project Collection Build Service (account name):\n\n\n\nPipelineGit"
  },
  {
    "objectID": "Content/Journal/posts/AzureFunctionManagedServiceIdentities/index.html",
    "href": "Content/Journal/posts/AzureFunctionManagedServiceIdentities/index.html",
    "title": "Azure Function Managed Service Identities",
    "section": "",
    "text": "Bootstrapping\nThe trouble with many security policies is that at least some element needs to know the password in order to instigate access to resources. That used to mean putting credentials into a configuration file or inserting them during a deployment process. The Manage Service Identities (MSI) facility has got around this by allowing all your resources to register a service principal with Active Directory, and then each resource grants the desired level of access to that service principal. By doing the security in this way, each of the resources never need to know credentials, they only request access and deal with the response. So, by removing credentials from the equation then there is no need to have to rotate passwords or update certs on a timely basis as they simply donÔøΩt exist between the resources.\n\n\nSo how do we accomplish this.\nWithin the azure function arm template declaration insert the following, this will register the function with your active directory.\n\"identity\": {\"type\": \"SystemAssigned\"},\nIn the variables section of the Arm Template, get the identity of the Azure Function. (replace ‚Äòvar_azf_name‚Äô with the name of your function)\n\"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\nWithin your Key Vault template your will need to add the functions access policy\n\"accessPolicies\": [{\"tenantId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').tenantId]\",\"objectId\": \"[reference(variables('var_msi_azf'), '2015-08-31-PREVIEW').principalId]\",\"permissions\": {\"certificates\": [\"get\"],\"keys\": [\"get\"],\"secrets\": [\"get\"]}}}]"
  },
  {
    "objectID": "Content/Journal/posts/AzureC4/index.html",
    "href": "Content/Journal/posts/AzureC4/index.html",
    "title": "Azure/C4",
    "section": "",
    "text": "I found a good resource, so I can include bot C4 diagrams and Azure icons within a PlantUML diagram. I‚Äôve done C4 within PlantUml before but it was a bit crude.\n\n\nhttps://github.com/RicardoNiepel/Azure-PlantUML"
  },
  {
    "objectID": "Content/Journal/posts/ARMTemplateParameter-VariableSetup/index.html",
    "href": "Content/Journal/posts/ARMTemplateParameter-VariableSetup/index.html",
    "title": "ARM Template Parameter Variable Setup",
    "section": "",
    "text": "For something so simple, arm templates can become complex things, so I prefer to try to set some ground rules before I go to deep. N.B this works for me, and may not suit everyone üòâ"
  },
  {
    "objectID": "Content/Journal/posts/ARMTemplateParameter-VariableSetup/index.html#parameters",
    "href": "Content/Journal/posts/ARMTemplateParameter-VariableSetup/index.html#parameters",
    "title": "ARM Template Parameter Variable Setup",
    "section": "Parameters",
    "text": "Parameters\nI prefer to inject any unique values via a VSTS/VSO or if your prefer Azure DevOps deployment process.\nIn the first part of the file I spell out the acronyms which form part of the naming convention for the resources, you could use nested templates for this, but I feel they add unnecessary complications, as the nested template must be available via a URL. The second part involves parameters that are specific to this application, such as the tenant id, application name etc.\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"para_acronym_region\": { \"value\": \"we\" },\n        \"para_acronym_resgrp\": { \"value\": \"resgrp\" },\n        \"para_acronym_appsvc\": { \"value\": \"appsvc\" },\n        \"para_acronym_svcpln\": { \"value\": \"svcpln\" },\n        \"para_acronym_stract\": { \"value\": \"str\" },\n        \"para_acronym_kv\": { \"value\": \"kv\" },\n        \"para_acronym_azfunc\": { \"value\": \"fn\" },\n        \"para_acronym_appin\": { \"value\": \"appins\" },\n        \"para_acronym_webapp\": { \"value\": \"webapp\" },\n        \"para_ad_tenantid\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_application_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_vanity_name\": { \"value\": \" OVERWRITTEN BY VSTS \" },\n        \"para_target_env\": { \"value\": \"dev\" },\n        \"para_kvSecretsObject\": {\n            \"value\": {\n                \"secrets\": [\n                        {\n                        \"secretName\": \"applicationuser\",\n                        \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        },\n                        {\n                            \"secretName\": \"AnotherSecrect\",\n                            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n                        }\n                    ]\n                }\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/ARMTemplateParameter-VariableSetup/index.html#variables",
    "href": "Content/Journal/posts/ARMTemplateParameter-VariableSetup/index.html#variables",
    "title": "ARM Template Parameter Variable Setup",
    "section": "Variables",
    "text": "Variables\nAs you can see from the variables, I build up my resource names from the parameters. I also pull in values for the hostingplan and component identities, so they can be used easily with the resource definitions.\n\"variables\": {\n    \"var_env_region\": \"[concat(parameters('para_target_env'), '-', parameters('para_acronym_region'))]\",\n    \"var_public_url\": \"[concat(parameters('para_target_env'), '.', parameters('para_application_name'), '.', parameters('para_vanity_name'))]\",\n    \"var_str_name\": \"[concat(parameters('para_application_name'), parameters('para_acronym_stract'), parameters('para_target_env'), parameters('para_acronym_region'))]\",\n    \"var_str_resId\": \"[resourceId(resourceGroup().Name,'Microsoft.Storage/storageAccounts', variables('var_str_name'))]\",\n    \"var_kv_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_kv'), '-', variables('var_env_region'))]\",\n    \"var_azf_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_azfunc'),'-', variables('var_env_region'))]\",\n    \"var_appin_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appin'),'-', variables('var_env_region'))]\",\n    \"var_hstpln_group\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_resgrp'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_env\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_appsvc'), '-', variables('var_env_region'))]\",\n    \"var_hstpln_name\": \"[concat(parameters('para_application_name'), '-', parameters('para_acronym_svcpln'), '-', variables('var_env_region'))]\",\n    \"var_webapp_name\": \"[concat(parameters('para_application_name'), '-' ,parameters('para_acronym_webapp'),'-', variables('var_env_region'))]\",\n    \"var_webapp_hstpln\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', variables('var_hstpln_group'), '/providers/Microsoft.Web/serverfarms/', variables('var_hstpln_name'))]\",\n    \"var_msi_azf\": \"[concat(resourceId('Microsoft.Web/sites', variables('var_azf_name')),'/providers/Microsoft.ManagedIdentity/Identities/default')]\"\n},"
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html",
    "href": "Content/Journal/posts/APIFirst/index.html",
    "title": "API First",
    "section": "",
    "text": "API First Process"
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#description",
    "href": "Content/Journal/posts/APIFirst/index.html#description",
    "title": "API First",
    "section": "Description",
    "text": "Description\nAPI First is an approach to software development that prioritizes the design and implementation of APIs before building the underlying application or service. This approach emphasizes the importance of APIs as the primary means of communication between different components of a system, as well as between different systems.\nBy designing APIs first, developers can ensure that the APIs are well-defined, consistent, and easy to use. This can lead to better collaboration between teams, faster development cycles, and more efficient integration with third-party services.\nThe approach detailed here takes the responsibility for creating the API definition away from the developer and places it in the hands of a specialist tool. This ensures that the API definition is created in a consistent manner and adheres to best practices. The API definition is then used to generate the client and server code, which can be integrated into the application or service. This approach can help to reduce errors and inconsistencies in the code, as well as improve the overall quality of the API. The API First approach can be particularly beneficial in complex systems where multiple teams are working on different components. By defining the APIs first, teams can work independently while still ensuring that their components will integrate seamlessly with the rest of the system. The API First approach can also help to improve the overall user experience of the application or service. By designing APIs that are easy to use and understand, developers can create applications that are more intuitive and user-friendly. Overall, the API First approach is a powerful way to improve the development process and create high-quality applications and services that are built around well-designed APIs."
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#tools",
    "href": "Content/Journal/posts/APIFirst/index.html#tools",
    "title": "API First",
    "section": "Tools",
    "text": "Tools\nThe tools used in this process are:\n\nTypeSpec - A tool for designing and generating APIs\nAutoRest - A tool for generating client and server code from API definitions\nQuarto - A tool for creating and publishing documents\nLUA - A lightweight scripting language used for automation\nPowerShell - A task automation and configuration management framework\nC# - A programming language used for building applications and services\nAzure Functions - A serverless compute service used for building and deploying applications and services"
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#process",
    "href": "Content/Journal/posts/APIFirst/index.html#process",
    "title": "API First",
    "section": "Process",
    "text": "Process\nThe process for creating an API using the API First approach is as follows:\n\nDesign the API - Use TypeSpec to design the API, including the endpoints, request and response formats, and any authentication or authorization requirements.\nGenerate the API definition - Use TypeSpec to generate the API definition in a standard format, such as OpenAPI or Swagger.\nGenerate the client and server code - Use AutoRest to generate the client and server code from the API definition.\nIntegrate the code - Integrate the generated code into the application or service.\nTest the API - Test the API to ensure that it works as expected and meets the requirements.\n\nDocument the API - Use Quarto to create documentation for the API, including usage examples and any relevant information for developers.\nAutomate the process - Use LUA and PowerShell to automate the process of generating the API definition, client and server code, and documentation."
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#benefits",
    "href": "Content/Journal/posts/APIFirst/index.html#benefits",
    "title": "API First",
    "section": "Benefits",
    "text": "Benefits\nThe benefits of using the API First approach include:\n\n\n\n\n\n\n\nCategory\nQualities\n\n\n\n\nüßë‚Äçüíª Developer Experience & Productivity\n- Improved developer productivity and satisfaction  - Easier onboarding of new developers and teams  - Reduced errors and inconsistencies in the code  - Easier maintenance and updates to the API  - Improved versioning and backward compatibility of the API  - Better integration with DevOps and CI/CD processes\n\n\nüöÄ Development Efficiency & Technical Excellence\n- Faster development cycles  - More efficient integration with third-party services  - Consistent and well-defined APIs  - Better quality APIs  - Better support for multiple platforms and devices  - More effective monitoring and analytics of API usage  - More effective management of API lifecycle and governance\n\n\nüåê Business Alignment & Strategic Impact\n- Better alignment with business goals and requirements  - Increased flexibility and scalability of the application or service  - Enhanced ability to adapt to changing market and customer needs  - Greater overall success and impact of the application or service in the market  - Increased revenue and business opportunities through the API  - Better alignment with industry trends and best practices in API development\n\n\nü§ù Collaboration & Communication\n- Improved collaboration between teams  - Improved communication and transparency with stakeholders and customers\n\n\nüé® Innovation & Design\n- Greater innovation and creativity in API design  - Improved user experience\n\n\nüîí Trust, Security & Adoption\n- Enhanced security and compliance with industry standards  - Greater adoption and usage of the API by developers and users  - Enhanced reputation and credibility of the organization providing the API"
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/index.html#summary",
    "href": "Content/Journal/posts/APIFirst/index.html#summary",
    "title": "API First",
    "section": "Summary",
    "text": "Summary\nThe API First approach is a powerful way to improve the development process and create high-quality applications and services that are built around well-designed APIs. By prioritizing the design and implementation of APIs, developers can ensure that their applications are more efficient, flexible, and user-friendly. The tools and processes outlined in this document can help to streamline the API First approach and make it easier for developers to create and maintain high-quality APIs."
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/Spec-kit.html",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/Spec-kit.html",
    "title": "Spec-Kit AI-Ready Specification Templates for Common Features",
    "section": "",
    "text": "Spec-kit\nSpec-kit is a collection of AI-Ready Specification Templates for various common features. These templates are designed to help developers create clear and comprehensive specifications that can be used to guide AI-driven code generation.\nSpec-kit templates follow the CARE framework, which stands for Context, Action, Result, and Evaluation. This framework helps ensure that specifications are detailed and structured in a way that minimizes ambiguity and enhances the quality of AI-generated code.\nSpec-Kit includes templates for a variety of features, such as user authentication, data validation, API endpoints, and more. Each template provides a structured format that developers can fill out with specific details relevant to their project.\nSpec-kit aims to streamline the process of creating AI-ready specifications, making it easier for developers to leverage AI tools effectively in their code development workflows.\nSpec-kit works with most AI code generation tools, including OpenAI‚Äôs Codex, GitHub Copilot, and Anthropic‚Äôs Claude. By using these templates, developers can improve the accuracy and reliability of the code generated by AI models.\nSpec-kit is a CLI that is installed and run locally from VS Code. Spec-kit sets up a .specify folder in your project root with markdown templates for common features. The .specify folder will intitally contain templates such as:\n* Memory - which should include your constituional principles for AI code generation.\n* Scripts - which contains helper scripts to generate SPEC.md files from templates.\n* Templates - which contains markdown files for various common features.\nThe .specify/Templates folder includes the following AI-Ready Specification Templates:\n\nREADME.md - Overview of Spec-kit and how to use it.\n\nspec-driven.md - Guide to Spec-Driven Development and CARE framework.\nauth-spec.md - AI-Ready Specification Template for User Authentication feature.\ndata-validation-spec.md - AI-Ready Specification Template for Data Validation feature.\napi-endpoint-spec.md - AI-Ready Specification Template for API Endpoint feature.\npagination-spec.md - AI-Ready Specification Template for Pagination feature.\nerror-handling-spec.md - AI-Ready Specification Template for Error Handling feature.\nlogging-spec.md - AI-Ready Specification Template for Logging feature.\nrate-limiting-spec.md - AI-Ready Specification Template for Rate Limiting feature.\nfile-upload-spec.md - AI-Ready Specification Template for File Upload feature.\n\n\n\nReferences\n\nGithub Spec-kit description\nGithub Spec-kit"
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/index.html",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/index.html",
    "title": "AI driven code developement",
    "section": "",
    "text": "AI driven code development is a powerful tool, but it is only as good as the data and instructions it receives. If the input is flawed or incomplete, the output will likely be unreliable. This principle, often summarized as ‚ÄúGarbage In, Garbage Out‚Äù (GIGO), highlights the importance of providing high-quality data and clear requirements when leveraging AI for code generation. To ensure the effectiveness of AI-driven code development, it is crucial to:\n\nDefine Clear Requirements: Clearly articulate the desired functionality, performance criteria, and constraints of the code to be generated. This helps the AI understand the context and objectives.\nProvide Quality Data: Use accurate, relevant, and well-structured data to train and guide the AI models. Poor-quality data can lead to incorrect or suboptimal code generation.\nIterative Refinement: Continuously review and refine the generated code. Human oversight is essential to identify and correct any issues, ensuring that the final output meets the required standards.\nTesting and Validation: Rigorously test the generated code to verify its functionality, performance, and security. Automated testing frameworks can be employed to streamline this process.\n\nBy adhering to these principles, developers can maximize the benefits of AI-driven code development while minimizing the risks associated with flawed inputs.",
    "crumbs": [
      "Now!",
      "AI Driven Code Developement Introduction"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/index.html#for-better-outcomes",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/index.html#for-better-outcomes",
    "title": "AI driven code developement",
    "section": "For Better Outcomes",
    "text": "For Better Outcomes\n\nProvide Stubs: Give the AI the function signature or interface you expect so it doesn‚Äôt ‚Äúhallucinate‚Äù its own.\nModularize: Break features into ‚Äútiny pieces‚Äù rather than one giant prompt to avoid overwhelming the model‚Äôs ‚Äúattention budget‚Äù.\nSpec-Driven Development:\n\nCARE Framework for AI-Driven Code Development\nUse tools like Claude Code in ‚ÄúPlan Mode‚Äù to let the AI draft a detailed SPEC.md based on your high-level goal before any code is written.\nSpec-Kit - A collection of AI-Ready Specification Templates for various common features.",
    "crumbs": [
      "Now!",
      "AI Driven Code Developement Introduction"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html",
    "title": "CARE Framework Example",
    "section": "",
    "text": "Section\nAI-Ready Specification\n\n\n\n\nContext\nCreate a .NET 8 Isolated Azure Function for order processing. It must support a Dead Letter Pattern to capture invalid orders for manual recovery.\n\n\nAction\n1. Input: HTTP POST JSON (OrderId, CustomerEmail, TotalAmount).\n\n\n\n2. Happy Path: Valid orders must be persisted to Azure Table Storage (OrdersTable).\n\n\n\n3. Failure Path: If validation fails (e.g., negative TotalAmount), the function must:\n\n\n\n- Return 400 Bad Request.\n\n\n\n- Persist the invalid payload to an Azure Storage Queue named order-error-poison.\n\n\nResult\n‚Ä¢ Use a Multi-Output Binding class (OrderResponse) to route data to Table and Queue storage.\n\n\n\n‚Ä¢ Implement Identity-based connections (__tableServiceUri and __queueServiceUri).\n\n\n\n‚Ä¢ Include structured logging for both successful and ‚Äúdead-lettered‚Äù outcomes.\n\n\nEvaluation Acceptance Criteria:\n1. No hardcoded connection strings.\n\n\n\n2. 201 Created only if written to Table Storage.\n\n\n\n3. Verify that the FailedOrder property is null during successful runs to avoid duplicate queue entries\n\n\n\n4. Unit test stubs using xUnit and Moq for both success and failure scenarios.\n\n\n\n.N.B. During the generation process, the AI will surgest new features or improvements. These should be carefully evaluated to ensure they align with the original requirement and do not introduce unnecessary complexity. Once you are happy the requirement should be fully updated to reflect the desired outcome, you can proceed to generate the final code.\n\n\n\nFed the requirement into an AI coding assistant (e.g., GPT-4, Claude Code) and received the following code output:\n\n\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\n\npublic class OrderResponse\n{\n    // Routes to Table Storage if not null\n    [TableOutput(\"OrdersTable\", Connection = \"OrdersTableService\")]\n    public OrderEntity? NewOrder { get; set; }\n\n    // Routes to Queue Storage if not null (Poison Queue)\n    [QueueOutput(\"order-error-poison\", Connection = \"OrderQueueService\")]\n    public OrderRequest? FailedOrder { get; set; }\n\n    // Required for the HTTP Trigger's return value\n    public HttpResponseData HttpResponse { get; set; } = null!;\n}\n\npublic record OrderRequest(string OrderId, string CustomerEmail, double TotalAmount);\n\npublic class OrderEntity\n{\n    public string PartitionKey { get; set; } = \"Orders\";\n    public string RowKey { get; set; } = null!;\n    public string CustomerEmail { get; set; } = null!;\n    public double TotalAmount { get; set; }\n}\n\n\n\n\nusing System.Net;\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\n\npublic class OrderProcessor\n{\n    private readonly ILogger _logger;\n\n    public OrderProcessor(ILoggerFactory loggerFactory)\n    {\n        _logger = loggerFactory.CreateLogger&lt;OrderProcessor&gt;();\n    }\n\n    [Function(\"ProcessOrder\")]\n    public async Task&lt;OrderResponse&gt; Run(\n        [HttpTrigger(AuthorizationLevel.Function, \"post\")] HttpRequestData req,\n        FunctionContext executionContext)\n    {\n        var _logger = executionContext.GetLogger(\"ProcessOrder\");\n        var order = await req.ReadFromJsonAsync&lt;OrderRequest&gt;();\n\n        // Validation logic (Failure Path)\n        if (order == null || order.TotalAmount &lt; 0)\n        {\n            _logger.LogWarning(\"Validation failed for OrderId: {OrderId}\", order?.OrderId ?? \"Unknown\");\n            \n            var badResponse = req.CreateResponse(System.Net.HttpStatusCode.BadRequest);\n            return new OrderResponse\n            {\n                HttpResponse = badResponse,\n                FailedOrder = order, // Sent to 'order-error-poison' queue\n                NewOrder = null      // Ensures no Table Storage entry\n            };\n        }\n\n        // Success Path\n        _logger.LogInformation(\"Processing successful order: {OrderId}\", order.OrderId);\n        \n        var okResponse = req.CreateResponse(System.Net.HttpStatusCode.Created);\n        return new OrderResponse\n        {\n            HttpResponse = okResponse,\n            NewOrder = new OrderEntity \n            { \n                RowKey = order.OrderId, \n                CustomerEmail = order.CustomerEmail, \n                TotalAmount = order.TotalAmount \n            },\n            FailedOrder = null // Ensures no Poison Queue entry\n        };\n    }\n\n}\n\n\n\n{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet-isolated\",\n    \"OrdersTableService__tableServiceUri\": \"https://&lt;account&gt;.table.core.windows.net/\",\n    \"OrderQueueService__queueServiceUri\": \"https://&lt;account&gt;.queue.core.windows.net/\" \n\n  }\n}\n\n\n\n\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\nusing Moq;\nusing System.Net;\nusing System.Text;\nusing Xunit;\n\npublic class OrderFunctionTests\n{\n    [Fact]\n    public async Task ProcessOrder_ValidInput_Returns201AndPersistsToTable()\n    {\n        // Arrange\n        var mockContext = new Mock&lt;FunctionContext&gt;();\n        var request = MockHelpers.CreateHttpRequestData(new OrderRequest(\"ORD123\", \"test@me.com\", 100.0));\n        var function = new OrderFunction();\n\n        // Act\n        var result = await function.Run(request, mockContext.Object);\n\n        // Assert\n        Assert.Equal(HttpStatusCode.Created, result.HttpResponse.StatusCode);\n        Assert.NotNull(result.NewOrder);\n        Assert.Null(result.FailedOrder);\n    }\n\n    [Fact]\n    public async Task ProcessOrder_InvalidAmount_Returns400AndQueuesError()\n    {\n        // Arrange\n        var mockContext = new Mock&lt;FunctionContext&gt;();\n        var request = MockHelpers.CreateHttpRequestData(new OrderRequest(\"BAD1\", \"err@me.com\", -10.0));\n        var function = new OrderFunction();\n\n        // Act\n        var result = await function.Run(request, mockContext.Object);\n\n        // Assert\n        Assert.Equal(HttpStatusCode.BadRequest, result.HttpResponse.StatusCode);\n        Assert.Null(result.NewOrder);\n        Assert.NotNull(result.FailedOrder);\n    }\n}\n\n\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Options;\nusing Moq;\nusing System.Text;\nusing System.Text.Json;\n\npublic static class MockHelpers\n{\n    public static HttpRequestData CreateHttpRequestData&lt;T&gt;(T body)\n    {\n        var json = JsonSerializer.Serialize(body);\n        var byteArray = Encoding.UTF8.GetBytes(json);\n        var memoryStream = new MemoryStream(byteArray);\n        memoryStream.Position = 0;\n\n        // 1. Setup Service Provider for Serializer\n        var serviceCollection = new ServiceCollection();\n        serviceCollection.AddSingleton(Options.Create(new WorkerOptions()));\n        var serviceProvider = serviceCollection.BuildServiceProvider();\n\n        // 2. Mock FunctionContext to provide InstanceServices\n        var mockContext = new Mock&lt;FunctionContext&gt;();\n        mockContext.Setup(c =&gt; c.InstanceServices).Returns(serviceProvider);\n\n        // 3. Mock HttpRequestData\n        var mockRequest = new Mock&lt;HttpRequestData&gt;(mockContext.Object);\n        mockRequest.Setup(r =&gt; r.Body).Returns(memoryStream);\n        \n        // Mock the CreateResponse method to return a valid HttpResponseData\n        mockRequest.Setup(r =&gt; r.CreateResponse()).Returns(() =&gt; \n        {\n            var response = new Mock&lt;HttpResponseData&gt;(mockContext.Object);\n            response.SetupProperty(r =&gt; r.StatusCode);\n            response.SetupProperty(r =&gt; r.Headers, new HttpHeadersCollection());\n            response.SetupProperty(r =&gt; r.Body, new MemoryStream());\n            return response.Object;\n        });\n\n        return mockRequest.Object;\n    }\n}\n\n\n\n\nname: Build and Deploy Order Processor\n\non:\n  push:\n    branches: [ \"main\" ]\n\npermissions:\n  id-token: write # Required for OIDC authentication\n  contents: read\n\nenv:\n  AZURE_FUNCTIONAPP_NAME: 'your-function-app-name'\n  DOTNET_VERSION: '8.0.x'\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: 'Checkout GitHub Action'\n        uses: actions/checkout@v4\n\n      - name: 'Setup DotNet ${{ env.DOTNET_VERSION }}'\n        uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n\n      - name: 'Restore and Build'\n        run: dotnet build --configuration Release\n\n      - name: 'Run Unit Tests'\n        run: dotnet test --no-build --configuration Release --verbosity normal\n\n      - name: 'Publish'\n        run: dotnet publish -c Release -o ./publish\n\n      - name: 'Login to Azure via OIDC'\n        uses: azure/login@v2\n        with:\n          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n\n      - name: 'Deploy to Azure Functions'\n        uses: Azure/functions-action@v1\n        with:\n          app-name: ${{ env.AZURE_FUNCTIONAPP_NAME }}\n          package: './publish'\n\n\n\nparam location string = resourceGroup().location\nparam appName string = 'order-proc-${uniqueString(resourceGroup().id)}'\nparam storageName string = 'st${uniqueString(resourceGroup().id)}'\n\n// 1. Storage Account (for Tables, Queues, and Function Host)\nresource storageAccount 'Microsoft.Storage/storageAccounts@2023-01-01' = {\n  name: storageName\n  location: location\n  sku: { name: 'Standard_LRS' }\n  kind: 'StorageV2'\n}\n\n// 2. Hosting Plan\nresource hostingPlan 'Microsoft.Web/serverfarms@2022-09-01' = {\n  name: '${appName}-plan'\n  location: location\n  sku: { name: 'Y1', tier: 'Dynamic' } // Consumption Plan\n}\n\n// 3. Function App with System-Assigned Identity\nresource functionApp 'Microsoft.Web/sites@2022-09-01' = {\n  name: appName\n  location: location\n  kind: 'functionapp'\n  identity: { type: 'SystemAssigned' }\n  properties: {\n    serverFarmId: hostingPlan.id\n    siteConfig: {\n      netFrameworkVersion: 'v8.0'\n      appSettings: [\n        { name: 'AzureWebJobsStorage__accountName', value: storageAccount.name }\n        { name: 'FUNCTIONS_EXTENSION_VERSION', value: '~4' }\n        { name: 'FUNCTIONS_WORKER_RUNTIME', value: 'dotnet-isolated' }\n        // Identity-based Connection Mappings\n        { name: 'OrdersTableService__tableServiceUri', value: storageAccount.properties.primaryEndpoints.table }\n        { name: 'OrderQueueService__queueServiceUri', value: storageAccount.properties.primaryEndpoints.queue }\n      ]\n    }\n  }\n}\n\n// 4. RBAC Roles (Least Privilege)\nvar storageTableDataContributor = '0a9a4e1f-409e-4856-9975-1546272b248a'\nvar storageQueueDataMessageSender = 'c6a89b2d-59bc-44d0-9896-0f6e12d7b80a'\n\nresource tableRole 'Microsoft.Authorization/roleAssignments@2022-04-01' = {\n  name: guid(storageAccount.id, 'TableContributor', functionApp.id)\n  scope: storageAccount\n  properties: {\n    roleDefinitionId: subscriptionResourceId('Microsoft.Authorization/roleDefinitions', storageTableDataContributor)\n    principalId: functionApp.identity.principalId\n    principalType: 'ServicePrincipal'\n  }\n}\n\nresource queueRole 'Microsoft.Authorization/roleAssignments@2022-04-01' = {\n  name: guid(storageAccount.id, 'QueueSender', functionApp.id)\n  scope: storageAccount\n  properties: {\n    roleDefinitionId: subscriptionResourceId('Microsoft.Authorization/roleDefinitions', storageQueueDataMessageSender)\n    principalId: functionApp.identity.principalId\n    principalType: 'ServicePrincipal'\n  }\n}",
    "crumbs": [
      "Now!",
      "CARE Example"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#example-order-processor-microservice-requirement",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#example-order-processor-microservice-requirement",
    "title": "CARE Framework Example",
    "section": "",
    "text": "Section\nAI-Ready Specification\n\n\n\n\nContext\nCreate a .NET 8 Isolated Azure Function for order processing. It must support a Dead Letter Pattern to capture invalid orders for manual recovery.\n\n\nAction\n1. Input: HTTP POST JSON (OrderId, CustomerEmail, TotalAmount).\n\n\n\n2. Happy Path: Valid orders must be persisted to Azure Table Storage (OrdersTable).\n\n\n\n3. Failure Path: If validation fails (e.g., negative TotalAmount), the function must:\n\n\n\n- Return 400 Bad Request.\n\n\n\n- Persist the invalid payload to an Azure Storage Queue named order-error-poison.\n\n\nResult\n‚Ä¢ Use a Multi-Output Binding class (OrderResponse) to route data to Table and Queue storage.\n\n\n\n‚Ä¢ Implement Identity-based connections (__tableServiceUri and __queueServiceUri).\n\n\n\n‚Ä¢ Include structured logging for both successful and ‚Äúdead-lettered‚Äù outcomes.\n\n\nEvaluation Acceptance Criteria:\n1. No hardcoded connection strings.\n\n\n\n2. 201 Created only if written to Table Storage.\n\n\n\n3. Verify that the FailedOrder property is null during successful runs to avoid duplicate queue entries\n\n\n\n4. Unit test stubs using xUnit and Moq for both success and failure scenarios.\n\n\n\n.N.B. During the generation process, the AI will surgest new features or improvements. These should be carefully evaluated to ensure they align with the original requirement and do not introduce unnecessary complexity. Once you are happy the requirement should be fully updated to reflect the desired outcome, you can proceed to generate the final code.",
    "crumbs": [
      "Now!",
      "CARE Example"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#execute-requirements",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#execute-requirements",
    "title": "CARE Framework Example",
    "section": "",
    "text": "Fed the requirement into an AI coding assistant (e.g., GPT-4, Claude Code) and received the following code output:\n\n\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\n\npublic class OrderResponse\n{\n    // Routes to Table Storage if not null\n    [TableOutput(\"OrdersTable\", Connection = \"OrdersTableService\")]\n    public OrderEntity? NewOrder { get; set; }\n\n    // Routes to Queue Storage if not null (Poison Queue)\n    [QueueOutput(\"order-error-poison\", Connection = \"OrderQueueService\")]\n    public OrderRequest? FailedOrder { get; set; }\n\n    // Required for the HTTP Trigger's return value\n    public HttpResponseData HttpResponse { get; set; } = null!;\n}\n\npublic record OrderRequest(string OrderId, string CustomerEmail, double TotalAmount);\n\npublic class OrderEntity\n{\n    public string PartitionKey { get; set; } = \"Orders\";\n    public string RowKey { get; set; } = null!;\n    public string CustomerEmail { get; set; } = null!;\n    public double TotalAmount { get; set; }\n}\n\n\n\n\nusing System.Net;\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\n\npublic class OrderProcessor\n{\n    private readonly ILogger _logger;\n\n    public OrderProcessor(ILoggerFactory loggerFactory)\n    {\n        _logger = loggerFactory.CreateLogger&lt;OrderProcessor&gt;();\n    }\n\n    [Function(\"ProcessOrder\")]\n    public async Task&lt;OrderResponse&gt; Run(\n        [HttpTrigger(AuthorizationLevel.Function, \"post\")] HttpRequestData req,\n        FunctionContext executionContext)\n    {\n        var _logger = executionContext.GetLogger(\"ProcessOrder\");\n        var order = await req.ReadFromJsonAsync&lt;OrderRequest&gt;();\n\n        // Validation logic (Failure Path)\n        if (order == null || order.TotalAmount &lt; 0)\n        {\n            _logger.LogWarning(\"Validation failed for OrderId: {OrderId}\", order?.OrderId ?? \"Unknown\");\n            \n            var badResponse = req.CreateResponse(System.Net.HttpStatusCode.BadRequest);\n            return new OrderResponse\n            {\n                HttpResponse = badResponse,\n                FailedOrder = order, // Sent to 'order-error-poison' queue\n                NewOrder = null      // Ensures no Table Storage entry\n            };\n        }\n\n        // Success Path\n        _logger.LogInformation(\"Processing successful order: {OrderId}\", order.OrderId);\n        \n        var okResponse = req.CreateResponse(System.Net.HttpStatusCode.Created);\n        return new OrderResponse\n        {\n            HttpResponse = okResponse,\n            NewOrder = new OrderEntity \n            { \n                RowKey = order.OrderId, \n                CustomerEmail = order.CustomerEmail, \n                TotalAmount = order.TotalAmount \n            },\n            FailedOrder = null // Ensures no Poison Queue entry\n        };\n    }\n\n}\n\n\n\n{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet-isolated\",\n    \"OrdersTableService__tableServiceUri\": \"https://&lt;account&gt;.table.core.windows.net/\",\n    \"OrderQueueService__queueServiceUri\": \"https://&lt;account&gt;.queue.core.windows.net/\" \n\n  }\n}",
    "crumbs": [
      "Now!",
      "CARE Example"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#unit-test-stub-orderprocessortests",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#unit-test-stub-orderprocessortests",
    "title": "CARE Framework Example",
    "section": "",
    "text": "using Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\nusing Moq;\nusing System.Net;\nusing System.Text;\nusing Xunit;\n\npublic class OrderFunctionTests\n{\n    [Fact]\n    public async Task ProcessOrder_ValidInput_Returns201AndPersistsToTable()\n    {\n        // Arrange\n        var mockContext = new Mock&lt;FunctionContext&gt;();\n        var request = MockHelpers.CreateHttpRequestData(new OrderRequest(\"ORD123\", \"test@me.com\", 100.0));\n        var function = new OrderFunction();\n\n        // Act\n        var result = await function.Run(request, mockContext.Object);\n\n        // Assert\n        Assert.Equal(HttpStatusCode.Created, result.HttpResponse.StatusCode);\n        Assert.NotNull(result.NewOrder);\n        Assert.Null(result.FailedOrder);\n    }\n\n    [Fact]\n    public async Task ProcessOrder_InvalidAmount_Returns400AndQueuesError()\n    {\n        // Arrange\n        var mockContext = new Mock&lt;FunctionContext&gt;();\n        var request = MockHelpers.CreateHttpRequestData(new OrderRequest(\"BAD1\", \"err@me.com\", -10.0));\n        var function = new OrderFunction();\n\n        // Act\n        var result = await function.Run(request, mockContext.Object);\n\n        // Assert\n        Assert.Equal(HttpStatusCode.BadRequest, result.HttpResponse.StatusCode);\n        Assert.Null(result.NewOrder);\n        Assert.NotNull(result.FailedOrder);\n    }\n}\n\n\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Options;\nusing Moq;\nusing System.Text;\nusing System.Text.Json;\n\npublic static class MockHelpers\n{\n    public static HttpRequestData CreateHttpRequestData&lt;T&gt;(T body)\n    {\n        var json = JsonSerializer.Serialize(body);\n        var byteArray = Encoding.UTF8.GetBytes(json);\n        var memoryStream = new MemoryStream(byteArray);\n        memoryStream.Position = 0;\n\n        // 1. Setup Service Provider for Serializer\n        var serviceCollection = new ServiceCollection();\n        serviceCollection.AddSingleton(Options.Create(new WorkerOptions()));\n        var serviceProvider = serviceCollection.BuildServiceProvider();\n\n        // 2. Mock FunctionContext to provide InstanceServices\n        var mockContext = new Mock&lt;FunctionContext&gt;();\n        mockContext.Setup(c =&gt; c.InstanceServices).Returns(serviceProvider);\n\n        // 3. Mock HttpRequestData\n        var mockRequest = new Mock&lt;HttpRequestData&gt;(mockContext.Object);\n        mockRequest.Setup(r =&gt; r.Body).Returns(memoryStream);\n        \n        // Mock the CreateResponse method to return a valid HttpResponseData\n        mockRequest.Setup(r =&gt; r.CreateResponse()).Returns(() =&gt; \n        {\n            var response = new Mock&lt;HttpResponseData&gt;(mockContext.Object);\n            response.SetupProperty(r =&gt; r.StatusCode);\n            response.SetupProperty(r =&gt; r.Headers, new HttpHeadersCollection());\n            response.SetupProperty(r =&gt; r.Body, new MemoryStream());\n            return response.Object;\n        });\n\n        return mockRequest.Object;\n    }\n}",
    "crumbs": [
      "Now!",
      "CARE Example"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#github-action-yaml-for-cicd-deployment",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#github-action-yaml-for-cicd-deployment",
    "title": "CARE Framework Example",
    "section": "",
    "text": "name: Build and Deploy Order Processor\n\non:\n  push:\n    branches: [ \"main\" ]\n\npermissions:\n  id-token: write # Required for OIDC authentication\n  contents: read\n\nenv:\n  AZURE_FUNCTIONAPP_NAME: 'your-function-app-name'\n  DOTNET_VERSION: '8.0.x'\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: 'Checkout GitHub Action'\n        uses: actions/checkout@v4\n\n      - name: 'Setup DotNet ${{ env.DOTNET_VERSION }}'\n        uses: actions/setup-dotnet@v4\n        with:\n          dotnet-version: ${{ env.DOTNET_VERSION }}\n\n      - name: 'Restore and Build'\n        run: dotnet build --configuration Release\n\n      - name: 'Run Unit Tests'\n        run: dotnet test --no-build --configuration Release --verbosity normal\n\n      - name: 'Publish'\n        run: dotnet publish -c Release -o ./publish\n\n      - name: 'Login to Azure via OIDC'\n        uses: azure/login@v2\n        with:\n          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n\n      - name: 'Deploy to Azure Functions'\n        uses: Azure/functions-action@v1\n        with:\n          app-name: ${{ env.AZURE_FUNCTIONAPP_NAME }}\n          package: './publish'",
    "crumbs": [
      "Now!",
      "CARE Example"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#bicep-template-for-infrastructure",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CAREexample.html#bicep-template-for-infrastructure",
    "title": "CARE Framework Example",
    "section": "",
    "text": "param location string = resourceGroup().location\nparam appName string = 'order-proc-${uniqueString(resourceGroup().id)}'\nparam storageName string = 'st${uniqueString(resourceGroup().id)}'\n\n// 1. Storage Account (for Tables, Queues, and Function Host)\nresource storageAccount 'Microsoft.Storage/storageAccounts@2023-01-01' = {\n  name: storageName\n  location: location\n  sku: { name: 'Standard_LRS' }\n  kind: 'StorageV2'\n}\n\n// 2. Hosting Plan\nresource hostingPlan 'Microsoft.Web/serverfarms@2022-09-01' = {\n  name: '${appName}-plan'\n  location: location\n  sku: { name: 'Y1', tier: 'Dynamic' } // Consumption Plan\n}\n\n// 3. Function App with System-Assigned Identity\nresource functionApp 'Microsoft.Web/sites@2022-09-01' = {\n  name: appName\n  location: location\n  kind: 'functionapp'\n  identity: { type: 'SystemAssigned' }\n  properties: {\n    serverFarmId: hostingPlan.id\n    siteConfig: {\n      netFrameworkVersion: 'v8.0'\n      appSettings: [\n        { name: 'AzureWebJobsStorage__accountName', value: storageAccount.name }\n        { name: 'FUNCTIONS_EXTENSION_VERSION', value: '~4' }\n        { name: 'FUNCTIONS_WORKER_RUNTIME', value: 'dotnet-isolated' }\n        // Identity-based Connection Mappings\n        { name: 'OrdersTableService__tableServiceUri', value: storageAccount.properties.primaryEndpoints.table }\n        { name: 'OrderQueueService__queueServiceUri', value: storageAccount.properties.primaryEndpoints.queue }\n      ]\n    }\n  }\n}\n\n// 4. RBAC Roles (Least Privilege)\nvar storageTableDataContributor = '0a9a4e1f-409e-4856-9975-1546272b248a'\nvar storageQueueDataMessageSender = 'c6a89b2d-59bc-44d0-9896-0f6e12d7b80a'\n\nresource tableRole 'Microsoft.Authorization/roleAssignments@2022-04-01' = {\n  name: guid(storageAccount.id, 'TableContributor', functionApp.id)\n  scope: storageAccount\n  properties: {\n    roleDefinitionId: subscriptionResourceId('Microsoft.Authorization/roleDefinitions', storageTableDataContributor)\n    principalId: functionApp.identity.principalId\n    principalType: 'ServicePrincipal'\n  }\n}\n\nresource queueRole 'Microsoft.Authorization/roleAssignments@2022-04-01' = {\n  name: guid(storageAccount.id, 'QueueSender', functionApp.id)\n  scope: storageAccount\n  properties: {\n    roleDefinitionId: subscriptionResourceId('Microsoft.Authorization/roleDefinitions', storageQueueDataMessageSender)\n    principalId: functionApp.identity.principalId\n    principalType: 'ServicePrincipal'\n  }\n}",
    "crumbs": [
      "Now!",
      "CARE Example"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AdaptiveCard/index.html",
    "href": "Content/Journal/posts/AdaptiveCard/index.html",
    "title": "Adaptive Cards",
    "section": "",
    "text": "Adaptive Cards is a Json structure which describes how to display information in various Microsoft Applications. Currently the standard is at version 1.2, and the definition can be seen here https://AdaptiveCards.io"
  },
  {
    "objectID": "Content/Journal/posts/AdaptiveCard/index.html#result",
    "href": "Content/Journal/posts/AdaptiveCard/index.html#result",
    "title": "Adaptive Cards",
    "section": "Result",
    "text": "Result\n\n\n\nTeams-AdaptiveCard"
  },
  {
    "objectID": "Content/Journal/posts/AdaptiveCard/index.html#json",
    "href": "Content/Journal/posts/AdaptiveCard/index.html#json",
    "title": "Adaptive Cards",
    "section": "Json",
    "text": "Json\n{\n    \"type\": \"message\",\n    \"attachments\": [\n        {\n            \"contentType\": \"application/vnd.microsoft.card.adaptive\",\n            \"contentUrl\": null,\n            \"content\": {\n                \"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\",\n                \"type\": \"AdaptiveCard\",\n                \"version\": \"1.2\",\n                \"body\": [\n                    {\n                        \"type\": \"TextBlock\",\n                        \"size\": \"Medium\",\n                        \"weight\": \"Bolder\",\n                        \"text\": \"Image WaterMark Exception\"\n                    },\n                    {\n                        \"type\": \"ColumnSet\",\n                        \"columns\": [\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"Image\",\n                                        \"style\": \"Default\",\n                                        \"url\": \"https://www.knightfrank.com/library/v3.0/images/knightfranklogo.png\",\n                                        \"size\": \"Large\"\n                                    }\n                                ],\n                                \"width\": \"auto\"\n                            },\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"TextBlock\",\n                                        \"weight\": \"Bolder\",\n                                        \"text\": \"WaterMark Validator\",\n                                        \"wrap\": true\n                                    },\n                                    {\n                                        \"type\": \"TextBlock\",\n                                        \"spacing\": \"None\",\n                                        \"text\": \"Created {{DATE(2017-02-14T06:08:39Z,SHORT)}}\",\n                                        \"isSubtle\": true,\n                                        \"wrap\": true\n                                    }\n                                ],\n                                \"width\": \"stretch\"\n                            }\n                        ]\n                    },\n                    {\n                        \"type\": \"ColumnSet\",\n                        \"columns\": [\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"Image\",\n                                        \"style\": \"Default\",\n                                        \"url\": \"https://content.knightfrank.com/property/cbm190150/images/b2bee1c5-3dfa-4bbd-9d13-94cde2044822-0.jpg?cio=true&w=730\",\n                                        \"size\": \"Large\"\n                                    }\n                                ],\n                                \"width\": \"auto\"\n                            },\n                            {\n                                \"type\": \"Column\",\n                                \"items\": [\n                                    {\n                                        \"type\": \"TextBlock\",\n                                        \"text\": \"A Watermark has been detected in this image\",\n                                        \"wrap\": true\n                                    }\n                                ],\n                                \"width\": \"stretch\"\n                            }\n                        ]\n                    }\n                ],\n                \"actions\": [\n                    {\n                        \"type\": \"Action.OpenUrl\",\n                        \"title\": \"View\",\n                        \"url\": \"https://hub.knightfrank.com/#/app/activity/view/60c1d8fb-1bd7-ea11-a95a-000d3ab2efee?tabname=Marketing\"\n\n                    }\n                ]\n            }\n        }\n    ]\n}"
  },
  {
    "objectID": "Content/Journal/index.html",
    "href": "Content/Journal/index.html",
    "title": "Journal",
    "section": "",
    "text": "February 2, 2026\n        \n        \n            AI driven code developement\n\n            \n            \n                \n                \n                    AI Driven Code Developement\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            January 21, 2026\n        \n        \n            Diagram first coding\n\n            \n            \n                \n                \n                    Diagram first coding\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            November 10, 2025\n        \n        \n            Chapter2\n\n            \n            \n                \n                \n                    Quarto WebBook\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            October 20, 2025\n        \n        \n            Plantuml Quarto Filter\n\n            \n            \n                \n                \n                    Plantuml\n                \n                \n                \n                    Quarto Filter\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            October 16, 2025\n        \n        \n            API First\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n                \n                    API First\n                \n                \n                \n                    TypeSpec\n                \n                \n                \n                    AutoRest\n                \n                \n                \n                    Quarto\n                \n                \n                \n                    LUA\n                \n                \n                \n                    PowerShell\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            May 1, 2025\n        \n        \n            Integration Testing Azure Functions\n\n            \n            \n                \n                \n                    Azure Functions\n                \n                \n                \n                    Testing\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 1, 2025\n        \n        \n            Frontify Marketing Platform\n\n            \n            \n                \n                \n                    Frontify\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 20, 2024\n        \n        \n            Data Annotation Vs FluentValidation\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Rules\n                \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 14, 2024\n        \n        \n            SOLID\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n                \n                    C#\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            September 27, 2023\n        \n        \n            Visual Studio Solution Template\n\n            \n            \n                \n                \n                    C#\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 5, 2023\n        \n        \n            Cast To a Reflected Type\n\n            \n            \n                \n                \n                    C#\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 22, 2023\n        \n        \n            Web UI\n\n            \n            \n                \n                \n                    Architecture\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 1, 2023\n        \n        \n            How to build Cloud components without any Azure Experience\n\n            \n            \n                \n                \n                    Architecture\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 20, 2023\n        \n        \n            Event Notificiation\n\n            \n            \n                \n                \n                    Event Grid\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 20, 2023\n        \n        \n            SQL Change Tracking\n\n            \n            \n                \n                \n                    SQL\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 19, 2023\n        \n        \n            Web Upload File AV Scanning\n\n            \n            \n                \n                \n                    Identity\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 10, 2022\n        \n        \n            Service Application Development\n\n            \n            \n                \n                \n                    Identity\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 21, 2022\n        \n        \n            User Assigned Managed Identities\n\n            \n            \n                \n                \n                    Identity\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 14, 2020\n        \n        \n            Adaptive Cards\n\n            \n            \n                \n                \n                    Json\n                \n                \n                \n                    Adaptive Card\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 20, 2020\n        \n        \n            Azure Pipeline Git Commit\n\n            \n            \n                \n                \n                    DevOps\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 19, 2020\n        \n        \n            Azure Functions Logging\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 17, 2020\n        \n        \n            Function Monkey\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 9, 2020\n        \n        \n            Rules\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Rules\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 1, 2019\n        \n        \n            Online Spreadsheet Database\n\n            \n            \n                \n                \n                    SQL\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            October 16, 2019\n        \n        \n            gRPC\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 8, 2019\n        \n        \n            EIP Content Based Routing\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 8, 2019\n        \n        \n            EIP Loqate\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 31, 2019\n        \n        \n            SAS Storage Account Access\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Account\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 25, 2019\n        \n        \n            Documentation\n\n            \n            \n                \n                \n                    Markdown\n                \n                \n                \n                    Documentation\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 16, 2019\n        \n        \n            Azure Functions Dependency Injection\n\n            \n            \n                \n                \n                    Azure Functions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 16, 2019\n        \n        \n            Fluent Assertions\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Fluent Assertions\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 16, 2019\n        \n        \n            IFrames\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Iframes\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 22, 2019\n        \n        \n            GOF Mediator Pattern\n\n            \n            \n                \n                \n                    Patterns\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 28, 2019\n        \n        \n            Getting Secrets from KeyVault in Azure Functions\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Key Vault\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 4, 2019\n        \n        \n            Storage Account Encryption\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Accounts\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 15, 2019\n        \n        \n            Storage Accounts\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Storage Accounts\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 12, 2019\n        \n        \n            Certificate pfx to Base64\n\n            \n            \n                \n                \n                    Secuirty\n                \n                \n                \n                    Base64\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 11, 2019\n        \n        \n            Azure/C4\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    C4\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 11, 2019\n        \n        \n            KeyVault\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Key Vault\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 11, 2019\n        \n        \n            WOPI\n\n            \n            \n                \n                \n                    Office\n                \n                \n                \n                    WOPI\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 8, 2019\n        \n        \n            PlantUml\n\n            \n            \n                \n                \n                    Plantuml\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            October 15, 2018\n        \n        \n            ARM Template Parameter Variable Setup\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Arm Templates\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            October 15, 2018\n        \n        \n            Azure Function Managed Service Identities\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Security\n                \n                \n                \n                    Managed Service Identities\n                \n                \n            \n            \n\n            Azure Function Managed Service Identities\n            \n        \n        \n    \n    \n    \n                  \n            August 14, 2018\n        \n        \n            Azure Application Hosting\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Hosting\n                \n                \n            \n            \n\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 14, 2018\n        \n        \n            Azure Static Websites\n\n            \n            \n                \n                \n                    Azure\n                \n                \n                \n                    Hosting\n                \n                \n                \n                    Static Websites\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "Content/Current/posts/2025-12-10/index.html",
    "href": "Content/Current/posts/2025-12-10/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-12-10\n\nAdded events to Calendar\nStill need to add on click handler."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html",
    "href": "Content/Current/posts/2025-11-24/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "About to start a project to evaluate the different documentation systems that are available for software architecture. The goal is to find the best fit for our team‚Äôs needs.\nTto alow architects to write their documenation using their own favorite tools. Whether it‚Äôs markdown, reStructuredText, or even plain text files.\nNeed to all use the same git storage system to keep everything organized and version controlled.\nWill need to support different diagramming tools as well. Some architects prefer using PlantUML, mermaid, structurizr, or even visio.\nWill need to create templates and guidelines to ensure consistency across all documentation.\nPricing\nDocumentation tools\n\nGitHub Pages\nAzure Devops Wiki\nDocusaurus\nGitBook\nDocument360\nRead The Docs\nQuarto\nConfluense\nNotion\nMkDocs\n\n\n\n\nThese tools generate a website from plain-text files stored in your GitHub repository. The documentation lives alongside your code, and the entire software development workflow (pull requests, versioning, code reviews) is used for the documentation as well.\n\nMkDocs: A fast and simple static site generator written in Python, geared towards technical documentation. It uses Markdown source files and has a variety of themes and plugins available.\nDocusaurus: An open-source static site generator built on React that is popular for documentation sites. It is easy to use and provides a modern look and feel with built-in search and versioning features.\nDocFX: A static site generator for building and publishing API documentation for .NET projects, but can also be used for general Markdown documentation. It‚Äôs developed by Microsoft and is highly customizable.\nJekyll: One of the original static site generators, powered by Ruby. It is the engine behind GitHub Pages, making it a seamless option for hosting your documentation directly from your GitHub repo for free.\n\n\n\n\nArchitecture documentation often requires diagrams. These tools allow you to define diagrams using text-based syntax, which can be versioned in Git like the rest of your documentation.\n\nPlantUML and Mermaid: These tools render simple text-based descriptions into diagrams (UML, C4 model, flowcharts, etc.). GitHub natively supports rendering Mermaid diagrams inline within Markdown files, which is a significant advantage.\nStructurizr: This tool is based on the C4 model for visualizing software architecture. It allows you to create diagrams and documentation from a single model using code or a Domain Specific Language (DSL), perfect for technical people who want to manage architecture rigorously in source control.\nDraw.io (now diagrams.net): While it has a traditional UI editor, it also offers a C4 model plugin and can store diagram definitions in a version-control-friendly format (like XML or text) within your GitHub repo.\n\n\n\n\nIf you prefer a hosted solution with a more traditional editing interface but still want your source content in GitHub, some SaaS products offer synchronization.\n\nGitBook: A modern platform for creating and managing documentation. It can connect to your GitHub repository and automatically sync content, providing a polished user interface while keeping the source in Git.\nNotion or Confluence (with integration): While their primary storage isn‚Äôt Git, they can be integrated or used in conjunction with a docs-as-code approach to serve different audiences, with the source of truth remaining in the GitHub repo.\n\nThe ‚Äúdocs-as-code‚Äù approach using a static site generator and plain text files is generally considered best practice for software architecture documentation because it aligns with engineering workflows and facilitates version control and collaboration within the existing toolset.\n\n\n\nQuarto is an open-source scientific and technical publishing system that allows you to create dynamic content using Markdown and integrate executable code (Python, R, Julia, etc.) into your documentation.\n\nGitHub Repo as Storage: Yes, this is Quarto‚Äôs native approach. Your documentation is authored in plain-text Markdown files (or .qmd files) and stored directly in your GitHub repository alongside your code.\nWorkflow: It fully embraces the ‚Äúdocs-as-code‚Äù philosophy. You use your standard development tools (VS Code, RStudio, command line) to edit files and the entire process (pull requests, code review, versioning) is managed via Git and GitHub.\nPublishing: Quarto can be published to a variety of destinations, including GitHub Pages, using the quarto publish command or automated with GitHub Actions.\nBest for: Highly technical teams who want documentation to be a seamless part of their existing engineering workflow and codebase, and require a high degree of customisation and the ability to embed code execution results.\n\n\n\n\nDocument360 is a full-featured, hosted knowledge base platform designed for creating, managing, and publishing both internal and external user documentation.\n\nGitHub Repo as Storage: No, not as its primary storage. Document360 stores your content in its own cloud-based database (MongoDB/Azure Blob Storage).\nWorkflow: It provides a user-friendly, web-based editor (which supports Markdown) and a more traditional content management system (CMS) experience with features like versioning, analytics, and a homepage builder.\nGitHub Integration: It offers an integration with GitHub that allows for a one-way sync of Markdown files from a GitHub repository into Document360. The synced articles are read-only within Document360‚Äôs editor; any changes must be made in the source GitHub repository and then synced back.\n\nBest for: Teams where technical writers and non-developers are primary content creators, who prefer a polished UI and dedicated documentation features (like analytics, detailed access control, etc.), but still want the content source to originate from a GitHub repo.\nGiven this list of documentation platforms.\n\nGitHub Pages\nAzure Devops Wiki\nDocusaurus\nGitBook\nDocument360\nRead The Docs\nQuarto\nConfluense\nNotion\nMkDocs\n\nwhich best meet our requirements of:\n\n20 active contributing users\n1000+ readonly users\nGithub backed document repository\nSupport for Markdown\nSupport for Plantuml, Mermaid, Stucturizr\n\nThe best options that meet all requirements, especially the GitHub-backed document repository, are the open-source static site generators such as Docusaurus, MkDocs, Quarto, and GitHub Pages. These platforms fully support the ‚Äúdocs-as-code‚Äù workflow required by a GitHub-backed repository. Here is a breakdown of why these are the top choices and how they meet your needs:\n\nGitHub-backed document repository: All these tools use plain Markdown files stored in your GitHub repo as the source of truth.\n20 active contributing users: The contribution model leverages standard Git workflows (pull requests, branching, code reviews), which scales well for active engineering teams.\n1000+ readonly users: Static site generators produce highly performant, scalable websites that can handle thousands of concurrent read-only users without performance issues.\nSupport for Markdown: Markdown is the native authoring format for all of them.\nSupport for PlantUML, Mermaid, Structurizr: Mermaid is natively supported by GitHub and many of these generators. PlantUML and Structurizr are also widely supported through extensions, plugins, or pre-processing during the build pipeline.\n\n\n\n\n\n\n\n\n\n\n\nPlatform\nNotes\n\n\n\n\nDocusaurus\nExcellent balance of modern design, ease of use, built-in features (search, versioning), and a React-based structure for customization.\n\n\nMkDocs\nSimple, fast, and highly effective for technical documentation, using Python. Easy to configure plugins for diagram support.\n\n\nQuarto\nIdeal for technical and scientific documentation, integrating executable code and advanced formatting capabilities.\n\n\nGitHub Pages\nThe underlying hosting platform for the others (often used with Jekyll or MkDocs), offering a seamless, free hosting solution directly from your repo.\n\n\n\n\n\n\nAzure DevOps Wiki: Its primary storage is not a GitHub repo (it uses an Azure repo for ‚Äúcode wiki‚Äù), and native support for all diagram types (especially PlantUML and Structurizr) is limited without workarounds.\nConfluence & Notion: These are traditional CMS/wiki platforms where the primary storage is their own database, not your GitHub repo. While integrations exist, they often rely on one-way syncs or embedding, which doesn‚Äôt fully align with the ‚ÄúGitHub-backed‚Äù requirement.\nDocument360 & GitBook: These are SaaS products. GitBook started with open-source roots but now operates as a hosted solution with sync capabilities. Document360 offers a one-way sync from GitHub, but the source of truth within the app is its own database.\nRead The Docs: Excellent for hosting documentation, particularly Sphinx projects, but often used for existing projects rather than a standalone platform choice like Docusaurus or MkDocs for greenfield projects. It still fits the requirements as a hosting solution for content from a GitHub repo.\n\n\n\n\n\nThe best fit version applications use the following versions of Markdown.\n\nDocusaurus uses MDX by default, which is a superset of GitHub Flavored Markdown (GFM) that allows you to embed JSX (React components) within your Markdown content. It can be configured to use a stricter CommonMark format if needed.\nMkDocs uses the Python-Markdown library, which is nearly completely compliant with the original Markdown reference implementation. It gains additional features, including support for diagrams and other advanced syntax, through extensions.\nQuarto is based on Pandoc Markdown, an extended and slightly revised version of John Gruber‚Äôs original Markdown syntax. Pandoc Markdown is a highly flexible and powerful superset that adds features necessary for technical and academic publishing, such as citations, cross-references, and definition lists.\nGitHub Pages uses GitHub Flavored Markdown (GFM) as its default, which is a formal specification based on the CommonMark standard. GFM is a strict superset of CommonMark, adding useful extensions like tables, strikethrough, task lists, and automatic linking of URLs and GitHub-specific references (issues, commits, users).\n\nIn summary, all four options are either based on the highly compatible CommonMark or GFM specifications, ensuring that your documentation is largely portable and readable across different platforms.\n\n\n\nRepresenting PlantUML and Structurizr diagrams in the recommended applications is generally achieved using plugins, extensions, or by generating images as part of your build pipeline. Since these tools use static site generation, the diagrams are typically converted to SVG or PNG images at build time rather than rendered dynamically in the browser.\n\n\nDocusaurus natively supports Mermaid, but PlantUML and Structurizr require community plugins or build-time generation.\n\nPlantUML: Use a plugin like remark-kroki or similar Docusaurus PlantUML plugins that convert plantuml code blocks to images during the build process, often using a public or private PlantUML server.\nStructurizr: A dedicated docusaurus-plugin-structurizr exists to generate diagrams from Structurizr DSL files (.dsl) and embed them into your markdown pages.\nMermaid: Docusaurus has native support for Mermaid via a simple configuration in docusaurus.config.js, making it a very seamless option.\n\n\n\n\nMkDocs has a rich ecosystem of plugins that handle diagram generation effectively.\n\nPlantUML: The mkdocs-plantuml-plugin is widely used. It requires a PlantUML server (which you can run locally via Docker) to render diagrams from puml code blocks in your markdown files during the build phase.\nStructurizr: You can use the Structurizr CLI to generate PlantUML files from your DSL, and then use the PlantUML plugin to render those. Alternatively, you can use the mkdocs-kroki-plugin which supports Structurizr DSL directly.\nMermaid: The Material for MkDocs theme has first-class support for Mermaid and other diagram libraries via its integration with the Kroki service.\n\n\n\n\nQuarto has native support for several diagram formats.\n\nPlantUML & Structurizr: Quarto does not have direct, native PlantUML rendering built-in like Mermaid. You would likely rely on pre-generating the image files (using a GitHub Action or local script with the PlantUML JAR/server) and then embedding them as standard Markdown images. Quarto‚Äôs general flexibility with Pandoc filters might allow for a more integrated approach, but this requires more configuration.\nMermaid & Graphviz: Quarto has native, out-of-the-box support for Mermaid and Graphviz diagrams embedded within special executable cells (e.g., ```{mermaid}).\n\n\n\n\nGitHub Pages uses Jekyll by default, and its capabilities are dependent on the plugins GitHub allows to run in its environment.\n\nMermaid: GitHub added native support for rendering Mermaid diagrams in Markdown files, which works seamlessly on GitHub Pages.\nPlantUML & Structurizr: GitHub Pages runs in ‚Äúsafe mode‚Äù, which restricts custom plugins. To use PlantUML or Structurizr with GitHub Pages, you must generate the SVG/PNG image files as part of your CI/CD pipeline (e.g., using GitHub Actions) and commit those images to your repository. You then reference the generated image files in your Markdown using standard image syntax . You cannot render them live within the Markdown source blocks on the GitHub Pages server itself."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#static-site-generators-docs-as-code",
    "href": "Content/Current/posts/2025-11-24/index.html#static-site-generators-docs-as-code",
    "title": "Daily Note",
    "section": "",
    "text": "These tools generate a website from plain-text files stored in your GitHub repository. The documentation lives alongside your code, and the entire software development workflow (pull requests, versioning, code reviews) is used for the documentation as well.\n\nMkDocs: A fast and simple static site generator written in Python, geared towards technical documentation. It uses Markdown source files and has a variety of themes and plugins available.\nDocusaurus: An open-source static site generator built on React that is popular for documentation sites. It is easy to use and provides a modern look and feel with built-in search and versioning features.\nDocFX: A static site generator for building and publishing API documentation for .NET projects, but can also be used for general Markdown documentation. It‚Äôs developed by Microsoft and is highly customizable.\nJekyll: One of the original static site generators, powered by Ruby. It is the engine behind GitHub Pages, making it a seamless option for hosting your documentation directly from your GitHub repo for free."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#diagramming-and-modeling-tools-diagrams-as-code",
    "href": "Content/Current/posts/2025-11-24/index.html#diagramming-and-modeling-tools-diagrams-as-code",
    "title": "Daily Note",
    "section": "",
    "text": "Architecture documentation often requires diagrams. These tools allow you to define diagrams using text-based syntax, which can be versioned in Git like the rest of your documentation.\n\nPlantUML and Mermaid: These tools render simple text-based descriptions into diagrams (UML, C4 model, flowcharts, etc.). GitHub natively supports rendering Mermaid diagrams inline within Markdown files, which is a significant advantage.\nStructurizr: This tool is based on the C4 model for visualizing software architecture. It allows you to create diagrams and documentation from a single model using code or a Domain Specific Language (DSL), perfect for technical people who want to manage architecture rigorously in source control.\nDraw.io (now diagrams.net): While it has a traditional UI editor, it also offers a C4 model plugin and can store diagram definitions in a version-control-friendly format (like XML or text) within your GitHub repo."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#integrated-solutions-saas-with-github-sync",
    "href": "Content/Current/posts/2025-11-24/index.html#integrated-solutions-saas-with-github-sync",
    "title": "Daily Note",
    "section": "",
    "text": "If you prefer a hosted solution with a more traditional editing interface but still want your source content in GitHub, some SaaS products offer synchronization.\n\nGitBook: A modern platform for creating and managing documentation. It can connect to your GitHub repository and automatically sync content, providing a polished user interface while keeping the source in Git.\nNotion or Confluence (with integration): While their primary storage isn‚Äôt Git, they can be integrated or used in conjunction with a docs-as-code approach to serve different audiences, with the source of truth remaining in the GitHub repo.\n\nThe ‚Äúdocs-as-code‚Äù approach using a static site generator and plain text files is generally considered best practice for software architecture documentation because it aligns with engineering workflows and facilitates version control and collaboration within the existing toolset."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#quarto",
    "href": "Content/Current/posts/2025-11-24/index.html#quarto",
    "title": "Daily Note",
    "section": "",
    "text": "Quarto is an open-source scientific and technical publishing system that allows you to create dynamic content using Markdown and integrate executable code (Python, R, Julia, etc.) into your documentation.\n\nGitHub Repo as Storage: Yes, this is Quarto‚Äôs native approach. Your documentation is authored in plain-text Markdown files (or .qmd files) and stored directly in your GitHub repository alongside your code.\nWorkflow: It fully embraces the ‚Äúdocs-as-code‚Äù philosophy. You use your standard development tools (VS Code, RStudio, command line) to edit files and the entire process (pull requests, code review, versioning) is managed via Git and GitHub.\nPublishing: Quarto can be published to a variety of destinations, including GitHub Pages, using the quarto publish command or automated with GitHub Actions.\nBest for: Highly technical teams who want documentation to be a seamless part of their existing engineering workflow and codebase, and require a high degree of customisation and the ability to embed code execution results."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#document360",
    "href": "Content/Current/posts/2025-11-24/index.html#document360",
    "title": "Daily Note",
    "section": "",
    "text": "Document360 is a full-featured, hosted knowledge base platform designed for creating, managing, and publishing both internal and external user documentation.\n\nGitHub Repo as Storage: No, not as its primary storage. Document360 stores your content in its own cloud-based database (MongoDB/Azure Blob Storage).\nWorkflow: It provides a user-friendly, web-based editor (which supports Markdown) and a more traditional content management system (CMS) experience with features like versioning, analytics, and a homepage builder.\nGitHub Integration: It offers an integration with GitHub that allows for a one-way sync of Markdown files from a GitHub repository into Document360. The synced articles are read-only within Document360‚Äôs editor; any changes must be made in the source GitHub repository and then synced back.\n\nBest for: Teams where technical writers and non-developers are primary content creators, who prefer a polished UI and dedicated documentation features (like analytics, detailed access control, etc.), but still want the content source to originate from a GitHub repo.\nGiven this list of documentation platforms.\n\nGitHub Pages\nAzure Devops Wiki\nDocusaurus\nGitBook\nDocument360\nRead The Docs\nQuarto\nConfluense\nNotion\nMkDocs\n\nwhich best meet our requirements of:\n\n20 active contributing users\n1000+ readonly users\nGithub backed document repository\nSupport for Markdown\nSupport for Plantuml, Mermaid, Stucturizr\n\nThe best options that meet all requirements, especially the GitHub-backed document repository, are the open-source static site generators such as Docusaurus, MkDocs, Quarto, and GitHub Pages. These platforms fully support the ‚Äúdocs-as-code‚Äù workflow required by a GitHub-backed repository. Here is a breakdown of why these are the top choices and how they meet your needs:\n\nGitHub-backed document repository: All these tools use plain Markdown files stored in your GitHub repo as the source of truth.\n20 active contributing users: The contribution model leverages standard Git workflows (pull requests, branching, code reviews), which scales well for active engineering teams.\n1000+ readonly users: Static site generators produce highly performant, scalable websites that can handle thousands of concurrent read-only users without performance issues.\nSupport for Markdown: Markdown is the native authoring format for all of them.\nSupport for PlantUML, Mermaid, Structurizr: Mermaid is natively supported by GitHub and many of these generators. PlantUML and Structurizr are also widely supported through extensions, plugins, or pre-processing during the build pipeline."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#recommended-platforms-best-fit",
    "href": "Content/Current/posts/2025-11-24/index.html#recommended-platforms-best-fit",
    "title": "Daily Note",
    "section": "",
    "text": "Platform\nNotes\n\n\n\n\nDocusaurus\nExcellent balance of modern design, ease of use, built-in features (search, versioning), and a React-based structure for customization.\n\n\nMkDocs\nSimple, fast, and highly effective for technical documentation, using Python. Easy to configure plugins for diagram support.\n\n\nQuarto\nIdeal for technical and scientific documentation, integrating executable code and advanced formatting capabilities.\n\n\nGitHub Pages\nThe underlying hosting platform for the others (often used with Jekyll or MkDocs), offering a seamless, free hosting solution directly from your repo.\n\n\n\n\n\n\nAzure DevOps Wiki: Its primary storage is not a GitHub repo (it uses an Azure repo for ‚Äúcode wiki‚Äù), and native support for all diagram types (especially PlantUML and Structurizr) is limited without workarounds.\nConfluence & Notion: These are traditional CMS/wiki platforms where the primary storage is their own database, not your GitHub repo. While integrations exist, they often rely on one-way syncs or embedding, which doesn‚Äôt fully align with the ‚ÄúGitHub-backed‚Äù requirement.\nDocument360 & GitBook: These are SaaS products. GitBook started with open-source roots but now operates as a hosted solution with sync capabilities. Document360 offers a one-way sync from GitHub, but the source of truth within the app is its own database.\nRead The Docs: Excellent for hosting documentation, particularly Sphinx projects, but often used for existing projects rather than a standalone platform choice like Docusaurus or MkDocs for greenfield projects. It still fits the requirements as a hosting solution for content from a GitHub repo."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#markdown-versions",
    "href": "Content/Current/posts/2025-11-24/index.html#markdown-versions",
    "title": "Daily Note",
    "section": "",
    "text": "The best fit version applications use the following versions of Markdown.\n\nDocusaurus uses MDX by default, which is a superset of GitHub Flavored Markdown (GFM) that allows you to embed JSX (React components) within your Markdown content. It can be configured to use a stricter CommonMark format if needed.\nMkDocs uses the Python-Markdown library, which is nearly completely compliant with the original Markdown reference implementation. It gains additional features, including support for diagrams and other advanced syntax, through extensions.\nQuarto is based on Pandoc Markdown, an extended and slightly revised version of John Gruber‚Äôs original Markdown syntax. Pandoc Markdown is a highly flexible and powerful superset that adds features necessary for technical and academic publishing, such as citations, cross-references, and definition lists.\nGitHub Pages uses GitHub Flavored Markdown (GFM) as its default, which is a formal specification based on the CommonMark standard. GFM is a strict superset of CommonMark, adding useful extensions like tables, strikethrough, task lists, and automatic linking of URLs and GitHub-specific references (issues, commits, users).\n\nIn summary, all four options are either based on the highly compatible CommonMark or GFM specifications, ensuring that your documentation is largely portable and readable across different platforms."
  },
  {
    "objectID": "Content/Current/posts/2025-11-24/index.html#diagramming-support",
    "href": "Content/Current/posts/2025-11-24/index.html#diagramming-support",
    "title": "Daily Note",
    "section": "",
    "text": "Representing PlantUML and Structurizr diagrams in the recommended applications is generally achieved using plugins, extensions, or by generating images as part of your build pipeline. Since these tools use static site generation, the diagrams are typically converted to SVG or PNG images at build time rather than rendered dynamically in the browser.\n\n\nDocusaurus natively supports Mermaid, but PlantUML and Structurizr require community plugins or build-time generation.\n\nPlantUML: Use a plugin like remark-kroki or similar Docusaurus PlantUML plugins that convert plantuml code blocks to images during the build process, often using a public or private PlantUML server.\nStructurizr: A dedicated docusaurus-plugin-structurizr exists to generate diagrams from Structurizr DSL files (.dsl) and embed them into your markdown pages.\nMermaid: Docusaurus has native support for Mermaid via a simple configuration in docusaurus.config.js, making it a very seamless option.\n\n\n\n\nMkDocs has a rich ecosystem of plugins that handle diagram generation effectively.\n\nPlantUML: The mkdocs-plantuml-plugin is widely used. It requires a PlantUML server (which you can run locally via Docker) to render diagrams from puml code blocks in your markdown files during the build phase.\nStructurizr: You can use the Structurizr CLI to generate PlantUML files from your DSL, and then use the PlantUML plugin to render those. Alternatively, you can use the mkdocs-kroki-plugin which supports Structurizr DSL directly.\nMermaid: The Material for MkDocs theme has first-class support for Mermaid and other diagram libraries via its integration with the Kroki service.\n\n\n\n\nQuarto has native support for several diagram formats.\n\nPlantUML & Structurizr: Quarto does not have direct, native PlantUML rendering built-in like Mermaid. You would likely rely on pre-generating the image files (using a GitHub Action or local script with the PlantUML JAR/server) and then embedding them as standard Markdown images. Quarto‚Äôs general flexibility with Pandoc filters might allow for a more integrated approach, but this requires more configuration.\nMermaid & Graphviz: Quarto has native, out-of-the-box support for Mermaid and Graphviz diagrams embedded within special executable cells (e.g., ```{mermaid}).\n\n\n\n\nGitHub Pages uses Jekyll by default, and its capabilities are dependent on the plugins GitHub allows to run in its environment.\n\nMermaid: GitHub added native support for rendering Mermaid diagrams in Markdown files, which works seamlessly on GitHub Pages.\nPlantUML & Structurizr: GitHub Pages runs in ‚Äúsafe mode‚Äù, which restricts custom plugins. To use PlantUML or Structurizr with GitHub Pages, you must generate the SVG/PNG image files as part of your CI/CD pipeline (e.g., using GitHub Actions) and commit those images to your repository. You then reference the generated image files in your Markdown using standard image syntax . You cannot render them live within the Markdown source blocks on the GitHub Pages server itself."
  },
  {
    "objectID": "Content/Current/posts/2025-11-11/index.html",
    "href": "Content/Current/posts/2025-11-11/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-11-11\n\nNimbus, reached out to nimbus, looking for somw online resources concerning there API\nJournal\n\nUpdating the Journal posts so they are in there own folder.\n\nIf the main page for each post is index.qmd, then the index screen can ignore all other qmd/md files."
  },
  {
    "objectID": "Content/Current/posts/2025-11-04/index.html",
    "href": "Content/Current/posts/2025-11-04/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-11-04\n\nQuarto Calendar Template is nearly done\n\nLoad a calendar with dates from blog posts\nIndicate on the calendar days when a posting has occured.\nClick on a date and a link is populated with the required page.\nA div on the page is populated with the blog post.\n\nStop Quarto formating the page with headers and footers\nSort out the page structure.\n\n\n\nAs said in a previous post the script is split in to two sections. The first scans the posts and extracts the date and url link and encodes them into a JSON object, which is then stored on the page.\n&lt;% const events = []; %&gt;\n&lt;%     items.forEach( item =&gt; { %&gt;\n&lt;%         if (item.date) { %&gt;\n&lt;%             const d = new Date(item.date); %&gt;\n&lt;%             const year = d.getFullYear(); %&gt;\n&lt;%             const month = d.getMonth() + 1; %&gt;\n&lt;%             const day = d.getDate(); %&gt;\n\n&lt;%             var date = month+\"-\"+day+\"-\"+year; %&gt;\n&lt;%             var link = item.path; %&gt;\n&lt;%             var event = { \"date\": date, \"link\": link } %&gt;\n\n&lt;%             events.push(event); %&gt;\n&lt;%         } %&gt;\n&lt;%     }); %&gt;\n&lt;% const serverData = { events: events }; %&gt;\nThe second piece of code is executed in the browser, loads a JsCalendar, and then proceeds to process the JSON object and load the calendar with events. The clicked date enables a link which links to the required blog post page.\n&lt;script src=\"https://unpkg.com/simple-jscalendar@1.4.5/source/jsCalendar.min.js\" integrity=\"sha384-F3Wc9EgweCL3C58eDn9902kdEH6bTDL9iW2JgwQxJYUIeudwhm4Wu9JhTkKJUtIJ\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/simple-jscalendar@1.4.5/source/jsCalendar.min.css\" integrity=\"sha384-CTBW6RKuDwU/TWFl2qLavDqLuZtBzcGxBXY8WvQ0lShXglO/DsUvGkXza+6QTxs0\" crossorigin=\"anonymous\"&gt;\n\n&lt;div id=\"calendar\"&gt;&lt;/div&gt;\n&lt;div id=\"data-container\" data-config=\"&lt;%- JSON.stringify(serverData) %&gt;\"&gt;&lt;/div&gt;\n\n&lt;b&gt;Day  : &lt;/b&gt;&lt;input id=\"my-input-a\"&gt;&lt;br&gt;\n&lt;b&gt;Link : &lt;/b&gt;&lt;a id=\"anchor\"&gt;link&lt;/a&gt;&lt;br&gt;\n&lt;div id=\"post\"&gt;POST&lt;/div&gt;\n\n&lt;script&gt;\n    document.addEventListener(\"DOMContentLoaded\", function() {\n        const container = document.getElementById(\"data-container\");\n        const jsonString = container.getAttribute(\"data-config\");\n        const data = JSON.parse(jsonString);\n        const calendarElement = document.getElementById(\"calendar\");\n        const calendar = new jsCalendar(calendarElement);\n\n        data.events.forEach( item =&gt; {\n            var d = new Date(item.date);\n            var year = d.getFullYear();\n            var month = d.getMonth() + 1; \n            var day = d.getDate();     \n\n            // hightlight event date\n            calendar.select(day+\"/\"+month+\"/\"+year);\n\n            var inputA = document.getElementById(\"my-input-a\");\n            var anchor = document.getElementById(\"anchor\");\n\n            // Add events\n            calendar.onDateClick(function(event, date){\n                const container = document.getElementById(\"data-container\");\n                const jsonString = container.getAttribute(\"data-config\");\n                const data = JSON.parse(jsonString);       \n                \n                data.events.forEach( event =&gt; {\n                    var d = new Date(event.date);\n\n                    if( d.getTime() === date.getTime()) {\n                        inputA.value = date.toString();\n                        anchor.href = event.link.replace(\"qmd\", \"html\");\n\n                        var post = document.getElementById(\"post\");\n                        post.innerHTML =  \"&lt;object type=\\\"text/html\\\" data=\\\"\"+link+\"\\\"&gt;&lt;/object&gt;\";\n\n                    }\n                })\n            });\n        });\n\n        // Refresh layout\n        calendar.refresh();\n    });\n&lt;/script&gt;"
  },
  {
    "objectID": "Content/Current/posts/2025-10-21/index.html",
    "href": "Content/Current/posts/2025-10-21/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-21\n\nDiabetes\n\nNHS Online Course https://app.changinghealth.com\nCalendar Template\n\nBeen trying to get a javascript calendar to show on a EJS page.\nCan show the JSCalendar\nCan itterate through the pages extracting the date elements\nISSUE: Seems to lie in getting them both to work together.\n\nSeems JSCalendar is being processed on the client side\nEJS is processed on the server side.\nThe Javascript calendar and loop appears in the processed HTML code on the client side, and so has nothing to process.\nEJS is prepocessed on the server side, and does not load the JS calendar library, so cannot reference it.\n\nMay have to include JS code functions inline.\n\n\nConclusion\n\nTo make this work you are going to need a server side generated calendar,\nWhich will only show the period from the earliest to the latest months activity.\n\n\n\n\n\n\n\noutput\n\n\n\n&lt;script src=\"https://unpkg.com/simple-jscalendar@1.4.5/source/jsCalendar.min.js\" integrity=\"sha384-F3Wc9EgweCL3C58eDn9902kdEH6bTDL9iW2JgwQxJYUIeudwhm4Wu9JhTkKJUtIJ\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/simple-jscalendar@1.4.5/source/jsCalendar.min.css\" integrity=\"sha384-CTBW6RKuDwU/TWFl2qLavDqLuZtBzcGxBXY8WvQ0lShXglO/DsUvGkXza+6QTxs0\" crossorigin=\"anonymous\"&gt;\n\n&lt;div id=\"calendar\"&gt;&lt;/div&gt;\n\n&lt;script type=\"text/javascript\"&gt;\n    var calendar = jsCalendar.new(\"#calendar\", \"now\", { \"monthFormat\": \"month YYYY\" });\n\n    for (const item of items) { \n        if (item.date) { \n            const d = new Date(item.date);\n            const year = d.getFullYear();\n            const month = d.getMonth() + 1;\n            const day = d.getDate();\n\n            calendar.setHighlightedDate(new Date(year, month, day));\n        }\n    }\n\n&lt;/script&gt;\n    &lt;% for (const item of items) { %&gt;\n        &lt;div class=\"listing-item\"&gt;\n\n        &lt;% if (item.date) { %&gt;\n            &lt;%\n                const d = new Date(item.date);\n                const year = d.getFullYear();\n                const month = d.getMonth() + 1; // getMonth() is 0-indexed\n                const day = d.getDate();\n            %&gt;\n\n            &lt;p&gt;&lt;%= day %&gt;-&lt;%= month %&gt;-&lt;%= year %&gt;&lt;/p&gt;\n        &lt;% } %&gt;\n\n        &lt;/div&gt;\n    &lt;% } %&gt;"
  },
  {
    "objectID": "Content/Current/posts/2025-10-16/index.html",
    "href": "Content/Current/posts/2025-10-16/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-16\n\nDiabetes\n\nContinued the Diabetes online course.\n\nAPI First Quarto Integration\n\nCreated a Journal page to document the process\nInvestigate the TypeSpec process\nInstalled the TypeSpec CLI\nCreated a sample TypeSpec file\nGenerated the OpenAPI file\nISSUE: No scripting options seem to be acailable to automate the process"
  },
  {
    "objectID": "Content/Current/posts/2025-10-14/index.html",
    "href": "Content/Current/posts/2025-10-14/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-14\n\nDiabetes\n\nContinued the Diabetes online course.\n\nQuarto PlantUML filter\n\nCreated a Quarto filter to process PlantUML diagrams within Quarto documents.\nPublished it as a Quarto extension.\nDocumented the filter as a journal entry.\nNot able to direct the output files to a subdirectory yet.\nNeed to find a way to echo the script rather than create a puml file.\nCreated a GitHub repo to hold the filter:"
  },
  {
    "objectID": "Content/Current/posts/2025-10-10/index.html",
    "href": "Content/Current/posts/2025-10-10/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-10\n\nQuarto / Plantuml plugin\n\nThe plugin currently doesnt support standard includes !include  etc this needs to be fixed :(\n\nStarted a Quarto / TypeSpec plugin with copilot\n\nRan out of copilot tokens\nAim was to create a fensed typespec element which would convert a typespec into a openapi/swagger html page\n\nLand Registy\n\nStill not heard anything from Al concerning Nimbus\n\nIf we go with Nimbus, which sounds likely as the cost is lower, we will have to rename the project.\nIf we change the title to a different name, we should investigate using multiple services, so we can support Scotland and Northern Ireland and any other country as they come along.\n\nMyKf\n\nInvestigate how MyKf will save the data in the hub db\nInvestigate what MyKf will save in the database\nTalk to Katy to find out who is the developer\n\nRams Solaiappan\n\n\n\nQuarto-Frontify\n\nCreated a Frontify Quarto Book\nPlantuml Diagrams\n\nDiagrams are loaded from files source\nAny raw plantuml diagram\nAny diagram that includes a !Include statement :(\n\nBook loaded into GitHub\nBook published to GitHub Pages\n\nThe Quarto Publish command produced errors :("
  },
  {
    "objectID": "Content/Current/posts/2025-10-08/index.html",
    "href": "Content/Current/posts/2025-10-08/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "Created Quarto Projects folder for project wikis\n\nNeed to look at how I can make the project wiki‚Äôs usable with quarto\nNeed to get plantuml working with r markdown\n\n\nCreated Quarto Current foler for daily notes\nTech Review, walked through LR sofar."
  },
  {
    "objectID": "Content/Books/index.html",
    "href": "Content/Books/index.html",
    "title": "Books",
    "section": "",
    "text": "October 16, 2025\n        \n        \n            Book1\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n                \n                    API First\n                \n                \n                \n                    TypeSpec\n                \n                \n                \n                    AutoRest\n                \n                \n                \n                    Quarto\n                \n                \n                \n                    LUA\n                \n                \n                \n                    PowerShell\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n    \n                  \n            October 16, 2025\n        \n        \n            Book2\n\n            \n            \n                \n                \n                    C#\n                \n                \n                \n                    Azure Functions\n                \n                \n                \n                    API First\n                \n                \n                \n                    TypeSpec\n                \n                \n                \n                    AutoRest\n                \n                \n                \n                    Quarto\n                \n                \n                \n                    LUA\n                \n                \n                \n                    PowerShell\n                \n                \n            \n            \n\n            \n            \n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "Content/Current/index.html",
    "href": "Content/Current/index.html",
    "title": "Current Backlog",
    "section": "",
    "text": "Order By\nDefault\n\n        Date - Oldest\n      \n\n        Date - Newest\n      \n\n        Title\n      \n\n        Author\n      \n\n    \n      \n      \n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\nOctober 8, 2025\n\n\nTo Do\n\n\nGary Newport\n\n\n\n\nNo matching items\n\n\n\n   \n    \n    Order By\nDefault\n\n        Date - Oldest\n      \n\n        Date - Newest\n      \n\n        Title\n      \n\n    \n      \n      \n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\nDecember 10, 2025\n\n\nDaily Note\n\n\n\n\n\nDecember 9, 2025\n\n\nDaily Note\n\n\n\n\n\nNovember 24, 2025\n\n\nDaily Note\n\n\n\n\n\nNovember 14, 2025\n\n\nDaily Note\n\n\n\n\n\nNovember 11, 2025\n\n\nDaily Note\n\n\n\n\n\nNovember 5, 2025\n\n\nDaily Note\n\n\n\n\n\nNovember 4, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 22, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 21, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 20, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 16, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 15, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 14, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 13, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 10, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 9, 2025\n\n\nDaily Note\n\n\n\n\n\nOctober 8, 2025\n\n\nDaily Note\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Content/Current/posts/2025-10-09/index.html",
    "href": "Content/Current/posts/2025-10-09/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "Quarto PlantUML diagram support\n\nTrying the published plantuml filters\n\n\n\ninline script"
  },
  {
    "objectID": "Content/Current/posts/2025-10-13/index.html",
    "href": "Content/Current/posts/2025-10-13/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-13\n\nDiabetes\n\nAsk about a holiday letter\nRecommended to do the X-pert Online course\nNHS Online Course https://app.changinghealth.com\nHad a look for insulin travel bag\n\nGP Appointment review\n\nWas Diabetic review with the nurse\nNurse took blood for chloesterol\nCancelled and rebooked Blood Test with St Peters this time.\n\nQuarto PlantUML filter\n\nTried to use the published PlantUML filters and could not get them to work.\nNeed to create my own filter."
  },
  {
    "objectID": "Content/Current/posts/2025-10-15/index.html",
    "href": "Content/Current/posts/2025-10-15/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-15\n\nDiabetes\n\nContinued the Diabetes online course.\n\nQuarto PlantUML filter\n\nContinued with the Quarto filter to process PlantUML diagrams within Quarto documents.\nWorked on placing all output files in a particular subdirectory.\nNeed to find a way to echo the script rather than create a puml file."
  },
  {
    "objectID": "Content/Current/posts/2025-10-20/index.html",
    "href": "Content/Current/posts/2025-10-20/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-20\n\nDiabetes\n\nContinued the Diabetes online course.\n\nQuarto\n\nCreated a example SAD Website - Quarto-Frontify\n\nJournal\n\nChanged the Journal template to create named directories for each post rather than date based directories. This makes it easier to manage posts with similar dates.\nOne thing to note is that the directory created includes spaces, which should be removed otherwise you have to replace them with ‚Äò%20‚Äô in any WikiLink :(\n\nLooking for a method to camelcase the ‚Äò${FOAM-TITLE}‚Äô variable."
  },
  {
    "objectID": "Content/Current/posts/2025-10-22/index.html",
    "href": "Content/Current/posts/2025-10-22/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-10-22\n\nDiabetes\n\nNHS Online Course https://app.changinghealth.com\n\nCalendar Template Fustrating‚Ä¶.\n\nSolved the issue.\n\nUsed EJS to create a JSON object and save it to a div.\nHave the Javascript get the JSON object from the DOM and then process it, Simples.\n\nThe frustrating part is that I origianily got the jsCalendar update code from AI, maybe AI can help the author out, as no such method or concept exists.\nIt seems that the author of jsCalendar has a beta extension that allows the logging of events.\n\nnot go this to work :(\nEvent Demo\nI created a stripped down version of the code below.\n\n\n\n&lt;% const dates = []; %&gt;\n&lt;%     items.forEach( item =&gt; { %&gt;\n        &lt;div class=\"listing-item\"&gt;\n&lt;%         if (item.date) { %&gt;\n&lt;%             const d = new Date(item.date); %&gt;\n&lt;%             const year = d.getFullYear(); %&gt;\n&lt;%             const month = d.getMonth() + 1; // getMonth() is 0-indexed %&gt;\n&lt;%             const day = d.getDate(); %&gt;\n&lt;%             dates.push(day+\"-\"+month+\"-\"+year); %&gt;\n               &lt;p&gt;&lt;%= item.date  %&gt;****&lt;%= d.toString() %&gt;--&lt;%= day %&gt;-&lt;%= month %&gt;-&lt;%= year %&gt;&lt;/p&gt;\n&lt;%         } %&gt;\n        &lt;/div&gt;\n&lt;%     }); %&gt;\n&lt;%     const serverData = { dates: dates }; %&gt;\n\n\n&lt;div id=\"data-container\" data-config=\"&lt;%- JSON.stringify(serverData) %&gt;\"&gt;&lt;/div&gt;\n\n&lt;script src=\"https://unpkg.com/simple-jscalendar@1.4.5/source/jsCalendar.min.js\" integrity=\"sha384-F3Wc9EgweCL3C58eDn9902kdEH6bTDL9iW2JgwQxJYUIeudwhm4Wu9JhTkKJUtIJ\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/simple-jscalendar@1.4.5/source/jsCalendar.min.css\" integrity=\"sha384-CTBW6RKuDwU/TWFl2qLavDqLuZtBzcGxBXY8WvQ0lShXglO/DsUvGkXza+6QTxs0\" crossorigin=\"anonymous\"&gt;\n\n&lt;div id=\"calendar\"&gt;&lt;/div&gt;\n\n&lt;script&gt;\n    var calendar = jsCalendar.new(\"#calendar\", \"now\", { \"monthFormat\": \"month YYYY\" });\n    document.addEventListener(\"DOMContentLoaded\", function() {\n        const container = document.getElementById(\"data-container\");\n        const jsonString = container.getAttribute(\"data-config\");\n        // Parse the JSON string retrieved from the attribute\n        const data = JSON.parse(jsonString);\n        data.dates.forEach( item =&gt; {\n            const d = new Date(item);\n            const year = d.getFullYear();\n            const month = d.getMonth() + 1; // getMonth() is 0-indexed\n            const day = d.getDate();     \n            calendar.setHighlightedDate(new Date(year, month, day));\n        });\n   \n  });\n&lt;/script&gt;"
  },
  {
    "objectID": "Content/Current/posts/2025-11-05/index.html",
    "href": "Content/Current/posts/2025-11-05/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-11-05\n\nHappy Guy Fawkes Night :-)\nCalendar template\n\nTried rather unsucessfully to display the calendar and the content divs side by side.\n\nTried flexbox\n\nSeems the Quarto css is interfering with the flexbox css.\nTried Article and Aside html tags :-(\n\n\nI‚Äôll have another go, but I think its time to move on.\n\nLook at Quarto‚Äôs inbuilt formatting\n\n\nChanging the title of the Current Blog to Now!\nQuarto\n\nThere seems to be an anoying limitation where you have to maintain the same page format across the website.\n\nFor instance\n\nNow!/Diary Page\n\nOn the Now!/Diary page ideally the todo section would be collapsible once all the tasks are complete.\nI dont need a list of the dated progress pages. Having a calendar in the sidebar indicating which days has a post would suffice.\nClick on a date would collapse the todo list and reveal the blog post.\n\nBlog\n\nSingle Page\n\nNo issues\n\nMulti-Page\n\nMulti-page documents\n\nThe index page should be included in the Blog list.\nThe index page should be able to link to pages and sub pages.\n\nSo longer POC can be investigated and the results grouped.\n\n\n\n\nProjects\n\n(Current Behaviour) Projects should be separtate GitHub websites that contains Quarto Book structured documents. Done"
  },
  {
    "objectID": "Content/Current/posts/2025-11-14/index.html",
    "href": "Content/Current/posts/2025-11-14/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-11-14\n\nCalendar Widget.\n\nPure Javascript\nHava a single div, to provide the reference in html\nHighlight\n\nCurrent day\nDays with a blog post\n\nStretch\n\nOn hover show the title/s of the post.\nAble to select a post.\n\n\n\nOn click load the post\nFlexible calendar table\n\nMonth\nThis Year\nYear to this month\n\nala Github\n\nShould have &lt; and &gt; to move to the previous and next period.\n\nejs\n\nLoad the blog posts into a json array and store it on the page.\n\nconst events = []; \nitems.forEach( item =&gt; { \n    if (item.date) { \n        const d = new Date(item.date); \n        const year = d.getFullYear(); \n        const month = d.getMonth() + 1; \n        const day = d.getDate(); \n\n        var date = month+\"-\"+day+\"-\"+year; \n        var link = item.path; \n        var event = { \"date\": date, \"link\": link } \n\n        events.push(event); \n    } \n}); \nconst serverData = { events: events }; \nJs\n\nCalendar table function\nAdd event function\nOn Click function\n\nCss keyed to the calendar"
  },
  {
    "objectID": "Content/Current/posts/2025-12-09/index.html",
    "href": "Content/Current/posts/2025-12-09/index.html",
    "title": "Daily Note",
    "section": "",
    "text": "2025-12-09\n\nLand Registry documentation.\nfleshing out the service handler documentation\nAsked AI to see if it could make surgestions on how to improve the service handler design."
  },
  {
    "objectID": "Content/Current/todo.html",
    "href": "Content/Current/todo.html",
    "title": "To Do",
    "section": "",
    "text": "Tasks\n\n\nLand Registry Titles\n\n\nTitle Service (Land Registry or Nimbus)\n\nStill not heard anything from Al concerning Nimbus\n\nIf we go with Nimbus, which sounds likely as the cost is lower, we will have to rename the project.\nIf we change the title to a different name, we should investigate using multiple services, so we can support Scotland and Northern Ireland and any other country as they come along.\n\nMyKf\n\nInvestigate how MyKf will save the data in the hub db\nInvestigate what MyKf will save in the database\nTalk to Katy to find out who is the developer\n\nRams Solaiappan\n\n\n\n\n\n\n\nQuarto\n\n\nQuarto\n\nPlantuml plugin\n\nDiagrams are loaded from files source\nAny raw plantuml diagram\nCreated a LUA PlantUML filter for Quarto\nAccepts a Puml file as a attribute\nAccepts inline PlantUML code\n\nCreates a local puml file if inline code is used\n\nRenders the PlantUML diagram as SVG\nRenders puml files in the current directory.\nWorks with subdirectories\nDirect where to store the generated SVG files\n\nQuarto and LUA processing executes relative to the location of the .qmd file. So we will need a method to identify which directory we are in so we can then redirect the output files to a specified subdirectory.\n\nCreate a GitHub repo for the filter\nPublish to Quarto extensions\nDocument the filter as a journal entry\nEcho script rather than create puml file.\n\nExecuting a ‚ÄòEcho‚Äô command terminates when it reaches a line break. So we will need to replace all line breaks with a special character sequence, e.g.¬†‚Äò‚Äô and then use a script to replace the special character sequence with a line break. Currently the directory is littered with intermediate .puml files which are not needed once the diagram has been created.\n\n\n\n\n\n\n\nAPI First Quarto Integration\n\n\nAPI First Quarto Integration\n\nCreate a Journal page to document the process\nInvestigate the TypeSpec process\nInvestigate the AutoRest process\nCreate a End to End example\n\nCreate a GitHub repo to hold the example\nPublish the example to GitHub\n\nCreate a Quarto filter to automate the process\n\nCreate a GitHub repo to hold the filter\nPublish the filter to Quarto extensions\nDocument the filter as a journal entry\n\n\n\n\n\n\nSAD Website\n\n\nSAD Website\n\nCreated a Frontify Quarto Book\nBook loaded into GitHub\nBook published to GitHub Pages\n\nBook ppublished as a website at https://newportg.github.io/Quarto-Frontify/\n\nRemove any references to KF\n\n\n\n\n\nQuarto Calendar Template\n\n\nQuarto Calendar Template\n\nLoad a calendar with dates from blog posts\nIndicate on the calendar days when a posting has occured.\nClick on a date and a link is populated with the required page\nA div on the page is populated with the blog post.\n\nStop Quarto formating the content page with headers and footers\n\nLoads the html and only extracts the content div\n\nSort out the page structure."
  },
  {
    "objectID": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html",
    "href": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "",
    "text": "KeyVault is a central resource where you should be storing all of your applications secrets/connection strings and config. The reason for using KeyVault rather than using a local config file, is really quite simple. config files dont exist for most azure artefacts, and if they did, it would encourage the sort of key management that has plagued .NET since day one. By using the KeyVault in this way means that the application code doesn‚Äôt need to know anything about the environment or its configuration. The build process should inject any secrets into the parameters file, which the ARM Template deployment consumes and deploys.\nSo How do we do it, in ARM Templates"
  },
  {
    "objectID": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html#arm-template-definition",
    "href": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html#arm-template-definition",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "ARM Template Definition",
    "text": "ARM Template Definition\n\nCreate a Azure Function\n\nIdentity should be ‚ÄòSystem Assigned‚Äô\nWEBSITE_ENABLE_SYNC_UPDATE_SITE\n\nN.B There is a reason for this, but I cant find it right now.\n\nAdd KeyVault Key names to App Settings\n\nThe KeyVault key is not a secret.\nThis creates a Appsetting which can be seen in the code, and assigns to it a KeyVault secret.\n\n\n\n\"name\": \"appsettings\",\n    \"properties\": {\n        \"applicationuser\": \"[concat('@Microsoft.KeyVault(SecretUri=', reference('applicationuser').secretUriWithVersion, ')')]\"\n    }\n\nDefine a KeyVault\n\nSet access policy\n\nTennant Id\nSystem Assigned Object Id\n\nLoad secrets etc into KeyVault\n\nFuture\n\nAssign keys to App Settings via a array copy structure\n\nThis would move the definition to the parameters file.\n\nThe issue with this approach is that the copy construct cannot operate on child resources (See Below).\n\n\n\n\n\"resources\": [\n  {\n    \"type\": \"{provider-namespace-and-type}\",\n    \"name\": \"parentResource\",\n    \"copy\": {  \n      /* yes, copy can be applied here */\n    },\n    \"properties\": {\n      \"exampleProperty\": {\n        /* no, copy cannot be applied here */\n      }\n    },\n    \"resources\": [\n      {\n        \"type\": \"{provider-type}\",\n        \"name\": \"childResource\",\n        /* copy can be applied if resource is promoted to top level */ \n      }\n    ]\n  }\n]"
  },
  {
    "objectID": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html#application-usage",
    "href": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html#application-usage",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "Application Usage",
    "text": "Application Usage\nAccess as you would a config variable\n        private static string StorageAccount = System.Environment.GetEnvironmentVariable(\"StorageAccount\");"
  },
  {
    "objectID": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html#code-snippets-from-integration",
    "href": "Content/Journal/posts/AccessingKeyVaultSecrectsFromAzFunc/index.html#code-snippets-from-integration",
    "title": "Getting Secrets from KeyVault in Azure Functions",
    "section": "Code snippets from integration",
    "text": "Code snippets from integration\n\nArm Parameters\n    \"para_kvSecretsObject\": {\n      \"value\": {\n        \"secrets\": [\n          {\n            \"secretName\": \"applicationuser\",\n            \"secretValue\": \"OVERWRITTEN BY VSTS\"\n          },\n          {\n            \"secretName\": \"StorageAccount\",\n            \"secretValue\": \"DefaultEndpointsProtocol=https;AccountName=kfaalphaneugeneralst01;AccountKey=&lt;ACCOUNTKEY&gt;;EndpointSuffix=core.windows.net\"\n          }\n        ]\n      }\n    }\n\n\nARM Template\n    {\n      \"apiVersion\": \"2016-08-01\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Storage/storageAccounts/', variables('var_str_name'))]\"\n      ],\n      \"identity\": {\n        \"type\": \"SystemAssigned\"\n      },\n      \"kind\": \"functionapp\",\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_azf_name')]\",\n      \"properties\": {\n        \"name\": \"[variables('var_azf_name')]\",\n        \"siteConfig\": {\n          \"appSettings\": [\n            {\n              \"name\": \"AzureWebJobsDashboard\",\n              \"value\": \"[concat('DefaultEndpointsProtocol=https;AccountName=',variables('var_str_name'),';AccountKey=',listKeys(variables('var_str_resId'),'2015-05-01-preview').key1) ]\"\n            },\n            {\n              \"name\": \"AzureWebJobsStorage\",\n              \"value\": \"[concat('DefaultEndpointsProtocol=https;AccountName=',variables('var_str_name'),';AccountKey=',listKeys(variables('var_str_resId'),'2015-05-01-preview').key1) ]\"\n            },\n            {\n              \"name\": \"APPINSIGHTS_INSTRUMENTATIONKEY\",\n              \"value\": \"[reference(concat('microsoft.insights/components/', variables('var_appin_name'))).InstrumentationKey]\"\n            },\n            {\n              \"WEBSITE_ENABLE_SYNC_UPDATE_SITE\": \"true\"\n            }\n          ],\n          \"alwaysOn\": false\n        },\n        \"clientAffinityEnabled\": false,\n        \"serverFarmId\": \"[variables('var_hstpln_name')]\",\n        \"hostingEnvironment\": \"[variables('var_hstpln_env')]\",\n        \"hostNameSslStates\": [\n        ]\n      },\n      \"resources\": [\n        {\n          \"apiVersion\": \"2015-08-01\",\n          \"dependsOn\": [\n            \"[resourceId('Microsoft.Web/sites', variables('var_azf_name'))]\",\n            \"[resourceId('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\",\n            \"secretsCopy\"\n          ],\n          \"name\": \"appsettings\",\n          \"properties\": {\n            \"applicationuser\": \"[concat('@Microsoft.KeyVault(SecretUri=', reference('applicationuser').secretUriWithVersion, ')')]\",\n            \"StorageAccount\": \"[concat( '@Microsoft.KeyVault(SecretUri=', reference('StorageAccount').secretUriWithVersion, ')')]\"\n          },\n          \"tags\": {\n            \"displayName\": \"AppSettings\"\n          },\n          \"type\": \"config\"\n        }\n      ],\n      \"tags\": {\n        \"displayName\": \"Az Function\"\n      },\n      \"type\": \"Microsoft.Web/sites\"\n    },\n    {\n      \"apiVersion\": \"2016-10-01\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Web/sites/', variables('var_azf_name'))]\"\n      ],\n      \"location\": \"[resourceGroup().location]\",\n      \"name\": \"[variables('var_kv_name')]\",\n      \"properties\": {\n        \"sku\": {\n          \"family\": \"A\",\n          \"name\": \"Standard\"\n        },\n        \"tenantId\": \"[variables('var_ten_id')]\",\n        \"accessPolicies\": [\n          {\n            \"tenantId\": \"[variables('var_ten_id')]\",\n            \"objectId\": \"[reference(variables('var_svc_prin'), '2015-08-31-PREVIEW').principalId]\",\n            \"permissions\": {\n              \"keys\": [\n                \"Get\",\n                \"List\",\n                \"Update\",\n                \"Create\",\n                \"Import\",\n                \"Delete\",\n                \"Recover\",\n                \"Backup\",\n                \"Restore\",\n                \"Decrypt\",\n                \"Encrypt\",\n                \"UnwrapKey\",\n                \"WrapKey\",\n                \"Verify\",\n                \"Sign\",\n                \"Purge\"\n              ],\n              \"secrets\": [\n                \"Get\",\n                \"List\",\n                \"Set\",\n                \"Delete\",\n                \"Recover\",\n                \"Backup\",\n                \"Restore\",\n                \"Purge\"\n              ],\n              \"certificates\": [\n                \"Get\",\n                \"List\",\n                \"Update\",\n                \"Create\",\n                \"Import\",\n                \"Delete\",\n                \"Recover\",\n                \"Backup\",\n                \"Restore\",\n                \"ManageContacts\",\n                \"ManageIssuers\",\n                \"GetIssuers\",\n                \"ListIssuers\",\n                \"SetIssuers\",\n                \"DeleteIssuers\",\n                \"Purge\"\n              ]\n            }\n          }\n        ],\n            \"enabledForDeployment\": false,\n            \"enabledForDiskEncryption\": false,\n            \"enabledForTemplateDeployment\": false\n      },\n      \"scale\": null,\n      \"tags\": {\n        \"displayName\": \"Key Vault\"\n      },\n      \"type\": \"Microsoft.KeyVault/vaults\"\n    },\n    {\n      \"apiVersion\": \"2015-06-01\",\n      \"copy\": {\n        \"name\": \"secretsCopy\",\n        \"count\": \"[length(parameters('para_kvSecretsObject').secrets)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.KeyVault/vaults/', variables('var_kv_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_kv_name'), '/', parameters('para_kvSecretsObject').secrets[copyIndex()].secretName)]\",\n      \"properties\": {\n        \"value\": \"[parameters('para_kvSecretsObject').secrets[copyIndex()].secretValue]\"\n      },\n      \"tags\": {\n        \"displayName\": \"Key Vault Secrets\"\n      },\n      \"type\": \"Microsoft.KeyVault/vaults/secrets\"\n    }"
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CARE.html",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CARE.html",
    "title": "CARE Framework for AI-Driven Code Development",
    "section": "",
    "text": "The SoftwareSeni Specification Template suggests using the CARE framework to prevent AI from making arbitrary architectural choices:\n\nContext: Why this exists and where it fits in the architecture.\nAction: Detailed functional steps with few-shot examples.\nResult: Measurable acceptance criteria (e.g., ‚Äúresponse &lt; 200ms‚Äù).\nEvaluation: Non-functional constraints like security and error handling.\n\n\n\nA typical vague requirement like ‚ÄúHandle user authentication‚Äù often leads to security vulnerabilities. Below is an example of an AI-Ready Requirement that produces high-quality code on the first try:\n\n\n\n\n\n\n\nSection\nAI-Ready Specification\n\n\n\n\nUser Story\nAs a user, I want to log in securely using my email and password.\n\n\nTechnical Context\nImplement using JWT tokens with bcrypt password hashing.\n\n\nInputs/Data\nemail: String, max 255 chars, unique. password: String, min 8 chars, must include upper/lower/number/symbol.\n\n\nAcceptance Criteria\n1. Lock account for 10 mins after 3 failed attempts.\n\n\n\n2. Terminate session after 30 mins of inactivity.\n\n\n\n3. Return generic error messages to prevent user enumeration.\n\n\nEdge Cases\nHandle null inputs empty strings, and SQL injection patterns in name fields.\n\n\nSuccess Examples\n1. login(‚Äòuser@test.com‚Äô, ‚ÄòValidPass123!‚Äô) ‚Üí Returns 200 OK + JWT.\n\n\n\n2. login(‚Äòwrong@test.com‚Äô, ‚Äòpass‚Äô) ‚Üí Returns 401 Unauthorized.",
    "crumbs": [
      "Now!",
      "CARE Framework"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/CARE.html#example-ai-ready-requirement",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/CARE.html#example-ai-ready-requirement",
    "title": "CARE Framework for AI-Driven Code Development",
    "section": "",
    "text": "A typical vague requirement like ‚ÄúHandle user authentication‚Äù often leads to security vulnerabilities. Below is an example of an AI-Ready Requirement that produces high-quality code on the first try:\n\n\n\n\n\n\n\nSection\nAI-Ready Specification\n\n\n\n\nUser Story\nAs a user, I want to log in securely using my email and password.\n\n\nTechnical Context\nImplement using JWT tokens with bcrypt password hashing.\n\n\nInputs/Data\nemail: String, max 255 chars, unique. password: String, min 8 chars, must include upper/lower/number/symbol.\n\n\nAcceptance Criteria\n1. Lock account for 10 mins after 3 failed attempts.\n\n\n\n2. Terminate session after 30 mins of inactivity.\n\n\n\n3. Return generic error messages to prevent user enumeration.\n\n\nEdge Cases\nHandle null inputs empty strings, and SQL injection patterns in name fields.\n\n\nSuccess Examples\n1. login(‚Äòuser@test.com‚Äô, ‚ÄòValidPass123!‚Äô) ‚Üí Returns 200 OK + JWT.\n\n\n\n2. login(‚Äòwrong@test.com‚Äô, ‚Äòpass‚Äô) ‚Üí Returns 401 Unauthorized.",
    "crumbs": [
      "Now!",
      "CARE Framework"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/DevContainers.html",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/DevContainers.html",
    "title": "Dev Containers",
    "section": "",
    "text": "Introduction\n\n\nSpec-kit Dev Container\nThe github Spec-kit Dev Container Template provides a dev container for AI driven code development. It includes a pre-configured environment with all the necessary tools and libraries for AI development, including Python, Jupyter, and popular AI frameworks like TensorFlow and PyTorch.\nSpec-kit Dev Container Template\n If you naviagte to the github repository you can click the ‚ÄúUse this template‚Äù button to create a new repository with the dev container template. Once you have created your repository, you can open it in VSCode and it will prompt you to open the dev container. This will create a new development environment that is isolated from your local machine and has all the necessary tools and libraries for AI development.\nN.B. I have tried this, but the script errors out when it tries the line ‚Äòsudo apt-get update -y‚Äô. You can comment out this line and the script will run successfully.\n\nRUN specify check to verify that spec-kit is installed correctly.\n\n\n\nReferences\n\nVSCode Dev Containers",
    "crumbs": [
      "Now!",
      "VSCode",
      "Dev Containers"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/Podman.html",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/Podman.html",
    "title": "Podman",
    "section": "",
    "text": "Introduction\nI was about to dive into the difficulty of installing Podman on Windows, But fortuately due to a convientent system rebuild I had a clean Windows 11 install and I was able to install Podman without any issues.\n\nInstall Podman Desktop from the Podman Windows Desktop page.\nInstall Podman CLI from the Podman Windows CLI page.\nInstall WSL2 from the Podman Windows page.\n\n\n\nReferences\n\nPodman\nPodman Windows",
    "crumbs": [
      "Now!",
      "VSCode",
      "Podman"
    ]
  },
  {
    "objectID": "Content/Journal/posts/AIDrivenCodeDevelopement/VSCode.html",
    "href": "Content/Journal/posts/AIDrivenCodeDevelopement/VSCode.html",
    "title": "VSCode",
    "section": "",
    "text": "Introduction\nI have been using Visual Studio Code (VSCode) for a while now and I have found it to be a great code editor. It has a lot of features that make it easy to use and it is also very customizable. I have been using it for both personal and professional projects and I have found it to be a great tool for both.\n\n\nDev-Containers\nOne of the features that I have found to be particularly useful is the ability to use dev-containers. This allows me to create a development environment that is isolated from my local machine. This is great for testing and debugging code without having to worry about breaking my local environment.\nInorder to use dev containers in VSCode, you need to install the extensions WSL and Dev Containers. Once you have these installed, you can create a dev container by creating a .devcontainer folder in your project and adding a devcontainer.json file. This file will contain the configuration for your dev container.\n\n\nPodman Settings\nI have been using Podman as my container runtime and I have found it to be a great alternative to Docker. In order to use Podman with VSCode, you need to set the ‚Äúdocker.dockerPath‚Äù setting to the path of the Podman executable. This will allow VSCode to use Podman instead of Docker when running containers.\n\n\nReferences\n\nVSCode\nVSCode Dev Containers",
    "crumbs": [
      "Now!",
      "VSCode"
    ]
  },
  {
    "objectID": "Content/Journal/posts/APIFirst/TypeSpec.html",
    "href": "Content/Journal/posts/APIFirst/TypeSpec.html",
    "title": "TypeSpec",
    "section": "",
    "text": "TypeSpec\nTypeSpec is a powerful tool for designing and generating APIs. It allows developers to define the structure and behavior of their APIs using a simple and intuitive syntax. TypeSpec supports a wide range of features, including:\n\nDefining endpoints and routes\n\nSpecifying request and response formats\nAdding authentication and authorization\nGenerating client and server code in multiple programming languages\nCreating interactive documentation for APIs\nValidating API definitions against industry standards such as OpenAPI and Swagger\nExtending TypeSpec with custom plugins and extensions\n\nTypeSpec is designed to be easy to use and understand, making it accessible to developers of all skill levels. It also integrates seamlessly with other tools and frameworks, such as AutoRest, making it a versatile choice for API development.\nTypeSpec can be used in a variety of scenarios, including:\n\nBuilding RESTful APIs for web and mobile applications\nCreating microservices and serverless functions\nDesigning APIs for data analytics and machine learning applications\nGenerating SDKs and client libraries for third-party developers\nAutomating API documentation and testing processes\nPrototyping and iterating on API designs quickly and efficiently\nIntegrating with CI/CD pipelines for continuous delivery of APIs\nEnsuring compliance with industry regulations and standards for API security and privacy\nFacilitating API versioning and backward compatibility for long-term maintenance\nSupporting multiple API styles, including REST, GraphQL, and gRPC\nEnabling rapid development and deployment of APIs in cloud-native environments\n\n\n\nReferences\n\nOne API Definition To Rule Them All: Generating GraphQL Schemas From Typespec\nTypespec Graphql Emitter\nFionaBronwen Github"
  },
  {
    "objectID": "Content/Journal/posts/AzureApplicationHosting/index.html",
    "href": "Content/Journal/posts/AzureApplicationHosting/index.html",
    "title": "Azure Application Hosting",
    "section": "",
    "text": "Each component is a separately deploy-able artefact, but we need a coherent single URL to link them all. The normal method would be to deploy out each individual component to Azure and each would get its own ‚Äòaurewebsites.com‚Äô URL. This approach would lead to confusion, as it would mean you would need to keep lists of URL‚Äôs By using the proxy feature of Azure Functions we can define routes to each of the installed artefacts while preserving a single URL for the application. So:-\n\n\n\nRoute\nResult\n\n\n\n\nzoomalong.co.uk\nWebsite\n\n\nzoomalong.co.uk/api\nAzure functions\n\n\nzoomalong.co.uk/static\nAzure Storage Account\n\n\n\nUsed to store any files or images etc. Because both the API and Web Application exist on the same URL then we won‚Äôt run into any CORS issues. Remember to bind the DNS Cname to the Azure function proxy and not the website.\n\n\n\naah1\n\n\nHow ? In your Azure Functions Project Create a files called proxies.json and insert the following code\nproxies.json\n{\n  \"$schema\": \"http://json.schemastore.org/proxies\",\n    \"proxies\": {\n      \"api\": {\n        \"matchCondition\": {\n          \"route\": \"/api/{*url}\"\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"app\": {\n        \"matchCondition\": {\n          \"route\": \"{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      },\n      \"appResources\": {\n        \"matchCondition\": {\n          \"route\": \"/static/{*url}\",\n          \"methods\": [ \"GET\", \"HEAD\", \"OPTIONS\" ]\n        },\n        \"backendUri\": \"Set With Build Variable/{url}\"\n      }\n    }\n  }\nChange in the build. * Build should include Azure App Service Deploy V3 or greater * Update this section with the file to be changed. *  * Add your substitutions to the Variables section * As Path to variable to be replace element.element.element * Value to be replaced. * as below\n\n\n\n\n\n\n\nProxy\nURL\n\n\n\n\nproxies.api.backendUri\nhttps://azure website url\n\n\nproxies.app.backendUri\nhttps://azure function url\n\n\nproxies.appResources.backendUri\nhttps:// azure storage account blob strorage\n\n\n\n\nRepeat in the release."
  },
  {
    "objectID": "Content/Journal/posts/AzureFunctionDI/index.html",
    "href": "Content/Journal/posts/AzureFunctionDI/index.html",
    "title": "Azure Functions Dependency Injection",
    "section": "",
    "text": "Azure functions has always been missing the startup sequence enjoyed by other .net core applications. DI is now a first class member and the above post by microsoft explains whats needed well."
  },
  {
    "objectID": "Content/Journal/posts/AzureFunctionDI/index.html#reference",
    "href": "Content/Journal/posts/AzureFunctionDI/index.html#reference",
    "title": "Azure Functions Dependency Injection",
    "section": "Reference:",
    "text": "Reference:\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-dotnet-dependency-injection"
  },
  {
    "objectID": "Content/Journal/posts/AzureFunctionsLogging/index.html",
    "href": "Content/Journal/posts/AzureFunctionsLogging/index.html",
    "title": "Azure Functions Logging",
    "section": "",
    "text": "Azure Functions include a built in ILogger so you can fulfill all your logging needs, There is one issue, if you try to use the Ilogger in any of you subsequent classes the ILogger seems to be null.\n\nExample\nnamespace MyFunctionApp\n{\n    public class MyFunctionClass\n    {\n        private readonly ILogger logger;\n\n        // DOESN'T WORK!\n        public MyFunctionClass(ILogger logger)\n        {\n              this.logger = logger;\n        }\n\n        [FunctionName(\"MyFunctionName\")]\n        public async Task&lt;IActionResult&gt; MyFunctionMethod(\n            [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)]\n            HttpRequest req,\n            ILogger log) // WORKS!\n       {\n           ...\n       }\n    }\n}\n\n\nSolution\nThe solution is simple but a little annoying :( Instead of using the ILogger use ILogger&lt;&gt; instead putting the class name as the type.\npublic class MyFunctionClass\n    {\n        private readonly ILogger logger;\n\n        // WORKS!\n        public MyFunctionClass(ILogger&lt;MyFunctionClass&gt; logger)\n        {\n              this.logger = logger;\n        }\n        ...\n    }\nThere is an additional gotcha, which has been raised to Microsoft as a bug, and that is you need to specify your NameSpaces that uses the ILogger in the hosts.json file\n{\n    \"version\": \"2.0\",\n    \"logging\": {\n        \"logLevel\": {\n            \"MyFunctionApp\": \"Trace\"\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/AzureStaticWebsites/index.html",
    "href": "Content/Journal/posts/AzureStaticWebsites/index.html",
    "title": "Azure Static Websites",
    "section": "",
    "text": "Microsoft announced that you can now enable static websites on a storage account. This will generate a new URL for your site, and enable read access to any static html files within the blob storage. There‚Äôs a link to Microsoft‚Äôs preview announcement here. https://azure.microsoft.com/en-us/blog/azure-storage-static-web-hosting-public-preview/\nAt the moment I prefer the approach made by Anthony Chu in his blog post https://anthonychu.ca/post/azure-functions-static-file-server/\nHe hosts a index.html file within a Azure function Http Trigger request. Although this only returns a static file, it does allow you to create a on demand website. The Html file you serve up can should only contain links to CDN resources or to readable JS or other files within your storage account blob storage. By proxying the azure function then everything could be accessed via the same URL.\nusing System.IO;\nusing System.Linq;\nusing System.Net;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Threading.Tasks;\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.Azure.WebJobs.Extensions.Http;\nusing Microsoft.Azure.WebJobs.Host;\nusing MimeTypes;\n\nnamespace Counterflip\n{\n    public static class WebSite\n    {\n        [FunctionName(\"WebSite\")]\n        public static async Task&lt;HttpResponseMessage&gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)]HttpRequestMessage req, TraceWriter log)\n        {\n            log.Info(\"C# HTTP trigger function processed a request.\");\n\n            try\n            {\n                var response = new HttpResponseMessage(HttpStatusCode.OK);\n                var stream = new FileStream(@\"www\\index.html\", FileMode.Open);\n                response.Content = new StreamContent(stream);\n                response.Content.Headers.ContentType =\n                    new MediaTypeHeaderValue(GetMimeType(@\"www\\index.html\"));\n                return response;\n            }\n            catch\n            {\n                return new HttpResponseMessage(HttpStatusCode.NotFound);\n            }\n        }\n\n        private static string GetMimeType(string filePath)\n        {\n            var fileInfo = new FileInfo(filePath);\n            return MimeTypeMap.GetMimeType(fileInfo.Extension);\n        }\n    }\n}"
  },
  {
    "objectID": "Content/Journal/posts/DataAnnotationVsFluentValidation/index.html",
    "href": "Content/Journal/posts/DataAnnotationVsFluentValidation/index.html",
    "title": "Data Annotation Vs FluentValidation",
    "section": "",
    "text": "Data Annotation\n\nFor\n\nIs SOLID\n\nValidations are placed in the model, next to the defintions\n\nPart of standard library\n\nAgainst\n\nValidations are not Rules, and cannot span members\nOnly represents simple validations in the openapi schema\n\n\n\n\nFluent Validation\n\nFor\n\nRules can span multiple methods\n\nAgainst\n\nNot SOLID\n\nValidations and Rules are specified in a separate file\n\nOnly represents simple validations in the openapi schema\nAdditonal setup\n\n\n\n\nConclusion\nI‚Äôve changed my mind, which is not uncommon. Where your class is interfacing with a different system such as being used in a API interface, then you need to convey to the user the maximum amount of information, about what is or isnt valid, so the validation details have to be encoded in the model. Where Data Annotations is lacking, E.g. OneOf, AnyOf, then you should build a custom validator to fill the gap.\nFluent Validation should be used internally, as it not only can apply general validations, it can also encode groups of validations into rules and apply them under different conditions.\n\n\nRef\nThis is a interesting read on validators, and also the implementation of custom validators. https://weblogs.asp.net/ricardoperes/net-8-data-annotations-validation"
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/index.html",
    "href": "Content/Journal/posts/DiagramFirstCoding/index.html",
    "title": "Diagram first coding",
    "section": "",
    "text": "What if you could create diagrams first, and then generate code from them? This is the idea behind diagram first coding, a software development approach that emphasizes visual representation of system architecture before writing any code.\nThis isnt so far fetched. Look at tools like C4, where the system is modeled using a series of heirarchical diagrams, which denote the strucutre of the system at different levels of abstraction. These diagrams can then be used to generate code stubs, which can be filled in with business logic later.\nThats the rub, systems like this always push to one side complexity to other systems when it gets too difficult to model. For example, you might model the high level architecture of a web application using C4 diagrams, but then have to resort to traditional coding for the detailed implementation of the business logic. However, with advances in model-driven development and code generation tools, its becoming increasingly feasible to adopt a diagram first coding approach. By leveraging these tools, developers can create more maintainable and scalable systems, as the visual representation of the architecture provides a clear understanding of the system‚Äôs structure and behavior.\nSo for example you might create a C4 diagram that models the high level architecture, and then APIFirst tools to generate the API endpoints and data models. This way, you can ensure that the code is aligned with the architecture, while still allowing for flexibility in the implementation of the business logic."
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/index.html#so-what-is-business-logic",
    "href": "Content/Journal/posts/DiagramFirstCoding/index.html#so-what-is-business-logic",
    "title": "Diagram first coding",
    "section": "So what is Business Logic?",
    "text": "So what is Business Logic?\nBusiness logic refers to the rules and processes that govern how a system operates and interacts with its users. It encompasses the core functionality of the application, including data processing, decision-making, and workflow management. In a diagram first coding approach, business logic can be represented visually through flowcharts, state diagrams, or other modeling techniques, allowing developers to better understand and communicate the system‚Äôs behavior before writing any code.\nSo its the logic inside a C4 Component that makes it do what it does. The code part of C4 provides the class hirarchy, but the business logic is what makes those classes actually do something useful. The business logic could be described in a variety of ways, such as pseudocode, flowcharts, or even natural language descriptions.\nThis is beginning to sound more like lego than coding. You have a set of pre-defined blocks (the C4 components) that you can snap together to create a system. Having a business logic engineer define the behavior of each block, and using a locode tool to generate the code for each block based on its defined behavior.\nA software developer would glue the blocks together, and ensure that they work together as intended. This approach could potentially lead to faster development times, as well as more maintainable and scalable systems."
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/index.html#challenges-of-diagram-first-coding",
    "href": "Content/Journal/posts/DiagramFirstCoding/index.html#challenges-of-diagram-first-coding",
    "title": "Diagram first coding",
    "section": "Challenges of Diagram First Coding",
    "text": "Challenges of Diagram First Coding\nWhile diagram first coding offers many benefits, it also presents several challenges. One of the main challenges is the need for specialized tools and expertise to create and interpret the diagrams. Additionally, there may be resistance from developers who are accustomed to traditional coding practices and may be hesitant to adopt a new approach. Another challenge is ensuring that the diagrams accurately represent the system‚Äôs architecture and behavior. Many of the concepts would disrupt the established software development lifecycle, requiring changes to existing processes and workflows.\nor just leave it to AI‚Ä¶.\n\nmainArchitecture\napiService\nrulesEngine"
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/rules-engine.html",
    "href": "Content/Journal/posts/DiagramFirstCoding/rules-engine.html",
    "title": "Component: Rules Engine (rules-engine.md)",
    "section": "",
    "text": "This sub-page defines the logic used by the business engine (e.g., Drools or NRule). Business Rules (Logic Engine)\n\n\n\n\n\n\n\n\n\nRule ID\nName\nCondition\nAction\n\n\n\n\nBR-001\nHigh Value Review\norder.amount &gt; 10000\nSet status = ‚Äúmanual_review‚Äù\n\n\nBR-002\nNew Customer Discount\ncustomer.isNew == true\nApply discount = 0.10\n\n\nBR-003\nBlacklist Check\ncustomer.id in blacklist\nSet status = ‚Äúrejected‚Äù\n\n\n\n\n\nInput: Order and Customer objects.\n\nEvaluation: Execute ‚ÄúBlacklist Check‚Äù first (Priority 1).\nCalculation: If not rejected, calculate discounts.\nOutput: Return modified Order object to API Service.\n\n\n\n\n\nArchitecture Rendering: Use the Structurizr Visualizer to paste the script from index.md.\nAPI Validation: Compile the TypeSpec code into OpenAPI using the TypeSpec Compiler.\nLogic Testing: Import the Business Rules table into a Decision Model and Notation (DMN) tool for execution."
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/rules-engine.html#rule-execution-workflow-dmn",
    "href": "Content/Journal/posts/DiagramFirstCoding/rules-engine.html#rule-execution-workflow-dmn",
    "title": "Component: Rules Engine (rules-engine.md)",
    "section": "",
    "text": "Input: Order and Customer objects.\n\nEvaluation: Execute ‚ÄúBlacklist Check‚Äù first (Priority 1).\nCalculation: If not rejected, calculate discounts.\nOutput: Return modified Order object to API Service."
  },
  {
    "objectID": "Content/Journal/posts/DiagramFirstCoding/rules-engine.html#implementation-tools",
    "href": "Content/Journal/posts/DiagramFirstCoding/rules-engine.html#implementation-tools",
    "title": "Component: Rules Engine (rules-engine.md)",
    "section": "",
    "text": "Architecture Rendering: Use the Structurizr Visualizer to paste the script from index.md.\nAPI Validation: Compile the TypeSpec code into OpenAPI using the TypeSpec Compiler.\nLogic Testing: Import the Business Rules table into a Decision Model and Notation (DMN) tool for execution."
  },
  {
    "objectID": "Content/Journal/posts/EIP-ContentBasedRouting/index.html",
    "href": "Content/Journal/posts/EIP-ContentBasedRouting/index.html",
    "title": "EIP Content Based Routing",
    "section": "",
    "text": "LA Pattern"
  },
  {
    "objectID": "Content/Journal/posts/FluentAsserts/index.html",
    "href": "Content/Journal/posts/FluentAsserts/index.html",
    "title": "Fluent Assertions",
    "section": "",
    "text": "Fluent Assertions is a helper library for testing that tries to add better context to your assertions.\nE.g. * Where you would have used * Assert.AreEqual(HttpStatusCode.OK, response.StatusCode); * or should it have been * Assert.AreEqual(response.StatusCode, HttpStatusCode.OK);\n\nYou can now do this :-\n\nresponse.StatusCode.Should().Be(HttpStatusCode.OK);\n\nOther than making the calling more explicit, it also gives a a clearer error message on failure.\nMessage: Expected response.StatusCode to be NotFound, but found OK.\n\n## Reference: * https://fluentassertions.com/"
  },
  {
    "objectID": "Content/Journal/posts/FunctionMonkey/index.html",
    "href": "Content/Journal/posts/FunctionMonkey/index.html",
    "title": "Function Monkey",
    "section": "",
    "text": "Function Monkey Hello World\n\n\n\nReferences\n\nFunction Monkey\nJason Roberts"
  },
  {
    "objectID": "Content/Journal/posts/IFrames/index.html",
    "href": "Content/Journal/posts/IFrames/index.html",
    "title": "IFrames",
    "section": "",
    "text": "What\n\nAn inline frame is used to embed another document within the current HTML document.\n\nHow\n\n&lt;iframe src=\"https://www.w3schools.com\"&gt;&lt;/iframe&gt;\nIFrames are not secure\nRight thats out of the way."
  },
  {
    "objectID": "Content/Journal/posts/IFrames/index.html#iframes-and-security",
    "href": "Content/Journal/posts/IFrames/index.html#iframes-and-security",
    "title": "IFrames",
    "section": "IFrames and Security",
    "text": "IFrames and Security\nThe element, by itself, is not a security risk. Unfortunately iframes have gotten a bad reputation because they can be used by malicious websites to include content that can infect a visitor‚Äôs computer without them seeing it on the page. This is done by having links point to the invisible scripts and those scripts set off malicious code. The user clicks the link and thinks that the link is broken because nothing appeared to happen, but a script was set off where they couldn‚Äôt see it.\nThe thing to remember when including an IFrame on your web page is that your users are only as safe as the content of all the sites you link to. If you have reason to feel a site is untrustworthy, don‚Äôt link to it in any fashion and most definitely don‚Äôt include its contents in an IFrame. Linking to your own pages within iframes, however, does not pose a security risk for you or your users."
  },
  {
    "objectID": "Content/Journal/posts/Loqate/index.html",
    "href": "Content/Journal/posts/Loqate/index.html",
    "title": "EIP Loqate",
    "section": "",
    "text": "Loqate\nLoqate is a service which supplies addresses and geocoding for a given search string. The Loqate service employs a iterative id base system where by, if you supply the id to subsequent calls then you will get narrower results. When you receive a result of type ‚ÄòAddress‚Äô then you can call Loqate‚Äôs Retrieve api call to get that items address details.\n\n\n\n\nLA Pattern"
  },
  {
    "objectID": "Content/Journal/posts/MediatR/index.html#hows-this-work",
    "href": "Content/Journal/posts/MediatR/index.html#hows-this-work",
    "title": "MediatR",
    "section": "How‚Äôs this work?",
    "text": "How‚Äôs this work?\n\nINotification (Mediator)\n\nDefines an interface for communicating with Colleague objects\n\nNotifier1, Notifier2 are\n\nEach Colleague class knows its Mediator object\nEach colleague communicates with its mediator whenever it would have otherwise communicated with another colleague\nConcrete implementations of the task to be performed,\nThey need to implement the Handler method defined by the INotificationHandler interface.\n\nNotificationMessage\n\nImplements cooperative behavior by coordinating Colleague objects knows and maintains its colleagues\nInstantiates the Notifier classes"
  },
  {
    "objectID": "Content/Journal/posts/MediatR/index.html#install",
    "href": "Content/Journal/posts/MediatR/index.html#install",
    "title": "MediatR",
    "section": "Install",
    "text": "Install\nInstall-Package MediatR"
  },
  {
    "objectID": "Content/Journal/posts/MediatR/index.html#reference",
    "href": "Content/Journal/posts/MediatR/index.html#reference",
    "title": "MediatR",
    "section": "Reference",
    "text": "Reference\n\nMediatr Framework\nDot Factory Pattern\nMedium Article\nMediator Tutorial 1\nMediator Tutorial 2\nMediator Tutorial 3"
  },
  {
    "objectID": "Content/Journal/posts/PackageDevelopment/index.html",
    "href": "Content/Journal/posts/PackageDevelopment/index.html",
    "title": "How to build Cloud components without any Azure Experience",
    "section": "",
    "text": "How to build Cloud components without any Azure Experience\nIt is a bit of a misnomer; you need people with cloud experience.\nSorry I am not going to say anything new here, but it just might be new here.\nIn typical application code is the same no matter what platform you develop on, you have an interface to the world, domain/business rules, and a data interface. Only the interface to the world needs to be managed by someone with more specialised knowledge. You could divide up the code and create two projects, one for the API, specialised to the environment it will be deployed, and one for the domain/business logic. This would enable you to simply wrap the domain/business rules in its own NuGet package and use normal developers to create them and allow them to use tradition tools to build and evaluate them.\n\n\n\npackage1\n\n\nThe previous solution would be fine, but it still leaves scope for organically grown code, which encapsulates complex business rules. The build test burden could be reduced further by making the projects even smaller. Reducing their complexity and increasing their automatic testability. In this case the Domain logic could be reduced to encoding process flow only, and business rules evaluation could be managed by a more suitable tool. Also if the data store interface is not specific to this application but a general interface used by many applications, then it also could make use of package management.\n\n\n\npackage2\n\n\nThe questions, I keep hearing when there is a proposed change in development strategy, such as ‚Ä¢ How do we keep up with package versioning ‚Ä¢ How do we make sure all applications are using the same package versions\nThe simple answer is developers should use good development practices.\n\nAt the start of any new development, they should\n\nRun the unit tests\nGet the latest packages\nRun the unit tests\nFix any issues caused by the updated packages\nRun the unit tests\n\nCreate new code\nCreate new tests\n\nRun the unit tests\n\nCreate a check in note\nCheck in\n\nThe only project where package version matters is the API project, as that what is presented to the user."
  },
  {
    "objectID": "Content/Journal/posts/PlantUml/index.html",
    "href": "Content/Journal/posts/PlantUml/index.html",
    "title": "PlantUml",
    "section": "",
    "text": "PlantUML is not directly supported by GitHub, but its still possible. Basically we pass your puml file to PlantUML to generate, and they return a PNG which gets included in the page\n\nGenerate you .puml file in the usual way using your favorite editor and confirm the diagram is as you want it.\nSave the file to the assets directory\nThen paste the following into you Markup.\n\nPlantUML is a text descriptive language which gets converted into UML and other types of graphs So this :-\n\nwould produce this :-\nA more complex example :-"
  },
  {
    "objectID": "Content/Journal/posts/QuartoWebBook/chapter1.html",
    "href": "Content/Journal/posts/QuartoWebBook/chapter1.html",
    "title": "Chapter1 Sub Page",
    "section": "",
    "text": "Chapter 1 Title\nContent for chapter 1 goes here. This can include code, images, etc.\n\n\nChapter 1 SubPage\nContent for chapter 1 Sub Page",
    "crumbs": [
      "Now!",
      "Chapter 1"
    ]
  },
  {
    "objectID": "Content/Journal/posts/QuartoWebBook/chapter2.html",
    "href": "Content/Journal/posts/QuartoWebBook/chapter2.html",
    "title": "Chapter2",
    "section": "",
    "text": "Chapter 2 Title\nContent for chapter 2 goes here. This can include code, images, etc.",
    "crumbs": [
      "Now!",
      "Chapter 2"
    ]
  },
  {
    "objectID": "Content/Journal/posts/Rules/index.html",
    "href": "Content/Journal/posts/Rules/index.html",
    "title": "Rules",
    "section": "",
    "text": "Splitting business rules away for the traffic flow of data has become commonplace in all development for the last few years. It has a lot of advantages as it crystallizes the rules for an object into a simple definable, readable and most of all testable item.\n\n\nBy concentrating the rules into a object it eliminates the need to have a data set in a certain way within a datastore, and then have to go through the procedure and resetting once your done. The developer can simply write a testcase which instantiates the object and load it with with data set in a certain way, and check the result of the rules."
  },
  {
    "objectID": "Content/Journal/posts/Rules/index.html#testable",
    "href": "Content/Journal/posts/Rules/index.html#testable",
    "title": "Rules",
    "section": "",
    "text": "By concentrating the rules into a object it eliminates the need to have a data set in a certain way within a datastore, and then have to go through the procedure and resetting once your done. The developer can simply write a testcase which instantiates the object and load it with with data set in a certain way, and check the result of the rules."
  },
  {
    "objectID": "Content/Journal/posts/Rules/index.html#participants",
    "href": "Content/Journal/posts/Rules/index.html#participants",
    "title": "Rules",
    "section": "Participants",
    "text": "Participants\nThe classes and objects participating in this pattern are:\n\nIRule‚Äì Defines the interface for all specific rules.\nIRuleResult‚Äì Defines the interface for the results of all specific rules.\nBaseRule‚Äì The base class provides basic functionality to all rules that inherit from it.\nRule‚Äì The class represents a concrete implementation of the BaseRule class.\nRulesChain‚Äì It is a helper class that contains the main rule for the current conditional statement and the rest of the conditional chain of rules.\nRulesEvaluator‚Äì This is the main class that supports the creation of readable rules and their relation. It evaluates the rules and returns their results."
  },
  {
    "objectID": "Content/Journal/posts/ServiceApplicationDevelopment/index.html",
    "href": "Content/Journal/posts/ServiceApplicationDevelopment/index.html",
    "title": "Service Application Development",
    "section": "",
    "text": "Description\nFor to long there has been little thought about the consequences of changing code that is not strictly associated with the task assigned, this has been made possible by the solution containing all the code for the entire application. This had been the practice years ago but has been widely dismissed due the revolution of agile design and coding. Moving forward with a service-oriented application design, the code should be split along functional lines, where each function becomes a self-contained package which can be reused. Packaging up functionality has been common practice for several years as it promotes * Specialisation, forcing the components to do ‚Äòone thing and one thing only‚Äô. * Simpler testing. * Versioning * Ease of upgrade\n\n\n\nServiceApplicationMonolith\n\n\n\n\nConsequences\nThis break up of the monolith is not without its drawbacks, at least in the begining. The usual complaints are :-\n\nCode is being duplicated.\n\nYes, it will be, until all the relervant functionality has been encapsulated.\n\nPaying again, for no additional functionality.\n\nYes, we will be, For historical reasons of a lack of control on our part.\nOr a philosophy of keep the hopper full and pushing ever forward with business functionality, and forgetting any technical requirements."
  },
  {
    "objectID": "Content/Journal/posts/SQLChangeTracking/index.html",
    "href": "Content/Journal/posts/SQLChangeTracking/index.html",
    "title": "SQL Change Tracking",
    "section": "",
    "text": "As of novemebr 2022, Azure Functions have aquired a new trigger, SQL. With some minor changes to a sql database configuration, a Azure Function can capture sql trigger events and report on them. A useful implementation of this would be to feed data into stream analytics and have a constantly updated dashboard.\n\nALTER DATABASE [Hub.Dev]\nSET CHANGE_TRACKING = ON\n(CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON);\n\nALTER TABLE [dbo].[WebEnquiry]\nENABLE CHANGE_TRACKING;\n\n\n    public class WebEnquiry\n    {\n        public string ReferenceId { get; set; }\n        public string FirstName { get; set; }\n        public string LastName { get; set; }\n        public string Type { get; set; }\n        public string Source { get; set; }\n        public string AdvertPostCode { get; set; }\n        public string PropertyBedrooms { get; set; }\n        public string OfficeId { get; set; }\n        public string PropertyReferenceNumber { get; set; }\n        public Guid AssetId { get; set; }\n    }\n\n\n\n\n    public static class ToDoTrigger\n    {\n        [FunctionName(\"WebEnquiry\")]\n        public static void Run(\n            [SqlTrigger(\"[Hub.Dev].[dbo].[WebEnquiry]\", ConnectionStringSetting = \"SqlConnectionString\")]\n            IReadOnlyList&lt;SqlChange&lt;WebEnquiry&gt;&gt; enquires,\n            ILogger logger)\n        {\n            foreach (SqlChange&lt;WebEnquiry&gt; enquiry in enquires)\n            {\n                WebEnquiry item = enquiry.Item;\n                logger.LogInformation($\"ReferenceId: {item.ReferenceId}, Source: {item.Source}, AdvertPostCode: {item.AdvertPostCode}, PropertyReferenceNumber: {item.PropertyReferenceNumber}\");\n            }\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/SQLChangeTracking/index.html#sql-table-class",
    "href": "Content/Journal/posts/SQLChangeTracking/index.html#sql-table-class",
    "title": "SQL Change Tracking",
    "section": "",
    "text": "public class WebEnquiry\n    {\n        public string ReferenceId { get; set; }\n        public string FirstName { get; set; }\n        public string LastName { get; set; }\n        public string Type { get; set; }\n        public string Source { get; set; }\n        public string AdvertPostCode { get; set; }\n        public string PropertyBedrooms { get; set; }\n        public string OfficeId { get; set; }\n        public string PropertyReferenceNumber { get; set; }\n        public Guid AssetId { get; set; }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/SQLChangeTracking/index.html#azure-function-sql-trigger",
    "href": "Content/Journal/posts/SQLChangeTracking/index.html#azure-function-sql-trigger",
    "title": "SQL Change Tracking",
    "section": "",
    "text": "public static class ToDoTrigger\n    {\n        [FunctionName(\"WebEnquiry\")]\n        public static void Run(\n            [SqlTrigger(\"[Hub.Dev].[dbo].[WebEnquiry]\", ConnectionStringSetting = \"SqlConnectionString\")]\n            IReadOnlyList&lt;SqlChange&lt;WebEnquiry&gt;&gt; enquires,\n            ILogger logger)\n        {\n            foreach (SqlChange&lt;WebEnquiry&gt; enquiry in enquires)\n            {\n                WebEnquiry item = enquiry.Item;\n                logger.LogInformation($\"ReferenceId: {item.ReferenceId}, Source: {item.Source}, AdvertPostCode: {item.AdvertPostCode}, PropertyReferenceNumber: {item.PropertyReferenceNumber}\");\n            }\n        }\n    }"
  },
  {
    "objectID": "Content/Journal/posts/StorageAccounts/index.html",
    "href": "Content/Journal/posts/StorageAccounts/index.html",
    "title": "Storage Accounts",
    "section": "",
    "text": "The blob container resource is a sub-resource of the storage account. When you create a storage account, you can now specify an array of storages resources. We are then going to specify objects of type ‚ÄúblobServices/containers‚Äù, and make sure to use an API version of 2018-02-01 or later. In the example below, we are creating a storage account with two containers.\n{\n    \"type\": \"Microsoft.Storage/storageAccounts\",\n    \"apiVersion\": \"2018-02-01\",\n    \"name\": \"[parameters('StorageAccountName')]\",\n    \"location\": \"[resourceGroup().location]\",\n    \"tags\": {\n        \"displayName\": \"[parameters('StorageAccountName')]\"\n    },\n    \"sku\": {\n        \"name\": \"Standard_LRS\"\n    },\n    \"kind\": \"StorageV2\",\n    \"properties\": {},\n    \"resources\": [\n        {\n            \"type\": \"blobServices/containers\",\n            \"apiVersion\": \"2018-03-01-preview\",\n            \"name\": \"[concat('default/', parameters('Container1Name'))]\",\n            \"dependsOn\": [\n                \"[parameters('StorageAccountName')]\"\n            ],\n            \"properties\": {\n                \"publicAccess\": \"Container\"\n            }\n        },\n        {\n            \"type\": \"blobServices/containers\",\n            \"apiVersion\": \"2018-03-01-preview\",\n            \"name\": \"[concat('default/', parameters('Container2Name'))]\",\n            \"dependsOn\": [\n                \"[parameters('StorageAccountName')]\"\n            ],\n            \"properties\": {\n                \"publicAccess\": \"None\"\n            }\n        }\n    ]\n}\nWe can then go ahead and deploy this, and see the two containers being created. You can see in the template we are setting different Public Access properties on the two containers, you have a choice of 3 values here: None (private container) Container (the whole container is publically accessible) Blob (only Blobs are publically accessible)\nThe above example is fine, but you need to specify each of the containers as separate parameters, and remember to add the correct number of container clauses to the script.\nSo if we specify the containers as a array in the parameters file\n    \"para_storageObject\": {\n      \"value\": {\n        \"containers\": [\n          {\n            \"containerName\": \"files\"\n          },\n          {\n            \"containerName\": \"elephants\"\n          },\n          {\n            \"containerName\": \"bananas\"\n          }\n        ]\n      }\n    },\nThen we can use a copy clause to cycle over the array and create our containers in the deploy\n    {\n      \"apiVersion\": \"2018-03-01-preview\",\n      \"copy\": {\n        \"name\": \"containersCopy\",\n        \"count\": \"[length(parameters('para_storageObject').containers)]\"\n      },\n      \"dependsOn\": [\n        \"[concat('Microsoft.Storage/storageAccounts/', variables('var_str_name'))]\"\n      ],\n      \"name\": \"[concat(variables('var_str_name'), '/default/', parameters('para_storageObject').containers[copyIndex()].containerName)]\",\n      \"properties\": {\n        \"publicAccess\": \"Container\"\n      },\n      \"tags\": {\n        \"displayName\": \"StorageAcct/Containers\"\n      },\n      \"type\": \"Microsoft.Storage/storageAccounts/blobServices/containers\"\n    },"
  },
  {
    "objectID": "Content/Journal/posts/UploadAVFileScanning/index.html",
    "href": "Content/Journal/posts/UploadAVFileScanning/index.html",
    "title": "Web Upload File AV Scanning",
    "section": "",
    "text": "Description\nWeb file uploads within a company has been a neglected space for a while now, as any file the appears on a laptop or via email is scanned by the inbuild Anti Virus software, but with new projects opening up internal strorage to external entities via web portals we have to consider the fact that somone could upload a file containing a virus and we would be none the wiser. For us to write a Anti Virus scanning tool is out of the question, even employing a piece of opensource software is not advisable, as we have no idea how upto date the virus templates are, or even if they are valid.\nA solution would be to send any prospective file to a external third party who specialises in file scanning and verification. A possible solution is descibed below.\n\n\n\nFile-AV-Scanning\n\n\n\n\nA Solution\n\nA file is uploaded via a a Wb Upload tool.\nThe web application registers its interest in file upload statuses with the SignalR handler\nThe file is passed to the Document API\nThe Document API passes the document to a Third Party AV scanning company.\n\nBecause a document could contain more than one file, It could take the Third Party a while to scan the complete document, so they normally make use of a Webhook interface so they can asynchronously send back a response.\n\nThe Document API receives a response and posts a success or fail message to the Event Grid File Upload Topic.\nIf the AV response was a success the file is written to the blob storage.\n\nThe notification pattern of EventGrid Topic/ Azure Function / Azure SignalR is a standard pattern. There is also a pattern for securing webhooks, although webhooks are not loved by all."
  },
  {
    "objectID": "Content/Journal/posts/VisualStudioSolutioTemplate/index.html",
    "href": "Content/Journal/posts/VisualStudioSolutioTemplate/index.html",
    "title": "Visual Studio Solution Template",
    "section": "",
    "text": "Create a new Solution\nCreate your directory structure\nAdd relervant Projects in the directory structure\n\nExport each project using the Project-&gt;Export Template menu item\n\nThis will create a zip file.\n\nRepeat for all Projects.\nGo to the directory where the zips have been created\n\nC:{user}- Knight FrankStudio {version}Exported Templates\n\nIn this directory create your folder structure\n\n\\Build\n\\Deploy\n\\Src\n\\Src\\API\n\\Src\\Client\n\\Tests\n\\Wiki\n\nExtract the Zip files into the relervant directory.\nIn the empty directories create a Read.Me text file.\n\nIf you dont then the directory will not be preserved.\n\nin the root create a .vstemplate xml file\nAdd the following\n\nUpdate the Name\nDescription\nProject Type, if necceary\nAdd any new solution folders\nAdd any new vstemplates\n\n\n&lt;VSTemplate Version=\"2.0.0\" Type=\"ProjectGroup\"\n    xmlns=\"http://schemas.microsoft.com/developer/vstemplate/2005\"&gt;\n    &lt;TemplateData&gt;\n        &lt;Name&gt;CSharp, Az Function, Blazor&lt;/Name&gt;\n        &lt;Description&gt;Blazor, Azure Function Template&lt;/Description&gt;\n        &lt;Icon&gt;__TemplateIcon.PNG&lt;/Icon&gt;\n        &lt;ProjectType&gt;CSharp&lt;/ProjectType&gt;\n    &lt;/TemplateData&gt;\n    &lt;TemplateContent&gt;\n        &lt;ProjectCollection&gt;\n            &lt;SolutionFolder Name=\"Build\"&gt;\n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Deploy\"&gt;\n                &lt;ProjectTemplateLink ProjectName=\"Deploy\"&gt;\n                    Deploy\\MyTemplate.vstemplate\n                &lt;/ProjectTemplateLink&gt;          \n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Src\"&gt;\n                &lt;SolutionFolder Name=\"API\"&gt;\n                    &lt;ProjectTemplateLink ProjectName=\"API\"&gt;\n                        Src\\API\\MyTemplate.vstemplate\n                    &lt;/ProjectTemplateLink&gt;                  \n                &lt;/SolutionFolder&gt;   \n                &lt;SolutionFolder Name=\"Client\"&gt;\n                    &lt;ProjectTemplateLink ProjectName=\"Client\"&gt;\n                        Src\\Client\\MyTemplate.vstemplate\n                    &lt;/ProjectTemplateLink&gt;                  \n                &lt;/SolutionFolder&gt;                   \n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Tests\"&gt;\n            &lt;/SolutionFolder&gt;\n            &lt;SolutionFolder Name=\"Wiki\"&gt;\n            &lt;/SolutionFolder&gt;\n        &lt;/ProjectCollection&gt;\n    &lt;/TemplateContent&gt;\n&lt;/VSTemplate&gt;\n\nSelect all the files and save as a compressed ZIP\nCopy the file Zip file to C:{user}- Knight FrankStudio {version}\n\n\n\nVSNewProject"
  },
  {
    "objectID": "Content/Journal/posts/WOPI/index.html",
    "href": "Content/Journal/posts/WOPI/index.html",
    "title": "WOPI",
    "section": "",
    "text": "For a full description of WOPI, please follow the link to the project. WOPI Project\nImplementing a WOPI host in Azure Functions involves creating a series of HTTP-triggered functions that adhere to the specific REST endpoints and operations defined by the WOPI protocol. This service acts as the intermediary between your file storage (e.g., Azure Blob Storage) and WOPI clients (like Microsoft 365 for the web). Here are the key steps and code examples for setting up the primary WOPI endpoints using C# Azure Functions (Isolated Worker Model, which is recommended)."
  },
  {
    "objectID": "Content/Journal/posts/WOPI/index.html#prerequisites",
    "href": "Content/Journal/posts/WOPI/index.html#prerequisites",
    "title": "WOPI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n.NET 8.0+\nAzure Functions Core Tools\nVisual Studio 2022 (or VS Code)"
  },
  {
    "objectID": "Content/Journal/posts/WOPI/index.html#the-wopi-discovery-endpoint",
    "href": "Content/Journal/posts/WOPI/index.html#the-wopi-discovery-endpoint",
    "title": "WOPI",
    "section": "The WOPI Discovery Endpoint",
    "text": "The WOPI Discovery Endpoint\nThe client first calls this endpoint to discover the supported actions and capabilities of your service. This is typically a static response. csharp\n// Discovery.cs: Example of a static discovery XML function\nusing System.Net;\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\n\npublic class WopiDiscovery\n{\n    private readonly ILogger _logger;\n\n    public WopiDiscovery(ILoggerFactory loggerFactory)\n    {\n        _logger = loggerFactory.CreateLogger&lt;WopiDiscovery&gt;();\n    }\n\n    [Function(\"WopiDiscovery\")]\n    public HttpResponseData Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", Route = \"hosting/discovery\")] HttpRequestData req)\n    {\n        _logger.LogInformation(\"WOPI Discovery endpoint hit.\");\n\n        var response = req.CreateResponse(HttpStatusCode.OK);\n        response.Headers.Add(\"Content-Type\", \"application/xml\");\n        \n        // This XML is a static string in a real implementation, defining supported actions\n        // (view, edit, etc.) for various file types. \n        // You'll need to generate a valid WOPI discovery XML based on your service's capabilities.\n        string discoveryXml = @\"&lt;wopi-discovery&gt;...&lt;/wopi-discovery&gt;\"; \n        \n        response.WriteString(discoveryXml);\n        return response;\n    }\n}\nUse code with caution."
  },
  {
    "objectID": "Content/Journal/posts/WOPI/index.html#the-checkfileinfo-endpoint",
    "href": "Content/Journal/posts/WOPI/index.html#the-checkfileinfo-endpoint",
    "title": "WOPI",
    "section": "The CheckFileInfo Endpoint",
    "text": "The CheckFileInfo Endpoint\nThis endpoint is crucial for every WOPI action. The client sends a GET request to your service for a specific file ID. Your function must return a JSON payload with file metadata and user permissions. The URL structure is ‚Ä¶/wopi/files/{file_id}. csharp\n// CheckFileInfo.cs: Handles the CheckFileInfo operation\nusing System.Net;\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json;\n\npublic class CheckFileInfo\n{\n    [Function(\"CheckFileInfo\")]\n    public async Task&lt;HttpResponseData&gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", Route = \"wopi/files/{fileId}\")] HttpRequestData req,\n        string fileId,\n        ILogger log)\n    {\n        log.LogInformation($\"CheckFileInfo request for file ID: {fileId}\");\n\n        // 1. Validate the 'access_token' (passed as a query parameter in the initial request)\n        string accessToken = req.Query[\"access_token\"];\n        if (!IsValidAccessToken(accessToken))\n        {\n            return req.CreateResponse(HttpStatusCode.Unauthorized);\n        }\n\n        // 2. Fetch file info from your storage (e.g., Azure Blob metadata)\n        // This is a placeholder; replace with actual logic.\n        var fileInfo = GetFileInfoFromStorage(fileId); \n\n        if (fileInfo == null)\n        {\n            return req.CreateResponse(HttpStatusCode.NotFound);\n        }\n\n        // 3. Construct the WOPI CheckFileInfo JSON response\n        var responseData = new \n        {\n            BaseFileName = fileInfo.Name,\n            OwnerId = fileInfo.OwnerId,\n            Size = fileInfo.Size,\n            Sha256 = fileInfo.Sha256, // Hash of the file contents\n            Version = fileInfo.Version,\n            // Permissions (UserCanWrite, UserCanPrint, etc.) are critical\n            UserCanWrite = true,\n            // Add other required properties as needed per WOPI specs\n        };\n\n        var response = req.CreateResponse(HttpStatusCode.OK);\n        response.Headers.Add(\"Content-Type\", \"application/json\");\n        await response.WriteStringAsync(JsonConvert.SerializeObject(responseData));\n        return response;\n    }\n\n    // Placeholder methods for your business logic\n    private bool IsValidAccessToken(string token) { /* Logic to validate your custom token */ return true; }\n    private FileMetadata GetFileInfoFromStorage(string fileId) { /* Logic to get file metadata */ return new FileMetadata(); }\n}\n\npublic class FileMetadata \n{\n    public string Name { get; set; }\n    public long Size { get; set; }\n    public string OwnerId { get; set; }\n    public string Sha256 { get; set; }\n    public string Version { get; set; }\n}\nUse code with caution."
  },
  {
    "objectID": "Content/Journal/posts/WOPI/index.html#the-getfile-endpoint",
    "href": "Content/Journal/posts/WOPI/index.html#the-getfile-endpoint",
    "title": "WOPI",
    "section": "The GetFile Endpoint",
    "text": "The GetFile Endpoint\nThis endpoint is used by the client to download the actual file content. The URL structure is the same as CheckFileInfo, but it uses a GET request without the WOPI override header. csharp\n// GetFile.cs: Handles the GetFile operation\nusing System.Net;\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\nusing System.IO;\nusing System.Threading.Tasks;\n\npublic class GetFile\n{\n    [Function(\"GetFile\")]\n    public async Task&lt;HttpResponseData&gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", Route = \"wopi/files/{fileId}/contents\")] HttpRequestData req,\n        string fileId,\n        ILogger log)\n    {\n        log.LogInformation($\"GetFile request for file ID: {fileId}\");\n\n        // 1. Validate the 'access_token' (again, in the query string)\n        string accessToken = req.Query[\"access_token\"];\n        if (!IsValidAccessToken(accessToken))\n        {\n            return req.CreateResponse(HttpStatusCode.Unauthorized);\n        }\n\n        // 2. Retrieve the file stream from your storage (e.g., Azure Blob Storage)\n        Stream fileStream = GetFileStreamFromStorage(fileId);\n\n        if (fileStream == null)\n        {\n            return req.CreateResponse(HttpStatusCode.NotFound);\n        }\n\n        // 3. Return the file stream as the response body\n        var response = req.CreateResponse(HttpStatusCode.OK);\n        response.Headers.Add(\"Content-Type\", \"application/octet-stream\"); // Adjust content type as necessary\n        await response.WriteAsync(fileStream); // Write the stream directly\n        return response;\n    }\n\n    // Placeholder methods\n    private bool IsValidAccessToken(string token) { return true; }\n    private Stream GetFileStreamFromStorage(string fileId) { /* Logic to return file stream */ return Stream.Null; }\n}\nUse code with caution."
  },
  {
    "objectID": "Content/Journal/posts/WOPI/index.html#key-considerations",
    "href": "Content/Journal/posts/WOPI/index.html#key-considerations",
    "title": "WOPI",
    "section": "Key Considerations",
    "text": "Key Considerations\n\nAuthentication: The WOPI protocol uses an access_token query parameter for authentication on initial requests. For subsequent operations (like PutFile), tokens might be passed in HTTP headers (Authorization or X-WOPI-Token). Your functions must validate this token on every single request.\nWOPI Headers: Many operations rely heavily on custom HTTP headers starting with X-WOPI- (e.g., X-WOPI-Override, X-WOPI-Lock). You‚Äôll need to read and respond to these headers in your Azure Functions.\nFile Locking: For edit operations, you must implement the Lock, Unlock, and RefreshLock operations using the X-WOPI-Lock header to prevent concurrent edits.\nWOPI Validator: Microsoft provides a WOPI Validator tool that you can use to test your implementation during development. Deployment: Your Azure Functions app must be publicly accessible on the internet for Microsoft 365 for the web to reach it.\nPnP-WOPI Sample: The Microsoft OfficeDev/PnP-WOPI repository on GitHub provides a solid ASP.NET MVC sample which can be used as a reference for the logic needed, even if the hosting model is different."
  },
  {
    "objectID": "Content/Research/articles/APIFirst/TypeSpec.html",
    "href": "Content/Research/articles/APIFirst/TypeSpec.html",
    "title": "TypeSpec",
    "section": "",
    "text": "TypeSpec is a powerful tool for designing and generating APIs. It allows developers to define the structure and behavior of their APIs using a simple and intuitive syntax. TypeSpec supports a wide range of features, including:\n\nDefining endpoints and routes\n\nSpecifying request and response formats\nAdding authentication and authorization\nGenerating client and server code in multiple programming languages\nCreating interactive documentation for APIs\nValidating API definitions against industry standards such as OpenAPI and Swagger\nExtending TypeSpec with custom plugins and extensions\n\nTypeSpec is designed to be easy to use and understand, making it accessible to developers of all skill levels. It also integrates seamlessly with other tools and frameworks, such as AutoRest, making it a versatile choice for API development.\nTypeSpec can be used in a variety of scenarios, including:\n\nBuilding RESTful APIs for web and mobile applications\nCreating microservices and serverless functions\nDesigning APIs for data analytics and machine learning applications\nGenerating SDKs and client libraries for third-party developers\nAutomating API documentation and testing processes\nPrototyping and iterating on API designs quickly and efficiently\nIntegrating with CI/CD pipelines for continuous delivery of APIs\nEnsuring compliance with industry regulations and standards for API security and privacy\nFacilitating API versioning and backward compatibility for long-term maintenance\nSupporting multiple API styles, including REST, GraphQL, and gRPC\nEnabling rapid development and deployment of APIs in cloud-native environments"
  },
  {
    "objectID": "Content/Research/index.html",
    "href": "Content/Research/index.html",
    "title": "Research",
    "section": "",
    "text": "Day  : \n    Link : link  \n\n    \n    \n\n            \n\n\n        \n\nNo matching items\n\n\n&lt;div class=\"row\"&gt;\n    &lt;div class=\"left\"&gt; Left &lt;/div&gt;\n    &lt;div class=\"right\"&gt; Right &lt;/div&gt;\n&lt;/div&gt;"
  }
]